{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = os.path.abspath('../..')\n",
    "save_dir = os.path.join(base_dir,'results')\n",
    "data_dir = os.path.join(base_dir,'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.manual_seed(0)\n",
    "import wandb\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "from torch.utils.data import TensorDataset,Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "# from neurora.rdm_corr import rdm_correlation_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLabelModel(nn.Module):\n",
    "    def __init__(self, encoded_space_dim=10, num_classes=4):\n",
    "        super().__init__()\n",
    "        \"\"\n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "    \n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        ### Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            nn.Linear(32*4*4, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, encoded_space_dim)\n",
    "        )\n",
    "        ##labeling module\n",
    "        self.decoder_labels_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 10),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(10, num_classes),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        batch_s = x.size(0)\n",
    "        img_features = self.encoder_cnn(x)\n",
    "        img_features = self.flatten(img_features)\n",
    "        \n",
    "        out_latent = self.encoder_lin(img_features)\n",
    "\n",
    "        label = self.decoder_labels_lin(out_latent)\n",
    "        label = F.softmax(label,dim=1)\n",
    "        return out_latent, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### custom loss computing triplet loss and labeling loss\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, margin=10):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, anchor, positive, negative, label, pred_label):\n",
    "        cosine_sim = torch.nn.CosineSimilarity(1)\n",
    "        distance_positive = torch.tensor(1)-cosine_sim(anchor,positive)\n",
    "   \n",
    "        distance_negative = torch.tensor(1)-cosine_sim(anchor,negative)\n",
    "\n",
    "        triplet_loss = torch.maximum(distance_positive - distance_negative + self.margin, torch.tensor(0))\n",
    "        triplet_loss = torch.sum(triplet_loss)\n",
    "        label_loss = self.cross_entropy(pred_label, label.float())\n",
    "        total_loss = triplet_loss + label_loss\n",
    "        return triplet_loss, label_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrainModels(nn.Module):\n",
    "    def __init__(self, latent_dims, num_classes):\n",
    "        super(TrainModels, self).__init__()\n",
    "        self.triplet_lab_model = TripletLabelModel(latent_dims, num_classes)\n",
    "        self.custom_loss = CustomLoss()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, anchor_im, positive_im, negative_im):\n",
    "        anchor_latent, anchor_label = self.triplet_lab_model(anchor_im)\n",
    "        positive_latent, _ = self.triplet_lab_model(positive_im)\n",
    "        negative_latent, _ = self.triplet_lab_model(negative_im)\n",
    "\n",
    "        return anchor_latent, positive_latent, negative_latent, anchor_label\n",
    "\n",
    "    def test_epoch(self, test_data):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "        self.eval()\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            # Define the lists to store the outputs for each batch\n",
    "            test_triplet_loss = []\n",
    "            test_label_loss = []\n",
    "            test_total_loss = []\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for anchor_ims, contrast_ims, labels in test_data:\n",
    "                # Move tensor to the proper device\n",
    "                anchor_ims = anchor_ims.to(device)\n",
    "                contrast_ims = contrast_ims.to(device)\n",
    "                labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "                labels = labels.to(device)\n",
    "                anchor_latent, positive_latent, negative_latent, pred_label = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "                # Append the network output and the original image to the lists\n",
    "                triplet_loss, label_loss, total_loss = self.custom_loss(anchor_latent,\n",
    "                                                                positive_latent, \n",
    "                                                                negative_latent, \n",
    "                                                                labels,\n",
    "                                                                pred_label)\n",
    "                total += labels.size(0)\n",
    "                correct += (torch.argmax(pred_label, dim = 1) == torch.argmax(labels, dim = 1)).sum().item()\n",
    "                test_triplet_loss.append(triplet_loss.item())\n",
    "                test_label_loss.append(label_loss.item())\n",
    "                test_total_loss.append(total_loss.item())\n",
    "        test_triplet_loss = sum(test_triplet_loss)/len(test_triplet_loss)\n",
    "        test_label_loss = sum(test_label_loss)/len(test_label_loss)\n",
    "        test_total_loss = sum(test_total_loss)/len(test_total_loss)\n",
    "        test_accuracy = correct/total\n",
    "        return test_triplet_loss, test_label_loss, test_total_loss, test_accuracy\n",
    "\n",
    "    def test_epoch_calculate_representation_separation(self, test_data):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "        self.eval()\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            accuracies = []\n",
    "            for anchor_ims, contrast_ims, labels in test_data:\n",
    "                # Move tensor to the proper device\n",
    "                anchor_ims = anchor_ims.to(device)\n",
    "                contrast_ims = contrast_ims.to(device)\n",
    "                # labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "                # labels = labels.to(device)\n",
    "                anchor_latent, _, _, _ = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "                # use sklearn to predict labels from anchor_latent\n",
    "                # calculate accuracy\n",
    "                # x's are anchor_latent and y's are labels\n",
    "                # append accuracy to list\n",
    "                # put anchor_latent and labels on cpu and convert to numpy\n",
    "                anchor_latent = anchor_latent.cpu().numpy()\n",
    "                lm = linear_model.LogisticRegression()\n",
    "                lm.fit(anchor_latent, labels)\n",
    "                # convert labels to sklearn format\n",
    "                accuracies.append(lm.score(anchor_latent, labels))\n",
    "        accuracy = sum(accuracies)/len(accuracies)\n",
    "        return accuracy\n",
    "\n",
    "    def train_epoch(self, train_data, optimizer, train_mode):\n",
    "        self.train()\n",
    "        train_triplet_loss = []\n",
    "        train_label_loss = []\n",
    "        train_total_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for anchor_ims, contrast_ims, labels in train_data:\n",
    "            \n",
    "            anchor_ims = anchor_ims.to(device)\n",
    "            contrast_ims = contrast_ims.to(device)\n",
    "            labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            anchor_latent, positive_latent, negative_latent, pred_label = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "           \n",
    "           \n",
    "           \n",
    "            triplet_loss, label_loss, total_loss = self.custom_loss(anchor_latent,\n",
    "                                                                positive_latent, \n",
    "                                                                negative_latent, \n",
    "                                                                labels,\n",
    "                                                                pred_label)\n",
    "            \n",
    "            \n",
    "            if train_mode==0:\n",
    "                triplet_loss.backward()\n",
    "            elif train_mode==1:\n",
    "                label_loss.backward()\n",
    "            elif train_mode==2:\n",
    "                total_loss.backward()\n",
    "\n",
    "        \n",
    "\n",
    "            optimizer.step()\n",
    "            train_triplet_loss.append(triplet_loss.item())\n",
    "            train_label_loss.append(label_loss.item())\n",
    "            train_total_loss.append(total_loss.item())\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.argmax(pred_label, dim = 1) == torch.argmax(labels, dim = 1)).sum().item()\n",
    "        train_triplet_loss = sum(train_triplet_loss)/len(train_triplet_loss)\n",
    "        train_label_loss = sum(train_label_loss)/len(train_label_loss)\n",
    "        train_total_loss = sum(train_total_loss)/len(train_total_loss)\n",
    "        train_accuracy = correct/total\n",
    "        return train_triplet_loss, train_label_loss, train_total_loss, train_accuracy\n",
    "\n",
    "    def training_loop(self, train_data, test_data,train_mode,\n",
    "                      epochs, optimizer):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_triplet_losses = []\n",
    "        val_triplet_losses = []\n",
    "        train_label_losses = []\n",
    "        val_label_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        latent_separation_accuracy = 0\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "          train_triplet_loss, train_label_loss, train_total_loss, train_accuracy =self.train_epoch(train_data, optimizer, \n",
    "                                             train_mode)\n",
    "          test_triplet_loss, test_label_loss, test_total_loss, test_accuracy = self.test_epoch(test_data)\n",
    "          separation_accuracy = self.test_epoch_calculate_representation_separation(test_data)\n",
    "          train_losses.append(train_total_loss)\n",
    "          val_losses.append(test_total_loss)\n",
    "          train_triplet_losses.append(train_triplet_loss)\n",
    "          val_triplet_losses.append(test_triplet_loss)\n",
    "          train_label_losses.append(train_label_loss)\n",
    "          val_label_losses.append(test_label_loss)\n",
    "          train_accuracies.append(train_accuracy)\n",
    "          val_accuracies.append(test_accuracy)\n",
    "          wandb.log({\"train triplet loss\": train_triplet_loss, \n",
    "            \"train label loss\":train_label_loss, \n",
    "            \"validation triplet loss\":test_triplet_loss, \n",
    "            \"validation label loss\":test_label_loss, \n",
    "            \"total train loss\":train_total_loss, \n",
    "            \"total validation loss\":test_total_loss, \n",
    "            \"train label accuracy\":train_accuracy, \n",
    "            \"validation label accuracy\":test_accuracy,\n",
    "            'latent separation accuracy':separation_accuracy})\n",
    "        return train_triplet_losses, train_label_losses, val_triplet_losses, val_label_losses ,train_losses, val_losses, train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_A_ims = np.load(os.path.join(data_dir, 'set_A.npy'))\n",
    "set_B_ims = np.load(os.path.join(data_dir, 'set_B.npy'))\n",
    "set_C_ims= np.load(os.path.join(data_dir, 'set_C.npy'))\n",
    "set_A_labs = np.load(os.path.join(data_dir, 'set_A_labs.npy'))\n",
    "set_B_labs = np.load(os.path.join(data_dir, 'set_B_labs.npy'))\n",
    "set_C_labs = np.load(os.path.join(data_dir, 'set_C_labs.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###initialize weights and bias tracking\n",
    "def wandb_init(epochs, lr, train_mode, batch_size, model_number,data_set):\n",
    "  wandb.init(project=\"ConceptualAlignmentLanguage\", entity=\"psych-711\",settings=wandb.Settings(start_method=\"thread\"))\n",
    "  wandb.config = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size, \n",
    "    # \"label_ratio\":label_ratio, \n",
    "    \"model_number\": model_number,\n",
    "    \"dataset\": data_set,\n",
    "    \"train_mode\":train_mode,\n",
    "  }\n",
    "  train_mode_dict = {0:'no label', 1:'label', 2:'label and triplet' }\n",
    "  wandb.run.name = f'{data_set}_{train_mode_dict[train_mode]}_{model_number}'\n",
    "  wandb.run.save()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_code(save_dir, num_models, epochs, num_classes, batch_size,\n",
    "             lr, latent_dims):\n",
    "  if os.path.isdir(save_dir):\n",
    "    pass\n",
    "  else:\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "\n",
    "  test_intervals = [(540, 600), (1140, 1200), (1740, 1800), (2340, 2400)]\n",
    "\n",
    "  # initialize an empty list to hold the indices\n",
    "  val_indices = []\n",
    "\n",
    "  # loop through the intervals and append the indices to the list\n",
    "  for start, stop in test_intervals:\n",
    "      val_indices.extend(list(range(start, stop)))\n",
    "\n",
    "  train_indices = (np.setdiff1d(np.arange(2400),np.array(val_indices)))\n",
    "\n",
    "  np.random.seed(56)\n",
    "  contrast_indices  = np.concatenate((np.random.choice(np.arange(start=600, stop=2400), 600, replace=False),\n",
    "                np.random.choice(np.concatenate((np.arange(start=0, stop=600), np.arange(start=1200, stop=2400))), 600, replace=False),\n",
    "                np.random.choice(np.concatenate((np.arange(start=0, stop=1200), np.arange(start=1800, stop=2400))), 600, replace=False),\n",
    "                np.random.choice(np.arange(start=1800, stop=2400), 600, replace=False)))\n",
    "\n",
    "  for data_set in ['set_A','set_B','set_C']:\n",
    "    for train_mode in tqdm(range(3)):\n",
    "     # torch.manual_seed(0)\n",
    "      for model in range(num_models):\n",
    "        wandb_init(epochs, lr, train_mode, batch_size, model,data_set)\n",
    "\n",
    "        if data_set=='set_A':\n",
    "          train_data = TensorDataset(torch.tensor(set_A_ims.transpose(0,3,1,2)/255).float(), torch.tensor(set_A_ims[contrast_indices].transpose(0,3,1,2)/255).float(),\\\n",
    "                                     torch.tensor(set_A_labs).to(torch.int64))\n",
    "        elif data_set=='set_B':\n",
    "          train_data = TensorDataset(torch.tensor(set_B_ims.transpose(0,3,1,2)/255).float(), torch.tensor(set_B_ims[contrast_indices].transpose(0,3,1,2)/255).float(),\\\n",
    "                                     torch.tensor(set_B_labs).to(torch.int64))\n",
    "        elif data_set=='set_C':\n",
    "          train_data = TensorDataset(torch.tensor(set_C_ims.transpose(0,3,1,2)/255).float(), torch.tensor(set_C_ims[contrast_indices].transpose(0,3,1,2)/255).float(),\\\n",
    "                                     torch.tensor(set_C_labs).to(torch.int64))\n",
    "          \n",
    "        val_data = torch.utils.data.Subset(train_data, val_indices)\n",
    "        train_data = torch.utils.data.Subset(train_data, train_indices)\n",
    "       \n",
    "\n",
    "        train_data = torch.utils.data.DataLoader(train_data, \n",
    "                                                batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "        val_data = torch.utils.data.DataLoader(val_data, \n",
    "                                                batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "        \n",
    "     \n",
    "\n",
    "        train_obj = TrainModels(latent_dims, num_classes).to(device) # GPU\n",
    "        optimizer = torch.optim.Adam(train_obj.parameters(), lr=lr, weight_decay=1e-05)\n",
    "        train_triplet_losses, train_label_losses, \\\n",
    "          val_triplet_losses, val_label_losses, \\\n",
    "            train_losses, val_losses, train_accuracies, val_accuracies= train_obj.training_loop(train_data = train_data,\n",
    "                                                            test_data = val_data,\n",
    "                                                            epochs = epochs,\n",
    "                                                            optimizer = optimizer, \n",
    "                                                            train_mode = train_mode)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print('validation triplet loss:',val_triplet_losses,'validation total loss:',val_losses,'validation accuracy:',val_accuracies)\n",
    "        # wandb.log({\"train_img_loss\": train_img_loss, \n",
    "        #           \"train_label_loss\":train_label_loss, \n",
    "        #           \"val_img_loss\":val_img_loss, \n",
    "        #           \"val_label_loss\":val_label_loss, \n",
    "        #           \"train_losses\":train_losses, \n",
    "        #           \"val_losses\":val_losses, \n",
    "        #           \"train_accuracy\":train_accuracy, \n",
    "        #           \"val_accuracy\":val_accuracy})\n",
    "        train_mode_dict = {0:'no label', 1:'label', 2:'label and triplet' }\n",
    "        torch.save(train_obj.state_dict(), os.path.join(save_dir,f'{data_set}_{train_mode_dict[train_mode]}_{model}'))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20230419_171001-wqi0k7au</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/wqi0k7au' target=\"_blank\">wild-sky-478</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/wqi0k7au' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/wqi0k7au</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:18<00:00,  3.14it/s]\n",
      " 33%|███▎      | 1/3 [05:27<10:54, 327.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [2399.6357421875, 2394.532958984375, 2359.48388671875, 2290.60498046875, 2222.582763671875, 2192.6298828125, 2179.99169921875, 2177.47265625, 2180.35498046875, 2179.1015625, 2178.019775390625, 2169.6552734375, 2171.4140625, 2169.357177734375, 2166.63720703125, 2164.930419921875, 2166.349609375, 2163.876220703125, 2164.4912109375, 2164.5078125, 2171.88720703125, 2157.707763671875, 2186.857421875, 2181.73388671875, 2150.795654296875, 2172.63525390625, 2171.926025390625, 2160.03662109375, 2166.302734375, 2171.970458984375, 2161.35888671875, 2163.75927734375, 2176.045166015625, 2158.4501953125, 2171.469970703125, 2171.290283203125, 2158.876220703125, 2168.9833984375, 2172.72705078125, 2158.57958984375, 2181.44677734375, 2185.0634765625, 2164.990234375, 2162.0302734375, 2176.5400390625, 2165.8818359375, 2166.25341796875, 2171.278076171875, 2162.655029296875, 2171.00537109375, 2161.588623046875, 2175.87548828125, 2155.171875, 2183.91357421875, 2192.685791015625, 2175.375, 2148.187744140625, 2165.67578125, 2175.463623046875, 2162.385009765625, 2181.72265625, 2163.76708984375, 2162.661865234375, 2166.615966796875, 2168.657470703125, 2164.40234375, 2171.48583984375, 2145.21728515625, 2184.873046875, 2167.13720703125, 2148.063720703125, 2164.836669921875, 2168.0791015625, 2162.05810546875, 2162.338134765625, 2162.730712890625, 2157.573486328125, 2165.298828125, 2154.2607421875, 2165.302001953125, 2157.384521484375, 2158.7099609375, 2169.55859375, 2154.1728515625, 2171.77197265625, 2168.482666015625, 2149.578125, 2166.41943359375, 2171.80517578125, 2154.588134765625, 2160.90380859375, 2166.618408203125, 2151.997314453125, 2168.3515625, 2169.95556640625, 2157.196044921875, 2160.479248046875, 2166.0009765625, 2162.664794921875, 2160.39501953125, 2173.641357421875, 2161.020263671875, 2156.669921875, 2170.486083984375, 2159.04345703125, 2153.02685546875, 2166.62158203125, 2166.53466796875, 2156.242431640625, 2172.09375, 2158.05224609375, 2163.525634765625, 2168.10986328125, 2157.4677734375, 2160.366943359375, 2158.504638671875, 2159.25830078125, 2158.676025390625, 2161.11181640625, 2162.97021484375, 2155.918212890625, 2161.518798828125, 2158.7568359375, 2160.6044921875, 2158.050537109375, 2158.703125, 2156.873291015625, 2150.05078125, 2164.730712890625, 2151.42138671875, 2156.75634765625, 2153.77783203125, 2153.97412109375, 2157.5400390625, 2156.219482421875, 2158.529296875, 2153.48291015625, 2157.65283203125, 2163.743408203125, 2159.12890625, 2154.96142578125, 2155.50341796875, 2156.286865234375, 2159.34619140625, 2161.9228515625, 2160.474609375, 2160.494140625, 2159.55908203125, 2158.405029296875, 2157.832275390625, 2157.939453125, 2158.044921875, 2158.5771484375, 2157.949951171875, 2158.0576171875, 2157.909912109375, 2157.9140625, 2160.21337890625, 2157.08935546875, 2158.783203125, 2160.61083984375, 2157.91845703125, 2163.48095703125, 2148.28466796875, 2160.850830078125, 2161.492431640625, 2154.58154296875, 2163.820068359375, 2158.14111328125, 2167.1796875, 2160.4306640625, 2163.62451171875, 2160.19677734375, 2155.079345703125, 2169.67578125, 2148.0908203125, 2161.20947265625, 2163.71923828125, 2158.2841796875, 2156.2744140625, 2161.59130859375, 2160.449462890625, 2159.926513671875, 2161.853515625, 2160.0341796875, 2158.318115234375, 2159.898681640625, 2173.1357421875, 2157.17626953125, 2156.57568359375, 2162.702392578125, 2159.39501953125, 2156.552978515625, 2161.08837890625, 2168.94873046875, 2150.009521484375, 2163.2607421875, 2169.720458984375, 2156.93115234375, 2160.92138671875, 2166.126953125, 2166.670166015625, 2162.82958984375, 2158.69091796875, 2159.68017578125, 2164.71044921875, 2159.7705078125, 2164.08447265625, 2164.0859375, 2163.529296875, 2165.135986328125, 2164.15673828125, 2162.443115234375, 2160.338623046875, 2160.4208984375, 2164.247314453125, 2169.233642578125, 2161.11767578125, 2162.10205078125, 2164.97802734375, 2164.3232421875, 2167.58935546875, 2168.264404296875, 2168.30859375, 2161.6171875, 2170.778564453125, 2174.25341796875, 2165.2080078125, 2172.041015625, 2171.53173828125, 2160.63232421875, 2170.39404296875, 2179.27294921875, 2157.19921875, 2158.0810546875, 2160.20654296875, 2168.27587890625, 2163.6435546875, 2163.02294921875, 2166.85595703125, 2166.13330078125, 2166.06640625, 2163.8369140625, 2163.82373046875, 2164.08056640625, 2166.95751953125, 2165.98291015625, 2165.92041015625, 2163.933349609375, 2164.150390625, 2165.21728515625, 2164.415771484375, 2166.975830078125, 2166.1689453125, 2166.0498046875, 2166.11962890625, 2165.856689453125, 2165.707763671875, 2165.836669921875, 2165.7958984375, 2166.08447265625, 2166.017822265625, 2166.03466796875, 2166.04345703125, 2166.0625, 2165.985107421875, 2165.947509765625, 2165.986328125, 2165.9169921875, 2166.17919921875, 2179.50927734375, 2167.01318359375, 2148.1240234375, 2164.4912109375, 2164.97314453125, 2166.17529296875, 2172.10107421875, 2166.13818359375, 2164.1845703125, 2163.9736328125, 2157.43017578125, 2164.025146484375, 2164.038330078125, 2161.983154296875, 2167.655517578125, 2167.9345703125, 2159.79638671875, 2160.066162109375, 2160.361083984375, 2160.01708984375, 2160.86083984375, 2171.509765625, 2172.176513671875, 2157.9482421875, 2160.0693359375, 2165.8642578125, 2173.10595703125, 2158.58740234375, 2165.66650390625, 2165.721435546875, 2162.1396484375, 2164.613037109375, 2165.013427734375, 2162.13134765625, 2160.3544921875, 2160.210693359375, 2162.32080078125, 2162.039306640625, 2162.1640625, 2164.30224609375, 2164.040283203125, 2161.58740234375, 2164.043212890625, 2164.397216796875, 2163.87744140625, 2161.917724609375, 2161.728759765625, 2161.994384765625, 2161.97705078125, 2160.190673828125, 2162.033935546875, 2162.558349609375, 2163.947021484375, 2158.8544921875, 2162.291015625, 2166.324462890625, 2160.66162109375, 2162.265869140625, 2160.332763671875, 2161.16845703125, 2164.018798828125, 2164.136474609375, 2164.127197265625, 2164.171875, 2164.36865234375, 2165.2490234375, 2164.479736328125, 2164.0224609375, 2163.98583984375, 2163.996337890625, 2164.02099609375, 2164.15625, 2165.0078125, 2164.0625, 2164.0576171875, 2164.02734375, 2164.035400390625, 2164.067138671875, 2164.02294921875, 2163.99560546875, 2163.9794921875, 2164.0556640625, 2164.44287109375, 2165.159912109375, 2165.982421875, 2164.708984375, 2163.96435546875, 2162.23291015625, 2161.494140625, 2166.1044921875, 2176.65625, 2168.1083984375, 2154.0048828125, 2160.15478515625, 2176.712890625, 2160.3056640625, 2156.1259765625, 2154.6953125, 2162.3486328125, 2165.68701171875, 2162.3251953125, 2161.472900390625, 2162.6552734375, 2159.644287109375, 2160.404296875, 2161.1904296875, 2158.16015625, 2158.34033203125, 2158.320068359375, 2164.06298828125, 2156.07177734375, 2157.3203125, 2159.971923828125, 2164.0166015625, 2167.7060546875, 2158.435546875, 2154.462646484375, 2154.762451171875, 2159.9833984375, 2159.774658203125, 2159.990966796875, 2160.1083984375, 2160.108154296875, 2160.04541015625, 2160.29052734375, 2156.529296875, 2160.4189453125, 2160.960205078125, 2161.8740234375, 2151.769775390625, 2157.9765625, 2160.021484375, 2164.8564453125, 2159.93115234375, 2159.70458984375, 2164.01123046875, 2163.546875, 2163.34765625, 2160.018798828125, 2160.13916015625, 2163.968505859375, 2162.35791015625, 2155.91943359375, 2159.927490234375, 2160.096435546875, 2159.921142578125, 2158.388916015625, 2159.932861328125, 2158.764404296875, 2158.041015625, 2159.489013671875, 2159.984130859375, 2160.219970703125, 2163.69140625, 2166.91259765625, 2161.64990234375, 2159.869140625, 2166.733642578125, 2166.43017578125, 2157.94580078125, 2159.908203125, 2158.51806640625, 2159.56884765625, 2152.7646484375, 2156.506103515625, 2162.0693359375, 2162.205078125, 2161.985595703125, 2162.063720703125, 2163.989501953125, 2162.291015625, 2164.63037109375, 2167.96728515625, 2164.00927734375, 2163.539794921875, 2163.3232421875, 2164.11376953125, 2164.160888671875, 2164.063720703125, 2164.090576171875, 2164.10595703125, 2164.015625, 2163.97314453125, 2163.934814453125, 2163.79638671875, 2163.982421875, 2159.8740234375, 2157.187744140625, 2166.04931640625, 2167.1591796875, 2165.999267578125, 2165.9794921875, 2164.10546875, 2163.989501953125, 2163.94970703125, 2161.91162109375, 2161.93994140625, 2161.955322265625, 2164.1845703125, 2161.8193359375, 2150.43408203125, 2160.06201171875, 2180.642578125, 2161.281005859375, 2155.68115234375, 2163.986328125, 2166.48828125, 2160.67236328125, 2155.4384765625, 2153.0966796875, 2157.494384765625, 2163.070556640625, 2156.60302734375, 2150.488525390625, 2152.212158203125, 2162.2919921875, 2170.101318359375, 2159.7509765625, 2160.7158203125, 2159.01708984375, 2158.44287109375, 2170.19482421875, 2170.2109375, 2163.342041015625, 2160.41796875, 2170.45703125, 2168.258544921875, 2160.98046875, 2154.032470703125, 2165.21728515625, 2173.852294921875, 2159.0126953125, 2154.462890625, 2162.887451171875, 2165.24951171875, 2161.090576171875, 2160.0234375, 2156.96044921875, 2159.43505859375, 2160.48046875, 2160.06787109375, 2164.37158203125, 2157.690673828125, 2156.75048828125, 2160.249267578125, 2164.613037109375, 2166.94482421875, 2170.39453125, 2168.0166015625, 2163.3369140625, 2161.02001953125, 2160.678466796875, 2163.46533203125, 2166.9814453125, 2164.02490234375, 2165.6005859375, 2166.224365234375, 2165.115966796875, 2163.4970703125, 2165.384765625, 2160.754150390625, 2157.92041015625, 2151.4599609375, 2161.38134765625, 2168.444580078125, 2161.763427734375, 2163.710693359375, 2168.21923828125, 2169.6162109375, 2167.830810546875, 2163.74072265625, 2168.7705078125, 2165.80810546875, 2160.17041015625, 2154.042724609375, 2158.324951171875, 2165.12646484375, 2159.98388671875, 2160.3955078125, 2160.02685546875, 2160.4150390625, 2164.357421875, 2163.84912109375, 2164.1787109375, 2161.910400390625, 2156.09814453125, 2155.97412109375, 2155.953369140625, 2155.638671875, 2156.073486328125, 2156.054931640625, 2156.00390625, 2156.005615234375, 2156.038818359375, 2156.075927734375, 2156.12109375, 2156.070068359375, 2156.056396484375, 2156.076904296875, 2156.1865234375, 2156.506103515625, 2159.596435546875, 2160.235595703125, 2160.032958984375, 2157.787353515625, 2157.93115234375, 2158.047119140625, 2165.688232421875, 2172.1513671875, 2165.79150390625, 2159.677734375, 2150.87548828125, 2162.5400390625, 2164.06494140625, 2167.00634765625, 2164.153564453125, 2163.943359375, 2159.6044921875, 2161.78955078125, 2162.054931640625, 2170.026611328125, 2154.2412109375, 2145.966796875, 2144.475830078125, 2140.314453125, 2149.99853515625, 2149.99267578125, 2150.0380859375, 2150.130615234375, 2152.333740234375, 2156.632080078125, 2161.940673828125, 2160.0078125, 2159.819091796875, 2154.0576171875, 2164.616943359375, 2166.073486328125, 2167.536865234375, 2169.03662109375, 2160.2802734375, 2167.9814453125, 2168.55908203125, 2169.48876953125, 2166.3046875, 2166.27978515625, 2169.614501953125, 2168.34814453125, 2163.43798828125, 2161.5927734375, 2163.9033203125, 2168.221435546875, 2175.30322265625, 2179.67724609375, 2177.564453125, 2173.02978515625, 2172.619384765625, 2174.518798828125, 2176.173828125, 2176.27001953125, 2172.76513671875, 2170.054931640625, 2167.73779296875, 2168.380615234375, 2169.95703125, 2169.669921875, 2170.29736328125, 2170.718994140625, 2173.6015625, 2179.04345703125, 2171.469482421875, 2172.431640625, 2163.10302734375, 2167.939697265625, 2166.327880859375, 2170.546142578125, 2171.65576171875, 2170.41845703125, 2169.38916015625, 2162.599853515625, 2162.849609375, 2168.33740234375, 2167.962890625, 2165.94677734375, 2166.15576171875, 2166.297607421875, 2167.84912109375, 2166.135009765625, 2164.64697265625, 2166.2744140625, 2165.95068359375, 2166.583251953125, 2165.072265625, 2165.025390625, 2165.0751953125, 2163.958740234375, 2163.841064453125, 2163.79541015625, 2163.74560546875, 2163.73681640625, 2163.57080078125, 2162.8076171875, 2164.001708984375, 2163.474365234375, 2163.2333984375, 2161.927734375, 2164.76416015625, 2162.647216796875, 2164.5986328125, 2165.8916015625, 2166.3017578125, 2165.9501953125, 2165.674560546875, 2164.98486328125, 2165.129638671875, 2164.795654296875, 2164.23876953125, 2164.4013671875, 2163.767822265625, 2162.8076171875, 2162.1376953125, 2162.045166015625, 2161.8671875, 2161.5205078125, 2161.645263671875, 2161.60107421875, 2160.984619140625, 2159.305908203125, 2159.961181640625, 2160.2470703125, 2159.45263671875, 2159.281005859375, 2157.9404296875, 2157.5673828125, 2156.2626953125, 2158.871826171875, 2157.56005859375, 2156.33447265625, 2154.78369140625, 2155.38037109375, 2155.9208984375, 2154.92578125, 2153.7919921875, 2154.6328125, 2152.546142578125, 2148.49267578125, 2150.68408203125, 2150.60791015625, 2152.56298828125, 2150.544921875, 2150.3203125, 2152.1904296875, 2155.790771484375, 2154.10107421875, 2152.36328125, 2158.22265625, 2162.736083984375, 2155.919189453125, 2154.0078125, 2155.4912109375, 2158.98779296875, 2160.335693359375, 2158.93505859375, 2160.145751953125, 2155.92822265625, 2160.74755859375, 2163.259765625, 2164.62744140625, 2163.2255859375, 2165.2578125, 2167.825927734375, 2164.8115234375, 2165.74560546875, 2165.64208984375, 2168.5830078125, 2168.214599609375, 2166.03271484375, 2167.02197265625, 2167.15966796875, 2167.302978515625, 2172.056396484375, 2167.7138671875, 2173.02294921875, 2169.6220703125, 2168.54833984375, 2171.390625, 2170.46630859375, 2164.844970703125, 2174.31494140625, 2175.6220703125, 2172.86083984375, 2170.991943359375, 2169.04736328125, 2169.653564453125, 2164.11669921875, 2162.7080078125, 2162.666259765625, 2162.478759765625, 2168.69287109375, 2163.380859375, 2155.540283203125, 2162.08154296875, 2161.8681640625, 2157.6962890625, 2158.4384765625, 2158.48388671875, 2159.372314453125, 2157.97705078125, 2159.974853515625, 2160.093994140625, 2163.396484375, 2160.072021484375, 2160.169921875, 2162.2919921875, 2163.34814453125, 2162.97998046875, 2159.99951171875, 2159.829833984375, 2161.896728515625, 2162.4921875, 2163.1875, 2161.482177734375, 2160.12060546875, 2160.70556640625, 2168.468994140625, 2167.46923828125, 2167.41259765625, 2167.189453125, 2167.44970703125, 2166.027587890625, 2166.40771484375, 2174.91748046875, 2164.34912109375, 2167.28662109375, 2167.36572265625, 2167.3603515625, 2162.5107421875, 2157.9384765625, 2158.08251953125, 2162.340576171875, 2160.31494140625, 2158.1650390625, 2163.87353515625, 2158.1005859375, 2159.9013671875, 2161.97265625, 2158.782470703125, 2163.136962890625, 2153.92822265625, 2157.771484375, 2160.456787109375, 2167.17529296875, 2165.31591796875, 2154.67626953125, 2176.015625, 2199.00634765625, 2187.79736328125, 2166.7197265625, 2161.171875, 2168.387939453125, 2153.28271484375, 2167.39404296875, 2171.994140625, 2161.20751953125, 2167.547607421875, 2167.398193359375, 2167.61279296875, 2149.3798828125, 2163.836669921875, 2169.5732421875, 2171.8408203125, 2172.720458984375, 2165.53955078125, 2165.208251953125, 2164.068603515625, 2160.856201171875, 2163.287353515625, 2158.8837890625, 2153.08349609375, 2156.12255859375, 2167.0966796875, 2175.124755859375, 2152.22314453125, 2154.389404296875, 2168.3779296875, 2174.42041015625, 2169.92431640625, 2164.048095703125, 2161.695068359375, 2161.817138671875, 2164.25732421875, 2168.958984375, 2169.9912109375, 2171.474609375, 2174.80126953125, 2170.518798828125, 2147.05908203125, 2170.619140625, 2175.2734375, 2176.711181640625, 2170.388916015625, 2166.5419921875, 2170.312255859375, 2170.21923828125, 2169.7724609375, 2168.510986328125, 2183.38134765625, 2182.431884765625, 2170.71875, 2164.95751953125, 2169.57958984375, 2160.24169921875, 2160.486572265625, 2163.08056640625, 2166.34765625, 2164.6357421875, 2165.6748046875, 2169.189453125, 2173.8720703125, 2171.44970703125, 2168.93359375, 2161.51318359375, 2163.584716796875, 2159.817138671875, 2157.57177734375, 2156.4140625, 2153.563720703125, 2153.945556640625, 2154.064697265625, 2154.01953125, 2154.13623046875, 2155.903076171875, 2154.34033203125, 2154.47705078125, 2150.198974609375, 2149.3232421875, 2154.77880859375, 2160.10595703125, 2156.78564453125, 2154.61474609375, 2159.41259765625, 2158.86669921875, 2161.5068359375, 2149.752685546875, 2140.27294921875, 2142.1259765625, 2155.159423828125, 2158.40380859375, 2162.5341796875, 2157.398193359375, 2147.9072265625, 2148.92578125, 2149.1279296875, 2150.75830078125, 2157.99267578125, 2146.0634765625, 2155.09228515625, 2154.11669921875, 2163.74609375, 2154.626708984375, 2143.3115234375, 2152.116943359375, 2165.439453125, 2162.231689453125, 2157.95361328125, 2158.1162109375, 2167.90283203125, 2175.39013671875, 2163.26708984375, 2155.723876953125, 2147.56103515625, 2145.8994140625, 2151.059814453125, 2150.740478515625, 2153.72705078125, 2155.989990234375, 2156.6474609375, 2162.04248046875, 2159.078369140625, 2152.447265625, 2147.0390625, 2160.275146484375, 2160.862060546875, 2154.45166015625, 2150.286865234375, 2155.14208984375, 2154.5595703125, 2153.996337890625, 2154.189453125, 2154.363525390625, 2151.588623046875, 2148.466796875, 2150.09912109375, 2152.17724609375, 2150.39111328125, 2151.595703125, 2156.55712890625, 2153.00732421875, 2154.152099609375, 2155.60595703125, 2155.93408203125, 2156.02685546875, 2156.20703125, 2157.7646484375, 2161.8798828125, 2162.88818359375, 2160.245361328125, 2153.2939453125, 2167.49755859375, 2165.422607421875, 2166.10546875, 2165.40185546875, 2169.89892578125, 2181.8798828125, 2171.10546875, 2167.4951171875, 2168.0283203125, 2169.06787109375, 2172.091796875, 2183.00146484375, 2172.26220703125, 2167.126953125, 2155.303955078125, 2163.798828125, 2182.70654296875, 2162.899169921875, 2159.1201171875, 2162.33203125, 2175.5615234375, 2177.302490234375, 2158.07958984375] validation total loss: [2401.022216796875, 2395.91943359375, 2360.870361328125, 2291.99169921875, 2223.96923828125, 2194.0166015625, 2181.37841796875, 2178.859375, 2181.74169921875, 2180.48828125, 2179.406494140625, 2171.0419921875, 2172.80078125, 2170.743896484375, 2168.02392578125, 2166.317138671875, 2167.736572265625, 2165.26318359375, 2165.878173828125, 2165.894775390625, 2173.274169921875, 2159.0947265625, 2188.244384765625, 2183.120849609375, 2152.1826171875, 2174.022216796875, 2173.31298828125, 2161.423583984375, 2167.689697265625, 2173.357421875, 2162.745849609375, 2165.146240234375, 2177.43212890625, 2159.837158203125, 2172.85693359375, 2172.67724609375, 2160.26318359375, 2170.370361328125, 2174.114013671875, 2159.966552734375, 2182.833740234375, 2186.450439453125, 2166.377197265625, 2163.417236328125, 2177.927001953125, 2167.268798828125, 2167.640380859375, 2172.6650390625, 2164.0419921875, 2172.392333984375, 2162.9755859375, 2177.262451171875, 2156.558837890625, 2185.300537109375, 2194.07275390625, 2176.761962890625, 2149.574462890625, 2167.0625, 2176.850341796875, 2163.771728515625, 2183.109375, 2165.154052734375, 2164.048828125, 2168.0029296875, 2170.04443359375, 2165.789306640625, 2172.872802734375, 2146.604248046875, 2186.260009765625, 2168.524169921875, 2149.45068359375, 2166.2236328125, 2169.4658203125, 2163.44482421875, 2163.724853515625, 2164.117431640625, 2158.960205078125, 2166.685791015625, 2155.647705078125, 2166.68896484375, 2158.771484375, 2160.096923828125, 2170.945556640625, 2155.559814453125, 2173.158935546875, 2169.86962890625, 2150.965087890625, 2167.806396484375, 2173.192138671875, 2155.97509765625, 2162.290771484375, 2168.00537109375, 2153.38427734375, 2169.738525390625, 2171.342529296875, 2158.5830078125, 2161.8662109375, 2167.387939453125, 2164.0517578125, 2161.781982421875, 2175.0283203125, 2162.4072265625, 2158.056884765625, 2171.873046875, 2160.430419921875, 2154.413818359375, 2168.008544921875, 2167.92138671875, 2157.629150390625, 2173.48046875, 2159.439208984375, 2164.91259765625, 2169.496826171875, 2158.854736328125, 2161.75390625, 2159.8916015625, 2160.645263671875, 2160.06298828125, 2162.498779296875, 2164.357177734375, 2157.30517578125, 2162.90576171875, 2160.143798828125, 2161.991455078125, 2159.4375, 2160.090087890625, 2158.26025390625, 2151.437744140625, 2166.11767578125, 2152.808349609375, 2158.143310546875, 2155.164794921875, 2155.361083984375, 2158.9267578125, 2157.606201171875, 2159.916015625, 2154.86962890625, 2159.03955078125, 2165.130126953125, 2160.515625, 2156.348388671875, 2156.890380859375, 2157.673828125, 2160.733154296875, 2163.309814453125, 2161.861572265625, 2161.881103515625, 2160.946044921875, 2159.7919921875, 2159.21923828125, 2159.326416015625, 2159.431884765625, 2159.964111328125, 2159.3369140625, 2159.444580078125, 2159.296875, 2159.301025390625, 2161.600341796875, 2158.476318359375, 2160.170166015625, 2161.997802734375, 2159.305419921875, 2164.867919921875, 2149.671630859375, 2162.23779296875, 2162.87939453125, 2155.968505859375, 2165.20703125, 2159.528076171875, 2168.566650390625, 2161.817626953125, 2165.011474609375, 2161.583740234375, 2156.46630859375, 2171.062744140625, 2149.477783203125, 2162.596435546875, 2165.106201171875, 2159.671142578125, 2157.661376953125, 2162.978271484375, 2161.83642578125, 2161.3134765625, 2163.240478515625, 2161.421142578125, 2159.705078125, 2161.285400390625, 2174.5224609375, 2158.563232421875, 2157.962646484375, 2164.08935546875, 2160.781982421875, 2157.93994140625, 2162.475341796875, 2170.335693359375, 2151.396484375, 2164.647705078125, 2171.107421875, 2158.318115234375, 2162.308349609375, 2167.513916015625, 2168.05712890625, 2164.216552734375, 2160.077880859375, 2161.067138671875, 2166.097412109375, 2161.157470703125, 2165.471435546875, 2165.472900390625, 2164.916259765625, 2166.52294921875, 2165.543701171875, 2163.830078125, 2161.7255859375, 2161.807861328125, 2165.63427734375, 2170.62060546875, 2162.504638671875, 2163.489013671875, 2166.364990234375, 2165.710205078125, 2168.976318359375, 2169.6513671875, 2169.695556640625, 2163.004150390625, 2172.16552734375, 2175.640380859375, 2166.594970703125, 2173.427978515625, 2172.918701171875, 2162.019287109375, 2171.781005859375, 2180.659912109375, 2158.586181640625, 2159.468017578125, 2161.593505859375, 2169.662841796875, 2165.030517578125, 2164.409912109375, 2168.242919921875, 2167.520263671875, 2167.453369140625, 2165.223876953125, 2165.210693359375, 2165.467529296875, 2168.344482421875, 2167.369873046875, 2167.307373046875, 2165.3203125, 2165.537353515625, 2166.604248046875, 2165.802734375, 2168.36279296875, 2167.55615234375, 2167.43701171875, 2167.506591796875, 2167.24365234375, 2167.0947265625, 2167.2236328125, 2167.182861328125, 2167.471435546875, 2167.40478515625, 2167.421630859375, 2167.4306640625, 2167.44970703125, 2167.372314453125, 2167.334716796875, 2167.373291015625, 2167.303955078125, 2167.566162109375, 2180.896240234375, 2168.400146484375, 2149.51123046875, 2165.878173828125, 2166.3603515625, 2167.5625, 2173.48828125, 2167.525390625, 2165.57177734375, 2165.36083984375, 2158.8173828125, 2165.412353515625, 2165.425537109375, 2163.3701171875, 2169.04248046875, 2169.321533203125, 2161.18359375, 2161.453369140625, 2161.748291015625, 2161.404296875, 2162.248046875, 2172.89697265625, 2173.5634765625, 2159.335205078125, 2161.45654296875, 2167.251220703125, 2174.492919921875, 2159.974365234375, 2167.053466796875, 2167.1083984375, 2163.526611328125, 2166.0, 2166.400390625, 2163.518310546875, 2161.741455078125, 2161.59765625, 2163.707763671875, 2163.42626953125, 2163.551025390625, 2165.689453125, 2165.427490234375, 2162.974609375, 2165.430419921875, 2165.784423828125, 2165.2646484375, 2163.304931640625, 2163.115966796875, 2163.381591796875, 2163.3642578125, 2161.577880859375, 2163.421142578125, 2163.945556640625, 2165.334228515625, 2160.24169921875, 2163.67822265625, 2167.711669921875, 2162.048828125, 2163.653076171875, 2161.719970703125, 2162.5556640625, 2165.406005859375, 2165.523681640625, 2165.514404296875, 2165.55908203125, 2165.755859375, 2166.63623046875, 2165.866943359375, 2165.40966796875, 2165.373046875, 2165.383544921875, 2165.408203125, 2165.54345703125, 2166.39501953125, 2165.44970703125, 2165.44482421875, 2165.41455078125, 2165.422607421875, 2165.454345703125, 2165.41015625, 2165.3828125, 2165.366455078125, 2165.44287109375, 2165.830078125, 2166.546875, 2167.369384765625, 2166.095947265625, 2165.351318359375, 2163.619873046875, 2162.88134765625, 2167.49169921875, 2178.04345703125, 2169.495361328125, 2155.391845703125, 2161.5419921875, 2178.10009765625, 2161.69287109375, 2157.51318359375, 2156.08251953125, 2163.73583984375, 2167.07421875, 2163.71240234375, 2162.860107421875, 2164.04248046875, 2161.031494140625, 2161.79150390625, 2162.57763671875, 2159.54736328125, 2159.7275390625, 2159.707275390625, 2165.4501953125, 2157.458984375, 2158.70751953125, 2161.359375, 2165.404052734375, 2169.09326171875, 2159.82275390625, 2155.849853515625, 2156.149658203125, 2161.37060546875, 2161.161865234375, 2161.378173828125, 2161.49560546875, 2161.495361328125, 2161.4326171875, 2161.677734375, 2157.91650390625, 2161.80615234375, 2162.347412109375, 2163.26123046875, 2153.156982421875, 2159.36376953125, 2161.40869140625, 2166.24365234375, 2161.318359375, 2161.091796875, 2165.3984375, 2164.93408203125, 2164.73486328125, 2161.406005859375, 2161.5263671875, 2165.355712890625, 2163.7451171875, 2157.306640625, 2161.314697265625, 2161.483642578125, 2161.308349609375, 2159.776123046875, 2161.320068359375, 2160.151611328125, 2159.42822265625, 2160.876220703125, 2161.371337890625, 2161.607177734375, 2165.07861328125, 2168.2998046875, 2163.037109375, 2161.25634765625, 2168.120849609375, 2167.817138671875, 2159.332763671875, 2161.295166015625, 2159.905029296875, 2160.955810546875, 2154.151611328125, 2157.893310546875, 2163.45654296875, 2163.59228515625, 2163.372802734375, 2163.450927734375, 2165.376708984375, 2163.67822265625, 2166.017578125, 2169.3544921875, 2165.396484375, 2164.927001953125, 2164.71044921875, 2165.5009765625, 2165.548095703125, 2165.450927734375, 2165.477783203125, 2165.4931640625, 2165.40283203125, 2165.3603515625, 2165.322021484375, 2165.18359375, 2165.369384765625, 2161.26123046875, 2158.574951171875, 2167.4365234375, 2168.54638671875, 2167.386474609375, 2167.36669921875, 2165.49267578125, 2165.376708984375, 2165.3369140625, 2163.298828125, 2163.3271484375, 2163.342529296875, 2165.57177734375, 2163.20654296875, 2151.8212890625, 2161.44921875, 2182.030029296875, 2162.668212890625, 2157.068359375, 2165.37353515625, 2167.87548828125, 2162.0595703125, 2156.82568359375, 2154.48388671875, 2158.881591796875, 2164.457763671875, 2157.990234375, 2151.875732421875, 2153.599365234375, 2163.67919921875, 2171.488525390625, 2161.13818359375, 2162.10302734375, 2160.404296875, 2159.830078125, 2171.58203125, 2171.59814453125, 2164.729248046875, 2161.805419921875, 2171.844482421875, 2169.64599609375, 2162.36767578125, 2155.419677734375, 2166.604736328125, 2175.23974609375, 2160.400146484375, 2155.850341796875, 2164.27490234375, 2166.636962890625, 2162.47802734375, 2161.410888671875, 2158.347900390625, 2160.822509765625, 2161.867919921875, 2161.455322265625, 2165.759033203125, 2159.077880859375, 2158.1376953125, 2161.636474609375, 2166.00048828125, 2168.332275390625, 2171.781982421875, 2169.40380859375, 2164.72412109375, 2162.4072265625, 2162.065673828125, 2164.8525390625, 2168.36865234375, 2165.412109375, 2166.98779296875, 2167.611572265625, 2166.503173828125, 2164.88427734375, 2166.77197265625, 2162.141357421875, 2159.3076171875, 2152.84716796875, 2162.7685546875, 2169.831787109375, 2163.150634765625, 2165.097900390625, 2169.6064453125, 2171.00341796875, 2169.218017578125, 2165.1279296875, 2170.15771484375, 2167.1953125, 2161.5576171875, 2155.429931640625, 2159.712158203125, 2166.513671875, 2161.37109375, 2161.782958984375, 2161.414306640625, 2161.802490234375, 2165.744873046875, 2165.236572265625, 2165.566162109375, 2163.2978515625, 2157.485595703125, 2157.361572265625, 2157.3408203125, 2157.026123046875, 2157.4609375, 2157.4423828125, 2157.391357421875, 2157.39306640625, 2157.42626953125, 2157.46337890625, 2157.508544921875, 2157.45751953125, 2157.44384765625, 2157.46435546875, 2157.573974609375, 2157.8935546875, 2160.98388671875, 2161.623046875, 2161.42041015625, 2159.1748046875, 2159.318603515625, 2159.4345703125, 2167.07568359375, 2173.538818359375, 2167.178955078125, 2161.065185546875, 2152.262939453125, 2163.927490234375, 2165.452392578125, 2168.393798828125, 2165.541015625, 2165.330810546875, 2160.991943359375, 2163.177001953125, 2163.4423828125, 2171.4140625, 2155.628662109375, 2147.35400390625, 2145.86328125, 2141.701904296875, 2151.385986328125, 2151.380126953125, 2151.425537109375, 2151.51806640625, 2153.72119140625, 2158.019287109375, 2163.327880859375, 2161.39501953125, 2161.206298828125, 2155.44482421875, 2166.004150390625, 2167.460693359375, 2168.92431640625, 2170.423828125, 2161.66748046875, 2169.368896484375, 2169.946533203125, 2170.876220703125, 2167.692138671875, 2167.667236328125, 2171.001953125, 2169.735595703125, 2164.825439453125, 2162.980224609375, 2165.290771484375, 2169.609130859375, 2176.69091796875, 2181.064697265625, 2178.951904296875, 2174.417236328125, 2174.0068359375, 2175.906005859375, 2177.56103515625, 2177.6572265625, 2174.15234375, 2171.4423828125, 2169.125244140625, 2169.76806640625, 2171.344482421875, 2171.057373046875, 2171.684814453125, 2172.1064453125, 2174.989013671875, 2180.430908203125, 2172.857177734375, 2173.819091796875, 2164.490478515625, 2169.3271484375, 2167.71533203125, 2171.93359375, 2173.043212890625, 2171.805908203125, 2170.776611328125, 2163.9873046875, 2164.237060546875, 2169.724853515625, 2169.350341796875, 2167.334228515625, 2167.543212890625, 2167.68505859375, 2169.236572265625, 2167.5224609375, 2166.034423828125, 2167.661865234375, 2167.338134765625, 2167.970703125, 2166.459716796875, 2166.412841796875, 2166.462646484375, 2165.34619140625, 2165.228515625, 2165.182861328125, 2165.133056640625, 2165.124267578125, 2164.958251953125, 2164.195068359375, 2165.38916015625, 2164.86181640625, 2164.620849609375, 2163.315185546875, 2166.151611328125, 2164.03466796875, 2165.986083984375, 2167.279052734375, 2167.689208984375, 2167.337646484375, 2167.06201171875, 2166.372314453125, 2166.51708984375, 2166.18310546875, 2165.626220703125, 2165.788818359375, 2165.1552734375, 2164.195068359375, 2163.525146484375, 2163.4326171875, 2163.254638671875, 2162.907958984375, 2163.03271484375, 2162.988525390625, 2162.3720703125, 2160.693359375, 2161.3486328125, 2161.634521484375, 2160.840087890625, 2160.66845703125, 2159.327880859375, 2158.954833984375, 2157.650146484375, 2160.25927734375, 2158.947509765625, 2157.721923828125, 2156.171142578125, 2156.767822265625, 2157.308349609375, 2156.313232421875, 2155.179443359375, 2156.020263671875, 2153.93359375, 2149.88037109375, 2152.071533203125, 2151.995361328125, 2153.950439453125, 2151.932373046875, 2151.707763671875, 2153.577880859375, 2157.17822265625, 2155.488525390625, 2153.750732421875, 2159.610107421875, 2164.12353515625, 2157.306640625, 2155.395263671875, 2156.87841796875, 2160.375, 2161.722900390625, 2160.322265625, 2161.532958984375, 2157.315673828125, 2162.134765625, 2164.64697265625, 2166.0146484375, 2164.61279296875, 2166.64501953125, 2169.213134765625, 2166.19873046875, 2167.1328125, 2167.029296875, 2169.97021484375, 2169.60205078125, 2167.420166015625, 2168.409423828125, 2168.547119140625, 2168.6904296875, 2173.44384765625, 2169.101318359375, 2174.410400390625, 2171.009521484375, 2169.93603515625, 2172.7783203125, 2171.853759765625, 2166.232421875, 2175.70263671875, 2177.009765625, 2174.24853515625, 2172.379638671875, 2170.434814453125, 2171.041015625, 2165.50439453125, 2164.095703125, 2164.053955078125, 2163.866455078125, 2170.08056640625, 2164.7685546875, 2156.927978515625, 2163.46923828125, 2163.255859375, 2159.083984375, 2159.826171875, 2159.87158203125, 2160.760009765625, 2159.36474609375, 2161.362548828125, 2161.481689453125, 2164.7841796875, 2161.459716796875, 2161.5576171875, 2163.6796875, 2164.73583984375, 2164.36767578125, 2161.387451171875, 2161.2177734375, 2163.28466796875, 2163.880126953125, 2164.575439453125, 2162.8701171875, 2161.508544921875, 2162.093505859375, 2169.85693359375, 2168.857177734375, 2168.800537109375, 2168.577392578125, 2168.837646484375, 2167.41552734375, 2167.795654296875, 2176.305419921875, 2165.7373046875, 2168.674560546875, 2168.753662109375, 2168.748291015625, 2163.898681640625, 2159.32666015625, 2159.470703125, 2163.728759765625, 2161.703125, 2159.55322265625, 2165.26171875, 2159.48876953125, 2161.28955078125, 2163.361083984375, 2160.1708984375, 2164.525146484375, 2155.31640625, 2159.15966796875, 2161.844970703125, 2168.5634765625, 2166.7041015625, 2156.064453125, 2177.40380859375, 2200.39453125, 2189.185546875, 2168.10791015625, 2162.56005859375, 2169.77587890625, 2154.6708984375, 2168.781982421875, 2173.382080078125, 2162.595703125, 2168.935791015625, 2168.786376953125, 2169.0009765625, 2150.76806640625, 2165.224609375, 2170.961181640625, 2173.228759765625, 2174.1083984375, 2166.927734375, 2166.596435546875, 2165.456787109375, 2162.244140625, 2164.67529296875, 2160.27197265625, 2154.4716796875, 2157.510498046875, 2168.48486328125, 2176.5126953125, 2153.611328125, 2155.77783203125, 2169.76611328125, 2175.80859375, 2171.3125, 2165.436279296875, 2163.083251953125, 2163.205322265625, 2165.645751953125, 2170.347412109375, 2171.379638671875, 2172.863037109375, 2176.189453125, 2171.906982421875, 2148.447509765625, 2172.007568359375, 2176.66162109375, 2178.09912109375, 2171.77685546875, 2167.929931640625, 2171.699951171875, 2171.607177734375, 2171.160400390625, 2169.89892578125, 2184.76953125, 2183.820068359375, 2172.10693359375, 2166.345703125, 2170.9677734375, 2161.6298828125, 2161.874755859375, 2164.46875, 2167.73583984375, 2166.02392578125, 2167.06298828125, 2170.57763671875, 2175.260009765625, 2172.837646484375, 2170.32177734375, 2162.9013671875, 2164.97265625, 2161.205078125, 2158.9599609375, 2157.80224609375, 2154.951904296875, 2155.333740234375, 2155.452880859375, 2155.40771484375, 2155.5244140625, 2157.291259765625, 2155.728515625, 2155.865234375, 2151.587158203125, 2150.71142578125, 2156.1669921875, 2161.494140625, 2158.173828125, 2156.0029296875, 2160.80078125, 2160.2548828125, 2162.89501953125, 2151.140869140625, 2141.6611328125, 2143.51416015625, 2156.54736328125, 2159.791748046875, 2163.922119140625, 2158.786376953125, 2149.29541015625, 2150.314208984375, 2150.51611328125, 2152.146484375, 2159.380859375, 2147.45166015625, 2156.480224609375, 2155.504638671875, 2165.1337890625, 2156.014404296875, 2144.699462890625, 2153.5048828125, 2166.8271484375, 2163.619384765625, 2159.34130859375, 2159.50390625, 2169.290771484375, 2176.778076171875, 2164.655029296875, 2157.11181640625, 2148.948974609375, 2147.287353515625, 2152.447509765625, 2152.128173828125, 2155.11474609375, 2157.37744140625, 2158.034912109375, 2163.429931640625, 2160.4658203125, 2153.8349609375, 2148.4267578125, 2161.662841796875, 2162.249755859375, 2155.83935546875, 2151.674560546875, 2156.52978515625, 2155.947265625, 2155.384033203125, 2155.5771484375, 2155.751220703125, 2152.976318359375, 2149.8544921875, 2151.48681640625, 2153.56494140625, 2151.779052734375, 2152.983642578125, 2157.945068359375, 2154.395263671875, 2155.539794921875, 2156.99365234375, 2157.322021484375, 2157.414794921875, 2157.594970703125, 2159.152587890625, 2163.267822265625, 2164.276123046875, 2161.63330078125, 2154.681884765625, 2168.88525390625, 2166.810302734375, 2167.4931640625, 2166.78955078125, 2171.28662109375, 2183.267333984375, 2172.4931640625, 2168.8828125, 2169.416015625, 2170.455810546875, 2173.479736328125, 2184.389404296875, 2173.650390625, 2168.515380859375, 2156.6923828125, 2165.187255859375, 2184.0947265625, 2164.287353515625, 2160.508056640625, 2163.719970703125, 2176.94921875, 2178.690185546875, 2159.467529296875] validation accuracy: [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wqi0k7au) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>█▆▅▄▃▃▄▁▃▃▂▆▄▄▃▆▅▄▃▄▆▆▅▆▄▃▄▆▄▃▆▆▄▃▃▃▆▂▂▄</td></tr><tr><td>total train loss</td><td>█▇█▆▅▅▄▄▄▄▄▄▄▄▄▄▄▄▃▄▃▄▃▄▄▃▃▃▂▂▂▁▂▂▁▁▁▁▁▂</td></tr><tr><td>total validation loss</td><td>█▅█▅▆▂▄▄▅▆▅▅▄▅▅▅▅▅▅▄▅▄▃▂▆▇▅▅▂▆▆▄▄▇▆▂▁▂▂▃</td></tr><tr><td>train label accuracy</td><td>█████████▇██▇█▃▄▃▄▄▁▂▂▂▃▁▂▃▂▄███▇█▇▅▅▄▅▇</td></tr><tr><td>train label loss</td><td>▂▆▅▄▄▅▅▂▄▆▆▅▅▆▅█▄▄▅▄▂▆▂▇█▇▆▇▅▅▁▃▆▅▅▆▆▆▅▅</td></tr><tr><td>train triplet loss</td><td>█▇█▆▅▅▄▄▄▄▄▄▄▄▄▄▄▄▃▄▃▄▃▄▄▃▃▃▂▂▂▁▂▂▁▁▁▁▁▂</td></tr><tr><td>validation label accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation label loss</td><td>▁▂▂▁▂▂▂▁▂▂▂▃▂▃▃▃▃▃▃▃▃▃▄▃▄▄▄▄▄▃▅▆▇▆▅▆▇▅▅█</td></tr><tr><td>validation triplet loss</td><td>█▅█▅▆▂▄▄▅▆▅▅▄▅▅▅▅▅▅▄▅▄▃▂▆▇▅▅▂▆▆▄▄▇▆▂▁▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>0.34167</td></tr><tr><td>total train loss</td><td>6227.98104</td></tr><tr><td>total validation loss</td><td>2159.46753</td></tr><tr><td>train label accuracy</td><td>0.23472</td></tr><tr><td>train label loss</td><td>1.3868</td></tr><tr><td>train triplet loss</td><td>6226.59436</td></tr><tr><td>validation label accuracy</td><td>0.25</td></tr><tr><td>validation label loss</td><td>1.38794</td></tr><tr><td>validation triplet loss</td><td>2158.07959</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wild-sky-478</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/wqi0k7au' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/wqi0k7au</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230419_171001-wqi0k7au/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wqi0k7au). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20230419_171528-fq5zzuzg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/fq5zzuzg' target=\"_blank\">generous-terrain-479</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/fq5zzuzg' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/fq5zzuzg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:57<00:00,  3.37it/s]\n",
      " 67%|██████▋   | 2/3 [10:39<05:18, 318.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [2399.879150390625, 2399.583984375, 2397.556640625, 2393.011474609375, 2386.93994140625, 2371.25390625, 2367.41943359375, 2358.211181640625, 2363.46875, 2371.2705078125, 2374.315185546875, 2378.244384765625, 2384.109130859375, 2385.0048828125, 2387.2265625, 2388.68505859375, 2388.7041015625, 2389.57275390625, 2388.594970703125, 2387.93505859375, 2386.35791015625, 2385.072998046875, 2383.437744140625, 2382.968017578125, 2382.152099609375, 2382.34423828125, 2382.568359375, 2381.978515625, 2381.42431640625, 2380.91064453125, 2380.1123046875, 2379.7939453125, 2379.096923828125, 2379.275634765625, 2379.138916015625, 2379.33642578125, 2379.09765625, 2378.8408203125, 2378.02099609375, 2378.996826171875, 2378.05078125, 2378.39208984375, 2377.77587890625, 2378.09423828125, 2377.730224609375, 2378.89990234375, 2378.463134765625, 2378.232421875, 2378.0908203125, 2377.720703125, 2377.4482421875, 2377.029296875, 2376.7041015625, 2377.554443359375, 2376.90771484375, 2376.60595703125, 2376.2138671875, 2375.80419921875, 2376.013916015625, 2376.119873046875, 2375.77783203125, 2375.931396484375, 2375.39794921875, 2375.102783203125, 2374.95361328125, 2374.759033203125, 2375.07080078125, 2374.837646484375, 2374.696044921875, 2374.97412109375, 2374.69873046875, 2374.27587890625, 2374.0439453125, 2373.568603515625, 2372.972412109375, 2372.899169921875, 2372.66015625, 2371.897216796875, 2371.94921875, 2372.36767578125, 2372.79248046875, 2375.2626953125, 2377.057373046875, 2374.78515625, 2374.48779296875, 2372.69091796875, 2372.79833984375, 2369.619140625, 2365.285888671875, 2363.76611328125, 2364.171142578125, 2366.0, 2366.5439453125, 2366.1494140625, 2365.99951171875, 2364.58935546875, 2364.1005859375, 2363.58642578125, 2363.0166015625, 2362.829833984375, 2362.86572265625, 2362.95751953125, 2363.05322265625, 2363.13134765625, 2362.987548828125, 2362.95751953125, 2362.80615234375, 2362.656005859375, 2362.52001953125, 2362.388916015625, 2362.154541015625, 2361.90380859375, 2361.690673828125, 2361.58837890625, 2361.4697265625, 2361.46240234375, 2361.4384765625, 2361.5244140625, 2361.57177734375, 2361.59326171875, 2361.66650390625, 2361.74951171875, 2361.83740234375, 2362.121826171875, 2362.2431640625, 2362.2216796875, 2362.1103515625, 2361.965087890625, 2361.8740234375, 2361.78857421875, 2361.6650390625, 2361.696044921875, 2361.869140625, 2361.8681640625, 2361.8994140625, 2362.26611328125, 2362.6708984375, 2362.664794921875, 2362.49951171875, 2362.40234375, 2362.3779296875, 2362.3310546875, 2362.2421875, 2362.153564453125, 2362.00927734375, 2361.874755859375, 2361.79833984375, 2361.869384765625, 2361.84814453125, 2361.82177734375, 2361.83837890625, 2361.8193359375, 2361.75439453125, 2361.728271484375, 2361.841064453125, 2361.75732421875, 2361.765625, 2361.986083984375, 2362.21240234375, 2362.564208984375, 2362.894775390625, 2363.061767578125, 2362.908935546875, 2362.583984375, 2362.322265625, 2362.0439453125, 2361.817138671875, 2361.498046875, 2361.0546875, 2360.877197265625, 2360.96142578125, 2361.0166015625, 2360.796875, 2360.9169921875, 2361.440185546875, 2361.93896484375, 2362.1142578125, 2362.284423828125, 2362.485595703125, 2363.2158203125, 2363.373046875, 2363.30517578125, 2363.2783203125, 2362.767578125, 2362.671875, 2363.31591796875, 2363.5146484375, 2363.3955078125, 2363.18212890625, 2363.2626953125, 2362.86181640625, 2362.533203125, 2362.52392578125, 2362.83642578125, 2362.82861328125, 2362.306640625, 2362.21826171875, 2362.361328125, 2362.57763671875, 2362.82421875, 2362.93505859375, 2362.9296875, 2362.716064453125, 2362.575439453125, 2362.4833984375, 2362.47412109375, 2362.267822265625, 2361.9794921875, 2362.081298828125, 2362.144775390625, 2362.064208984375, 2362.367919921875, 2362.74169921875, 2362.757080078125, 2362.583740234375, 2362.522216796875, 2362.591064453125, 2362.86962890625, 2363.0849609375, 2363.203125, 2362.941162109375, 2362.50830078125, 2362.337890625, 2362.525146484375, 2362.931884765625, 2363.397216796875, 2363.688720703125, 2363.58203125, 2363.39111328125, 2363.40380859375, 2363.3798828125, 2363.39208984375, 2363.387939453125, 2363.6181640625, 2363.45263671875, 2363.01806640625, 2362.71875, 2362.75146484375, 2362.89990234375, 2362.99951171875, 2363.163818359375, 2363.238525390625, 2363.218017578125, 2363.09521484375, 2363.022216796875, 2362.80859375, 2362.6845703125, 2362.684326171875, 2362.56103515625, 2362.3740234375, 2362.48828125, 2362.7548828125, 2362.79443359375, 2362.861328125, 2363.012451171875, 2362.968017578125, 2363.06005859375, 2363.0537109375, 2362.90625, 2362.8408203125, 2362.79736328125, 2362.75048828125, 2362.85009765625, 2362.76318359375, 2362.72705078125, 2362.73828125, 2362.7392578125, 2362.7626953125, 2362.783203125, 2362.73681640625, 2362.74072265625, 2362.55419921875, 2363.071044921875, 2364.199462890625, 2364.2880859375, 2364.5634765625, 2364.979736328125, 2365.3544921875, 2364.888916015625, 2364.12939453125, 2364.04248046875, 2364.6650390625, 2365.109619140625, 2365.436767578125, 2365.81982421875, 2366.02490234375, 2366.17626953125, 2366.14111328125, 2365.98046875, 2365.65380859375, 2365.361328125, 2365.23388671875, 2365.103271484375, 2365.02783203125, 2365.12060546875, 2365.16357421875, 2364.7197265625, 2364.57177734375, 2364.578125, 2364.73046875, 2364.708984375, 2364.6728515625, 2364.583984375, 2364.5244140625, 2364.443603515625, 2364.32421875, 2364.304443359375, 2364.2099609375, 2364.142822265625, 2364.186279296875, 2364.22802734375, 2364.1494140625, 2364.061279296875, 2364.074951171875, 2364.010498046875, 2364.0634765625, 2363.86083984375, 2363.793212890625, 2363.75244140625, 2363.64208984375, 2363.634765625, 2363.58984375, 2363.5234375, 2363.4453125, 2363.35400390625, 2363.33349609375, 2363.404541015625, 2363.456298828125, 2363.494873046875, 2363.6083984375, 2363.5810546875, 2363.613037109375, 2363.65234375, 2363.568359375, 2363.46484375, 2363.46630859375, 2363.4033203125, 2363.37744140625, 2363.36181640625, 2363.3525390625, 2363.31884765625, 2363.36669921875, 2363.413330078125, 2363.39990234375, 2363.36181640625, 2363.3876953125, 2363.22900390625, 2362.7470703125, 2362.52392578125, 2362.51171875, 2362.58642578125, 2362.709228515625, 2362.825927734375, 2362.86962890625, 2363.00048828125, 2363.1337890625, 2363.221923828125, 2363.25341796875, 2363.3251953125, 2363.39013671875, 2363.3623046875, 2363.404541015625, 2363.4208984375, 2363.38671875, 2363.32958984375, 2363.2685546875, 2363.179443359375, 2363.138427734375, 2363.14208984375, 2363.1357421875, 2363.208984375, 2363.245361328125, 2363.240234375, 2363.13720703125, 2363.1181640625, 2363.12060546875, 2363.09228515625, 2363.115234375, 2363.11083984375, 2363.146240234375, 2363.158447265625, 2363.142822265625, 2363.087158203125, 2363.04541015625, 2363.111328125, 2363.0498046875, 2363.01513671875, 2362.9951171875, 2362.88134765625, 2362.85791015625, 2362.8466796875, 2362.800537109375, 2362.81103515625, 2362.849609375, 2362.81689453125, 2362.7822265625, 2362.81591796875, 2362.82958984375, 2362.876953125, 2362.888427734375, 2362.7509765625, 2362.732177734375, 2362.6748046875, 2362.720703125, 2362.75537109375, 2362.76025390625, 2362.791748046875, 2362.80322265625, 2362.83935546875, 2362.808837890625, 2362.83740234375, 2362.73828125, 2362.65673828125, 2362.5634765625, 2362.552734375, 2362.60986328125, 2362.63134765625, 2362.6455078125, 2362.655517578125, 2362.60986328125, 2362.640625, 2362.596435546875, 2362.64990234375, 2362.58740234375, 2362.5771484375, 2362.57421875, 2362.66943359375, 2362.689208984375, 2362.62646484375, 2362.58935546875, 2362.56005859375, 2362.538330078125, 2362.703857421875, 2363.236328125, 2363.427490234375, 2363.07861328125, 2362.62109375, 2362.5205078125, 2362.5419921875, 2362.677734375, 2362.92578125, 2363.36279296875, 2363.9775390625, 2364.462890625, 2364.3408203125, 2364.107421875, 2364.126953125, 2364.208984375, 2364.31884765625, 2364.5751953125, 2364.93310546875, 2365.361083984375, 2365.644287109375, 2365.82470703125, 2365.83251953125, 2365.525390625, 2365.28857421875, 2365.02978515625, 2364.809326171875, 2364.61376953125, 2364.45849609375, 2364.2255859375, 2363.988525390625, 2363.8251953125, 2363.73291015625, 2363.691162109375, 2363.6484375, 2363.658203125, 2363.603515625, 2363.598876953125, 2363.59912109375, 2363.61767578125, 2363.708984375, 2363.740234375, 2363.72265625, 2363.7587890625, 2363.76904296875, 2363.775146484375, 2363.790771484375, 2363.75830078125, 2363.390625, 2362.70751953125, 2362.29931640625, 2362.374267578125, 2362.64990234375, 2363.023193359375, 2363.25830078125, 2363.3818359375, 2363.49951171875, 2363.614990234375, 2363.677978515625, 2363.755859375, 2363.8349609375, 2363.79345703125, 2363.806640625, 2363.81494140625, 2363.6025390625, 2363.3955078125, 2363.297119140625, 2363.236328125, 2363.216796875, 2363.176025390625, 2363.20654296875, 2363.261962890625, 2363.28955078125, 2363.33203125, 2363.376953125, 2363.3818359375, 2363.3857421875, 2363.470458984375, 2363.454345703125, 2363.48046875, 2363.46630859375, 2363.509765625, 2363.447998046875, 2363.434814453125, 2363.33447265625, 2363.30859375, 2363.287109375, 2363.312255859375, 2363.3134765625, 2363.26025390625, 2363.30126953125, 2363.2548828125, 2363.25048828125, 2363.274658203125, 2363.24755859375, 2363.240478515625, 2363.205810546875, 2363.14208984375, 2363.05712890625, 2363.026611328125, 2362.978515625, 2362.9765625, 2362.99365234375, 2362.975341796875, 2362.943359375, 2362.957275390625, 2362.953125, 2362.966552734375, 2362.9365234375, 2362.97412109375, 2362.96533203125, 2363.00244140625, 2363.0322265625, 2363.0390625, 2363.01220703125, 2363.010498046875, 2363.01611328125, 2363.0205078125, 2363.0126953125, 2363.040771484375, 2362.8837890625, 2362.806640625, 2362.71826171875, 2362.7001953125, 2362.728759765625, 2362.750732421875, 2362.72265625, 2362.779296875, 2362.77294921875, 2362.82275390625, 2362.70849609375, 2362.705078125, 2362.74169921875, 2362.770751953125, 2362.7490234375, 2362.69921875, 2362.707275390625, 2362.706298828125, 2362.75732421875, 2362.707763671875, 2362.70947265625, 2362.4775390625, 2362.27197265625, 2362.1591796875, 2362.13427734375, 2362.13720703125, 2362.13134765625, 2362.213623046875, 2362.2470703125, 2362.293701171875, 2362.3330078125, 2362.385009765625, 2362.3740234375, 2362.4150390625, 2362.41162109375, 2362.37109375, 2362.402587890625, 2362.3916015625, 2362.4833984375, 2362.51416015625, 2362.5400390625, 2362.5087890625, 2362.529296875, 2362.533935546875, 2362.4873046875, 2362.41455078125, 2362.26025390625, 2362.240234375, 2362.17333984375, 2362.13427734375, 2362.111328125, 2362.15283203125, 2362.06201171875, 2362.07177734375, 2362.13623046875, 2362.150634765625, 2362.16064453125, 2362.193603515625, 2362.211181640625, 2362.231689453125, 2362.237060546875, 2362.2265625, 2362.25830078125, 2362.25439453125, 2362.296142578125, 2362.22900390625, 2362.180908203125, 2362.19287109375, 2362.2255859375, 2362.25927734375, 2362.2744140625, 2362.262451171875, 2362.26220703125, 2362.3232421875, 2362.3466796875, 2362.4072265625, 2362.39208984375, 2362.3095703125, 2362.29296875, 2362.31396484375, 2362.3154296875, 2362.3154296875, 2362.382080078125, 2362.29443359375, 2362.24365234375, 2362.3095703125, 2362.3154296875, 2362.33984375, 2362.341064453125, 2362.15478515625, 2362.056640625, 2362.0322265625, 2362.05908203125, 2362.05615234375, 2362.07275390625, 2362.078369140625, 2362.170166015625, 2362.1708984375, 2362.222900390625, 2362.2041015625, 2362.23583984375, 2362.27197265625, 2362.2802734375, 2362.21826171875, 2362.2119140625, 2361.9326171875, 2361.97412109375, 2362.835205078125, 2363.5830078125, 2364.07421875, 2364.38134765625, 2364.607421875, 2364.190185546875, 2363.408203125, 2363.404052734375, 2363.8525390625, 2363.88427734375, 2364.43994140625, 2365.6962890625, 2365.725341796875, 2365.59912109375, 2365.548828125, 2365.55126953125, 2365.22607421875, 2365.0673828125, 2364.956787109375, 2364.7880859375, 2364.700927734375, 2364.59716796875, 2364.5400390625, 2364.47314453125, 2364.42724609375, 2364.23193359375, 2364.156982421875, 2363.99462890625, 2363.97509765625, 2363.872314453125, 2363.84375, 2363.830810546875, 2363.7421875, 2363.7529296875, 2363.67626953125, 2363.626220703125, 2363.60302734375, 2363.61083984375, 2363.52490234375, 2363.546875, 2363.5439453125, 2363.529296875, 2363.510009765625, 2363.4892578125, 2363.503173828125, 2363.45458984375, 2363.390625, 2363.122314453125, 2363.067138671875, 2363.3408203125, 2363.860107421875, 2364.397705078125, 2364.7294921875, 2364.860107421875, 2364.947021484375, 2364.947509765625, 2364.87646484375, 2364.828369140625, 2364.66357421875, 2364.5205078125, 2364.435791015625, 2364.40966796875, 2364.3779296875, 2364.3505859375, 2364.3173828125, 2364.2841796875, 2364.27685546875, 2364.2138671875, 2364.15234375, 2364.025390625, 2363.9580078125, 2363.8994140625, 2363.85205078125, 2363.77294921875, 2363.63720703125, 2363.651611328125, 2363.599853515625, 2363.5361328125, 2363.5, 2363.5244140625, 2363.482421875, 2363.458251953125, 2363.427001953125, 2363.4677734375, 2363.529296875, 2363.43310546875, 2363.32421875, 2363.1669921875, 2363.091796875, 2363.049560546875, 2363.053466796875, 2363.05908203125, 2363.021728515625, 2363.033203125, 2363.02685546875, 2362.87841796875, 2362.7490234375, 2362.71337890625, 2362.7197265625, 2362.76513671875, 2362.88623046875, 2363.02734375, 2363.1328125, 2362.73193359375, 2362.657470703125, 2362.564208984375, 2362.5400390625, 2362.57275390625, 2362.591064453125, 2362.60888671875, 2362.673828125, 2362.783935546875, 2362.90869140625, 2362.9443359375, 2362.8818359375, 2362.7646484375, 2362.65234375, 2362.51416015625, 2362.457763671875, 2362.3642578125, 2362.3759765625, 2362.476318359375, 2362.51025390625, 2362.66259765625, 2362.40478515625, 2362.23779296875, 2362.241455078125, 2362.17724609375, 2362.1904296875, 2362.260986328125, 2362.263671875, 2362.32763671875, 2362.401611328125, 2362.42822265625, 2362.54541015625, 2362.353515625, 2362.224853515625, 2362.115478515625, 2362.025390625, 2362.0166015625, 2361.92041015625, 2361.935791015625, 2361.912841796875, 2361.927978515625, 2362.010986328125, 2362.60400390625, 2363.5595703125, 2363.81884765625, 2364.10400390625, 2364.972412109375, 2370.13818359375, 2372.3857421875, 2380.91845703125, 2383.226318359375, 2384.266357421875, 2383.10302734375, 2384.60791015625, 2387.049072265625, 2377.08154296875, 2381.279541015625, 2376.273193359375, 2379.521240234375, 2377.06982421875, 2380.65673828125, 2380.352294921875, 2377.69189453125, 2377.31591796875, 2378.177734375, 2377.736572265625, 2376.65869140625, 2375.728515625, 2376.272705078125, 2376.99169921875, 2377.50439453125, 2377.8046875, 2377.25830078125, 2376.04443359375, 2376.67138671875, 2377.067626953125, 2377.043212890625, 2376.84130859375, 2376.584716796875, 2376.31884765625, 2376.110107421875, 2376.0576171875, 2376.010498046875, 2375.9873046875, 2375.91845703125, 2375.813720703125, 2375.543701171875, 2376.28466796875, 2377.008544921875, 2377.40478515625, 2377.556884765625, 2377.488037109375, 2377.069580078125, 2376.80810546875, 2377.021484375, 2377.09326171875, 2376.954833984375, 2376.896484375, 2377.3525390625, 2377.41552734375, 2377.19482421875, 2376.903564453125, 2376.65380859375, 2376.39697265625, 2376.177734375, 2376.102294921875, 2376.263916015625, 2376.35205078125, 2376.33837890625, 2376.3330078125, 2376.261962890625, 2376.17578125, 2376.09765625, 2376.01416015625, 2375.949951171875, 2375.95703125, 2375.95751953125, 2375.947265625, 2376.30322265625, 2377.1064453125, 2377.53466796875, 2377.50048828125, 2377.056396484375, 2376.4990234375, 2375.916015625, 2376.14111328125, 2376.341552734375, 2376.4365234375, 2376.4345703125, 2376.379638671875, 2376.27099609375, 2376.125, 2375.9658203125, 2375.91259765625, 2375.83984375, 2375.78759765625, 2375.744140625, 2375.720947265625, 2375.70263671875, 2375.677978515625, 2375.61669921875, 2375.7607421875, 2375.809326171875, 2375.83935546875, 2375.84619140625, 2375.82763671875, 2375.8232421875, 2375.821044921875, 2375.811279296875, 2375.79345703125, 2375.7783203125, 2375.76123046875, 2375.69384765625, 2375.68310546875, 2375.68408203125, 2375.650390625, 2375.630615234375, 2375.56396484375, 2375.59765625, 2375.63671875, 2375.66552734375, 2375.634521484375, 2375.6064453125, 2375.538818359375, 2375.59912109375, 2375.6259765625, 2375.6279296875, 2375.637451171875, 2375.5732421875, 2375.516357421875, 2375.509033203125, 2375.46826171875, 2375.5478515625, 2375.603271484375, 2375.614501953125, 2375.642578125, 2375.65234375, 2375.62841796875, 2375.5771484375, 2375.53466796875, 2375.457763671875, 2375.404296875, 2375.49365234375, 2375.5283203125, 2375.5390625, 2375.52978515625, 2375.529296875, 2375.50830078125, 2375.45361328125, 2375.76513671875, 2376.41552734375, 2376.791748046875, 2376.927734375, 2376.87109375, 2376.666015625, 2376.482421875, 2376.24169921875, 2376.0634765625, 2375.90087890625, 2375.748779296875, 2375.67138671875, 2375.86767578125, 2376.26416015625, 2376.46240234375, 2376.5107421875, 2376.45263671875, 2376.25341796875, 2376.038330078125, 2375.806640625, 2375.6484375, 2375.497314453125, 2375.367919921875, 2375.23828125, 2375.20556640625, 2375.198974609375, 2375.20654296875, 2375.25439453125, 2375.260009765625, 2375.24609375, 2375.23095703125, 2375.1708984375, 2375.16796875, 2375.155029296875, 2375.11865234375, 2375.09912109375, 2375.0947265625, 2375.082275390625, 2375.0673828125, 2375.047119140625, 2375.052734375, 2375.006103515625, 2374.970947265625, 2374.955810546875, 2375.17626953125, 2375.34130859375, 2375.3984375, 2375.43359375, 2375.447265625] validation total loss: [2401.265625, 2400.970458984375, 2398.94287109375, 2394.396728515625, 2388.3232421875, 2372.6318359375, 2368.791748046875, 2359.564697265625, 2364.793701171875, 2372.5498046875, 2375.552001953125, 2379.453125, 2385.3251953125, 2386.163330078125, 2388.34619140625, 2389.78466796875, 2389.779296875, 2390.736328125, 2389.674072265625, 2389.042724609375, 2387.384033203125, 2386.10205078125, 2384.443359375, 2383.998046875, 2383.153564453125, 2383.34912109375, 2383.580810546875, 2382.980712890625, 2382.423095703125, 2381.907958984375, 2381.102783203125, 2380.786376953125, 2380.084716796875, 2380.268310546875, 2380.130126953125, 2380.3310546875, 2380.090576171875, 2379.833251953125, 2379.006103515625, 2379.9931640625, 2379.0361328125, 2379.383056640625, 2378.760986328125, 2379.086181640625, 2378.71484375, 2379.894287109375, 2379.449951171875, 2379.218505859375, 2379.079345703125, 2378.706787109375, 2378.43408203125, 2378.0146484375, 2377.687744140625, 2378.544921875, 2377.89208984375, 2377.593017578125, 2377.2001953125, 2376.787353515625, 2376.99755859375, 2377.10498046875, 2376.7607421875, 2376.91748046875, 2376.380859375, 2376.08642578125, 2375.9365234375, 2375.73974609375, 2376.053466796875, 2375.818359375, 2375.67724609375, 2375.958984375, 2375.680419921875, 2375.25439453125, 2375.024169921875, 2374.548828125, 2373.949951171875, 2373.875732421875, 2373.63232421875, 2372.863037109375, 2372.91015625, 2373.3251953125, 2373.735595703125, 2376.199462890625, 2378.038818359375, 2375.897705078125, 2375.51806640625, 2373.6298828125, 2373.785400390625, 2370.570556640625, 2366.20849609375, 2364.697021484375, 2365.096435546875, 2366.9326171875, 2367.47314453125, 2367.06640625, 2366.9345703125, 2365.502197265625, 2365.0126953125, 2364.503662109375, 2363.927490234375, 2363.742919921875, 2363.783935546875, 2363.86767578125, 2363.9638671875, 2364.046875, 2363.89794921875, 2363.865966796875, 2363.71533203125, 2363.566162109375, 2363.42822265625, 2363.29638671875, 2363.064453125, 2362.814208984375, 2362.5986328125, 2362.496337890625, 2362.37890625, 2362.37109375, 2362.3466796875, 2362.431884765625, 2362.479736328125, 2362.503173828125, 2362.574462890625, 2362.657470703125, 2362.74609375, 2363.029541015625, 2363.150146484375, 2363.129150390625, 2363.019287109375, 2362.87255859375, 2362.781005859375, 2362.6962890625, 2362.5732421875, 2362.60302734375, 2362.775634765625, 2362.778564453125, 2362.810546875, 2363.173095703125, 2363.578857421875, 2363.5751953125, 2363.408203125, 2363.30908203125, 2363.28515625, 2363.2412109375, 2363.150390625, 2363.061279296875, 2362.9169921875, 2362.781494140625, 2362.70556640625, 2362.779541015625, 2362.756103515625, 2362.728515625, 2362.744873046875, 2362.731689453125, 2362.66552734375, 2362.634765625, 2362.748046875, 2362.66357421875, 2362.67236328125, 2362.89404296875, 2363.119873046875, 2363.47119140625, 2363.802001953125, 2363.96875, 2363.815673828125, 2363.491943359375, 2363.23046875, 2362.951171875, 2362.7236328125, 2362.4052734375, 2361.965576171875, 2361.7861328125, 2361.87060546875, 2361.9296875, 2361.705810546875, 2361.824951171875, 2362.35302734375, 2362.8486328125, 2363.021484375, 2363.1982421875, 2363.3984375, 2364.1259765625, 2364.280029296875, 2364.216552734375, 2364.197265625, 2363.67578125, 2363.5791015625, 2364.232177734375, 2364.43212890625, 2364.302734375, 2364.090087890625, 2364.178466796875, 2363.7734375, 2363.441162109375, 2363.43115234375, 2363.74560546875, 2363.73876953125, 2363.215087890625, 2363.128173828125, 2363.2744140625, 2363.490234375, 2363.735107421875, 2363.84619140625, 2363.840576171875, 2363.62548828125, 2363.486083984375, 2363.3955078125, 2363.386962890625, 2363.1787109375, 2362.88916015625, 2362.992919921875, 2363.056884765625, 2362.9736328125, 2363.279052734375, 2363.65673828125, 2363.669189453125, 2363.492919921875, 2363.432861328125, 2363.5068359375, 2363.785400390625, 2363.996337890625, 2364.11328125, 2363.851318359375, 2363.419189453125, 2363.250732421875, 2363.43798828125, 2363.842529296875, 2364.30908203125, 2364.60400390625, 2364.4951171875, 2364.300537109375, 2364.313232421875, 2364.29052734375, 2364.302734375, 2364.29736328125, 2364.530517578125, 2364.367431640625, 2363.9306640625, 2363.630126953125, 2363.6640625, 2363.814697265625, 2363.913330078125, 2364.076416015625, 2364.15087890625, 2364.130126953125, 2364.00732421875, 2363.93408203125, 2363.720703125, 2363.59765625, 2363.597900390625, 2363.472900390625, 2363.28515625, 2363.403564453125, 2363.673583984375, 2363.708251953125, 2363.7734375, 2363.924560546875, 2363.881103515625, 2363.974853515625, 2363.968505859375, 2363.8193359375, 2363.7529296875, 2363.709716796875, 2363.6640625, 2363.765625, 2363.6787109375, 2363.642333984375, 2363.653564453125, 2363.654296875, 2363.677978515625, 2363.698974609375, 2363.65283203125, 2363.658447265625, 2363.4658203125, 2363.98779296875, 2365.125, 2365.197021484375, 2365.47021484375, 2365.8876953125, 2366.266845703125, 2365.797119140625, 2365.03515625, 2364.94873046875, 2365.579345703125, 2366.02001953125, 2366.34326171875, 2366.72607421875, 2366.931884765625, 2367.084716796875, 2367.052490234375, 2366.8916015625, 2366.56298828125, 2366.269775390625, 2366.14208984375, 2366.011962890625, 2365.938232421875, 2366.03564453125, 2366.081787109375, 2365.630615234375, 2365.481201171875, 2365.48828125, 2365.645751953125, 2365.62255859375, 2365.58544921875, 2365.49658203125, 2365.4365234375, 2365.35595703125, 2365.238037109375, 2365.219482421875, 2365.12548828125, 2365.05859375, 2365.102783203125, 2365.143798828125, 2365.064697265625, 2364.976806640625, 2364.991943359375, 2364.927001953125, 2364.981201171875, 2364.771240234375, 2364.699462890625, 2364.658447265625, 2364.548583984375, 2364.542236328125, 2364.500244140625, 2364.43359375, 2364.354736328125, 2364.263427734375, 2364.2431640625, 2364.314697265625, 2364.366943359375, 2364.406005859375, 2364.520263671875, 2364.492431640625, 2364.5244140625, 2364.564208984375, 2364.47998046875, 2364.37646484375, 2364.37841796875, 2364.316162109375, 2364.290771484375, 2364.276123046875, 2364.2666015625, 2364.232666015625, 2364.28076171875, 2364.327880859375, 2364.31396484375, 2364.275390625, 2364.3017578125, 2364.1416015625, 2363.656982421875, 2363.43310546875, 2363.421875, 2363.498779296875, 2363.625, 2363.74462890625, 2363.782470703125, 2363.910888671875, 2364.043701171875, 2364.132080078125, 2364.163818359375, 2364.236572265625, 2364.30224609375, 2364.2744140625, 2364.316162109375, 2364.332275390625, 2364.298583984375, 2364.2421875, 2364.181884765625, 2364.0927734375, 2364.0517578125, 2364.055908203125, 2364.050048828125, 2364.1240234375, 2364.1611328125, 2364.15673828125, 2364.05322265625, 2364.033935546875, 2364.036376953125, 2364.0078125, 2364.031494140625, 2364.02783203125, 2364.064208984375, 2364.0751953125, 2364.05859375, 2364.00244140625, 2363.96044921875, 2364.027099609375, 2363.966552734375, 2363.9326171875, 2363.913330078125, 2363.7978515625, 2363.773681640625, 2363.762451171875, 2363.716552734375, 2363.727783203125, 2363.767578125, 2363.732666015625, 2363.69677734375, 2363.72998046875, 2363.743896484375, 2363.79150390625, 2363.80322265625, 2363.664794921875, 2363.64599609375, 2363.58935546875, 2363.635986328125, 2363.671875, 2363.677734375, 2363.708984375, 2363.72021484375, 2363.75634765625, 2363.72607421875, 2363.755126953125, 2363.65478515625, 2363.572265625, 2363.47900390625, 2363.468994140625, 2363.52685546875, 2363.54931640625, 2363.56298828125, 2363.572265625, 2363.5263671875, 2363.5576171875, 2363.513427734375, 2363.5673828125, 2363.50390625, 2363.49267578125, 2363.48974609375, 2363.58544921875, 2363.60595703125, 2363.542724609375, 2363.50537109375, 2363.4765625, 2363.45166015625, 2363.613525390625, 2364.1484375, 2364.343505859375, 2363.990478515625, 2363.5283203125, 2363.428466796875, 2363.453125, 2363.594970703125, 2363.8388671875, 2364.268798828125, 2364.886962890625, 2365.38232421875, 2365.258056640625, 2365.01708984375, 2365.034912109375, 2365.116455078125, 2365.226806640625, 2365.4853515625, 2365.842529296875, 2366.26708984375, 2366.55029296875, 2366.732421875, 2366.75048828125, 2366.4326171875, 2366.19482421875, 2365.936279296875, 2365.716552734375, 2365.522705078125, 2365.36962890625, 2365.135009765625, 2364.89697265625, 2364.7333984375, 2364.640869140625, 2364.599365234375, 2364.55712890625, 2364.5673828125, 2364.513671875, 2364.509765625, 2364.510498046875, 2364.52978515625, 2364.621826171875, 2364.653564453125, 2364.634521484375, 2364.670166015625, 2364.679931640625, 2364.68603515625, 2364.7021484375, 2364.670166015625, 2364.30029296875, 2363.614990234375, 2363.20654296875, 2363.28271484375, 2363.56201171875, 2363.94287109375, 2364.173828125, 2364.292724609375, 2364.40869140625, 2364.523681640625, 2364.58642578125, 2364.664794921875, 2364.744873046875, 2364.704345703125, 2364.718505859375, 2364.728271484375, 2364.51416015625, 2364.3056640625, 2364.206787109375, 2364.146240234375, 2364.127197265625, 2364.086669921875, 2364.11767578125, 2364.173828125, 2364.202392578125, 2364.245849609375, 2364.2919921875, 2364.297607421875, 2364.3017578125, 2364.38720703125, 2364.371337890625, 2364.39599609375, 2364.380615234375, 2364.423583984375, 2364.36181640625, 2364.34912109375, 2364.24951171875, 2364.224609375, 2364.203857421875, 2364.230224609375, 2364.2314453125, 2364.178466796875, 2364.2197265625, 2364.173828125, 2364.1689453125, 2364.1923828125, 2364.16455078125, 2364.157470703125, 2364.1240234375, 2364.06103515625, 2363.97412109375, 2363.941650390625, 2363.892578125, 2363.890380859375, 2363.9072265625, 2363.88916015625, 2363.857666015625, 2363.8720703125, 2363.868896484375, 2363.883056640625, 2363.85400390625, 2363.892822265625, 2363.88427734375, 2363.92041015625, 2363.94873046875, 2363.955078125, 2363.92822265625, 2363.926513671875, 2363.9326171875, 2363.937744140625, 2363.93017578125, 2363.95947265625, 2363.80126953125, 2363.723876953125, 2363.63525390625, 2363.617431640625, 2363.646484375, 2363.6689453125, 2363.641357421875, 2363.698486328125, 2363.692626953125, 2363.743408203125, 2363.625732421875, 2363.619384765625, 2363.65478515625, 2363.68359375, 2363.661865234375, 2363.612060546875, 2363.62060546875, 2363.620361328125, 2363.671875, 2363.623291015625, 2363.6259765625, 2363.392822265625, 2363.185791015625, 2363.07275390625, 2363.047607421875, 2363.05078125, 2363.04541015625, 2363.128662109375, 2363.1630859375, 2363.2109375, 2363.247314453125, 2363.297607421875, 2363.285888671875, 2363.326904296875, 2363.323486328125, 2363.283447265625, 2363.3154296875, 2363.304931640625, 2363.395751953125, 2363.426025390625, 2363.45166015625, 2363.420654296875, 2363.44189453125, 2363.44677734375, 2363.400634765625, 2363.327880859375, 2363.173095703125, 2363.1533203125, 2363.08642578125, 2363.0478515625, 2363.025634765625, 2363.068603515625, 2362.975341796875, 2362.983642578125, 2363.047607421875, 2363.06201171875, 2363.072265625, 2363.10595703125, 2363.12451171875, 2363.146240234375, 2363.153076171875, 2363.140625, 2363.1708984375, 2363.166748046875, 2363.208251953125, 2363.14111328125, 2363.093994140625, 2363.107177734375, 2363.141357421875, 2363.17431640625, 2363.18896484375, 2363.1767578125, 2363.1767578125, 2363.23828125, 2363.2626953125, 2363.32421875, 2363.309814453125, 2363.22607421875, 2363.2080078125, 2363.2275390625, 2363.228759765625, 2363.228759765625, 2363.296142578125, 2363.206787109375, 2363.156494140625, 2363.223876953125, 2363.2314453125, 2363.257568359375, 2363.259765625, 2363.069091796875, 2362.968505859375, 2362.943603515625, 2362.97021484375, 2362.967529296875, 2362.984619140625, 2362.990966796875, 2363.084228515625, 2363.08642578125, 2363.139404296875, 2363.1201171875, 2363.151123046875, 2363.18701171875, 2363.195556640625, 2363.1337890625, 2363.12841796875, 2362.841552734375, 2362.880126953125, 2363.744140625, 2364.489990234375, 2364.980224609375, 2365.28759765625, 2365.5146484375, 2365.096435546875, 2364.31396484375, 2364.310302734375, 2364.761962890625, 2364.7900390625, 2365.345458984375, 2366.606689453125, 2366.63134765625, 2366.5048828125, 2366.45654296875, 2366.469970703125, 2366.13720703125, 2365.976318359375, 2365.865478515625, 2365.697021484375, 2365.609375, 2365.50537109375, 2365.4482421875, 2365.381591796875, 2365.336181640625, 2365.14013671875, 2365.06494140625, 2364.902587890625, 2364.88330078125, 2364.781005859375, 2364.752685546875, 2364.739990234375, 2364.651611328125, 2364.662841796875, 2364.584716796875, 2364.533935546875, 2364.5107421875, 2364.5185546875, 2364.4326171875, 2364.454833984375, 2364.452392578125, 2364.43798828125, 2364.419189453125, 2364.398681640625, 2364.4130859375, 2364.36474609375, 2364.301025390625, 2364.029052734375, 2363.97265625, 2364.24658203125, 2364.76953125, 2365.305419921875, 2365.6357421875, 2365.76611328125, 2365.852783203125, 2365.853271484375, 2365.78271484375, 2365.735107421875, 2365.5703125, 2365.42724609375, 2365.342529296875, 2365.31640625, 2365.28466796875, 2365.25732421875, 2365.22412109375, 2365.19091796875, 2365.18359375, 2365.12060546875, 2365.05908203125, 2364.932373046875, 2364.865478515625, 2364.806884765625, 2364.760009765625, 2364.68017578125, 2364.544189453125, 2364.55859375, 2364.5068359375, 2364.443359375, 2364.407470703125, 2364.43212890625, 2364.390380859375, 2364.366455078125, 2364.335205078125, 2364.376220703125, 2364.438720703125, 2364.3408203125, 2364.230712890625, 2364.0732421875, 2363.998046875, 2363.9560546875, 2363.96044921875, 2363.96630859375, 2363.92919921875, 2363.940673828125, 2363.9345703125, 2363.785400390625, 2363.65576171875, 2363.6201171875, 2363.626953125, 2363.67333984375, 2363.796875, 2363.9384765625, 2364.044189453125, 2363.63916015625, 2363.564208984375, 2363.47119140625, 2363.447509765625, 2363.48095703125, 2363.5, 2363.5185546875, 2363.583984375, 2363.694580078125, 2363.820068359375, 2363.856201171875, 2363.791748046875, 2363.673095703125, 2363.560546875, 2363.422607421875, 2363.36669921875, 2363.27392578125, 2363.286376953125, 2363.3876953125, 2363.422119140625, 2363.575439453125, 2363.314697265625, 2363.146728515625, 2363.150146484375, 2363.086181640625, 2363.099853515625, 2363.171142578125, 2363.1748046875, 2363.23974609375, 2363.31494140625, 2363.34228515625, 2363.46044921875, 2363.26513671875, 2363.133544921875, 2363.02294921875, 2362.9326171875, 2362.923828125, 2362.827880859375, 2362.84375, 2362.82177734375, 2362.83740234375, 2362.917236328125, 2363.509521484375, 2364.47119140625, 2364.72412109375, 2365.009033203125, 2365.887451171875, 2371.421142578125, 2373.62939453125, 2382.15283203125, 2384.411865234375, 2385.420166015625, 2384.25634765625, 2385.761474609375, 2388.173828125, 2378.185546875, 2382.458740234375, 2377.4013671875, 2380.6923828125, 2378.138671875, 2381.734619140625, 2381.421630859375, 2378.7587890625, 2378.382568359375, 2379.243896484375, 2378.802734375, 2377.725341796875, 2376.799560546875, 2377.341064453125, 2378.05810546875, 2378.5703125, 2378.870361328125, 2378.32421875, 2377.1142578125, 2377.738037109375, 2378.133544921875, 2378.109130859375, 2377.907470703125, 2377.651123046875, 2377.3857421875, 2377.177734375, 2377.125244140625, 2377.078125, 2377.054931640625, 2376.986328125, 2376.882568359375, 2376.615234375, 2377.3525390625, 2378.074951171875, 2378.470703125, 2378.62255859375, 2378.553955078125, 2378.13623046875, 2377.87548828125, 2378.088134765625, 2378.159912109375, 2378.021484375, 2377.96337890625, 2378.41845703125, 2378.4814453125, 2378.260986328125, 2377.969970703125, 2377.720703125, 2377.46484375, 2377.24658203125, 2377.17138671875, 2377.332275390625, 2377.420166015625, 2377.40625, 2377.401123046875, 2377.330322265625, 2377.244384765625, 2377.166748046875, 2377.083740234375, 2377.019775390625, 2377.02685546875, 2377.02734375, 2377.01708984375, 2377.37158203125, 2378.172607421875, 2378.600341796875, 2378.566162109375, 2378.122314453125, 2377.565673828125, 2376.98486328125, 2377.208740234375, 2377.408447265625, 2377.50341796875, 2377.50146484375, 2377.44677734375, 2377.33837890625, 2377.19287109375, 2377.03466796875, 2376.981689453125, 2376.9091796875, 2376.857177734375, 2376.81396484375, 2376.790771484375, 2376.7724609375, 2376.748046875, 2376.686767578125, 2376.830322265625, 2376.878662109375, 2376.908447265625, 2376.915283203125, 2376.896728515625, 2376.892578125, 2376.89013671875, 2376.88037109375, 2376.862548828125, 2376.84765625, 2376.83056640625, 2376.763427734375, 2376.752685546875, 2376.753662109375, 2376.719970703125, 2376.700439453125, 2376.634033203125, 2376.66748046875, 2376.706298828125, 2376.735107421875, 2376.7041015625, 2376.67626953125, 2376.60888671875, 2376.668701171875, 2376.695556640625, 2376.697265625, 2376.70703125, 2376.642822265625, 2376.586181640625, 2376.578857421875, 2376.538330078125, 2376.61767578125, 2376.672607421875, 2376.68359375, 2376.711669921875, 2376.721435546875, 2376.697509765625, 2376.646484375, 2376.604248046875, 2376.527587890625, 2376.474365234375, 2376.563232421875, 2376.59765625, 2376.6083984375, 2376.59912109375, 2376.5986328125, 2376.57763671875, 2376.523193359375, 2376.83349609375, 2377.482177734375, 2377.857666015625, 2377.99365234375, 2377.93701171875, 2377.732177734375, 2377.548828125, 2377.308349609375, 2377.130615234375, 2376.968505859375, 2376.81689453125, 2376.739990234375, 2376.935546875, 2377.330810546875, 2377.528564453125, 2377.576904296875, 2377.518798828125, 2377.31982421875, 2377.10498046875, 2376.87353515625, 2376.7158203125, 2376.5654296875, 2376.436767578125, 2376.3076171875, 2376.27490234375, 2376.268310546875, 2376.27587890625, 2376.32373046875, 2376.3291015625, 2376.315185546875, 2376.30029296875, 2376.240234375, 2376.237548828125, 2376.224609375, 2376.1884765625, 2376.1689453125, 2376.164794921875, 2376.15234375, 2376.137451171875, 2376.1171875, 2376.122802734375, 2376.076416015625, 2376.041259765625, 2376.026123046875, 2376.24560546875, 2376.409912109375, 2376.466552734375, 2376.50146484375, 2376.51513671875] validation accuracy: [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.30833333333333335, 0.2708333333333333, 0.4375, 0.48333333333333334, 0.5, 0.4583333333333333, 0.4666666666666667, 0.6291666666666667, 0.6625, 0.6833333333333333, 0.7083333333333334, 0.5541666666666667, 0.7, 0.6208333333333333, 0.7416666666666667, 0.7375, 0.7458333333333333, 0.7333333333333333, 0.7458333333333333, 0.7458333333333333, 0.7458333333333333, 0.7458333333333333, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7958333333333333, 0.825, 0.75, 0.7041666666666667, 0.7333333333333333, 0.8208333333333333, 0.75, 0.7708333333333334, 0.925, 0.8541666666666666, 0.9, 0.8375, 0.8291666666666667, 0.9041666666666667, 0.8125, 0.9416666666666667, 0.9458333333333333, 0.8958333333333334, 0.975, 0.9375, 0.8833333333333333, 0.9625, 0.9541666666666667, 0.8958333333333334, 0.95, 0.9666666666666667, 0.9583333333333334, 0.95, 0.9666666666666667, 0.9708333333333333, 0.9458333333333333, 0.9416666666666667, 0.9666666666666667, 0.9666666666666667, 0.9416666666666667, 0.95, 0.9666666666666667, 0.9708333333333333, 0.9583333333333334, 0.9291666666666667, 0.9666666666666667, 0.9666666666666667, 0.95, 0.9666666666666667, 0.975, 0.9625, 0.9416666666666667, 0.9625, 0.9625, 0.9625, 0.95, 0.9625, 0.975, 0.925, 0.925, 0.9708333333333333, 0.95, 0.9208333333333333, 0.9458333333333333, 0.9708333333333333, 0.9708333333333333, 0.9208333333333333, 0.9416666666666667, 0.9458333333333333, 0.95, 0.975, 0.9583333333333334, 0.9291666666666667, 0.9541666666666667, 0.975, 0.975, 0.9083333333333333, 0.9291666666666667, 0.975, 0.9666666666666667, 0.9916666666666667, 0.975, 0.9625, 0.9708333333333333, 0.975, 0.975, 0.975, 0.975, 0.9625, 0.9625, 0.9708333333333333, 0.975, 0.9708333333333333, 0.9375, 0.9625, 0.9583333333333334, 0.9125, 0.9583333333333334, 0.9625, 0.9291666666666667, 0.9541666666666667, 0.9708333333333333, 0.9291666666666667, 0.9333333333333333, 0.9416666666666667, 0.9708333333333333, 0.9375, 0.8916666666666667, 0.9625, 0.9708333333333333, 0.8958333333333334, 0.8958333333333334, 0.975, 0.9708333333333333, 0.8958333333333334, 0.9416666666666667, 0.9708333333333333, 0.9708333333333333, 0.9583333333333334, 0.9541666666666667, 0.9666666666666667, 0.9583333333333334, 0.9333333333333333, 0.9333333333333333, 0.9541666666666667, 0.95, 0.9541666666666667, 0.9583333333333334, 0.9541666666666667, 0.9416666666666667, 0.9333333333333333, 0.9541666666666667, 0.9583333333333334, 0.9416666666666667, 0.9375, 0.9666666666666667, 0.9458333333333333, 0.9208333333333333, 0.9333333333333333, 0.9666666666666667, 0.9458333333333333, 0.9166666666666666, 0.9125, 0.9375, 0.9416666666666667, 0.9416666666666667, 0.9375, 0.9208333333333333, 0.9208333333333333, 0.9458333333333333, 0.9416666666666667, 0.9041666666666667, 0.9291666666666667, 0.9583333333333334, 0.9625, 0.9541666666666667, 0.9541666666666667, 0.9625, 0.9291666666666667, 0.9208333333333333, 0.925, 0.9333333333333333, 0.925, 0.9208333333333333, 0.925, 0.925, 0.9291666666666667, 0.9291666666666667, 0.9333333333333333, 0.9375, 0.9333333333333333, 0.9291666666666667, 0.925, 0.9375, 0.9416666666666667, 0.9208333333333333, 0.9, 0.925, 0.9416666666666667, 0.9375, 0.9291666666666667, 0.9166666666666666, 0.9208333333333333, 0.9291666666666667, 0.9333333333333333, 0.9333333333333333, 0.9208333333333333, 0.9125, 0.9125, 0.9125, 0.9166666666666666, 0.9166666666666666, 0.9125, 0.9125, 0.9083333333333333, 0.9, 0.9416666666666667, 0.9041666666666667, 0.875, 0.9625, 0.9791666666666666, 0.9666666666666667, 0.9208333333333333, 0.9666666666666667, 0.9916666666666667, 0.9833333333333333, 0.9208333333333333, 0.95, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9708333333333333, 0.9375, 0.9375, 0.9708333333333333, 0.975, 0.9708333333333333, 0.9666666666666667, 0.9416666666666667, 0.9166666666666666, 0.8916666666666667, 0.9375, 0.9541666666666667, 0.9416666666666667, 0.9083333333333333, 0.9208333333333333, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333, 0.9208333333333333, 0.9208333333333333, 0.9125, 0.9083333333333333, 0.9, 0.9041666666666667, 0.9166666666666666, 0.9166666666666666, 0.8958333333333334, 0.9, 0.8958333333333334, 0.9458333333333333, 0.9833333333333333, 0.9875, 0.9833333333333333, 0.9708333333333333, 0.9541666666666667, 0.9541666666666667, 0.9625, 0.9625, 0.9583333333333334, 0.9541666666666667, 0.9541666666666667, 0.9416666666666667, 0.9333333333333333, 0.9416666666666667, 0.9416666666666667, 0.9375, 0.9375, 0.9416666666666667, 0.9333333333333333, 0.9291666666666667, 0.9208333333333333, 0.9125, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9125, 0.9166666666666666, 0.9208333333333333, 0.9166666666666666, 0.9291666666666667, 0.9583333333333334, 0.9625, 0.9583333333333334, 0.9291666666666667, 0.9125, 0.9, 0.9291666666666667, 0.9583333333333334, 0.9583333333333334, 0.9583333333333334, 0.9541666666666667, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333, 0.9291666666666667, 0.9291666666666667, 0.9208333333333333, 0.9208333333333333, 0.9208333333333333, 0.9208333333333333, 0.9166666666666666, 0.9166666666666666, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9, 0.9083333333333333, 0.9083333333333333, 0.9125, 0.9125, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9, 0.9083333333333333, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9208333333333333, 0.9208333333333333, 0.9166666666666666, 0.9125, 0.9041666666666667, 0.9041666666666667, 0.9041666666666667, 0.9041666666666667, 0.9041666666666667, 0.9041666666666667, 0.9041666666666667, 0.9083333333333333, 0.9125, 0.9125, 0.9083333333333333, 0.9041666666666667, 0.9, 0.9041666666666667, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9041666666666667, 0.9083333333333333, 0.9125, 0.9166666666666666, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9208333333333333, 0.9583333333333334, 0.925, 0.9083333333333333, 0.9291666666666667, 0.975, 0.9666666666666667, 0.9416666666666667, 0.9041666666666667, 0.9208333333333333, 0.9791666666666666, 0.9583333333333334, 0.9, 0.9, 0.9375, 0.9708333333333333, 0.9708333333333333, 0.9708333333333333, 0.9375, 0.9458333333333333, 0.9875, 0.9875, 0.9666666666666667, 0.9125, 0.975, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9625, 0.95, 0.9583333333333334, 0.9708333333333333, 0.975, 0.975, 0.9708333333333333, 0.9666666666666667, 0.9625, 0.9583333333333334, 0.95, 0.9458333333333333, 0.9375, 0.9291666666666667, 0.9208333333333333, 0.9375, 0.9458333333333333, 0.95, 0.95, 0.9458333333333333, 0.9416666666666667, 0.9583333333333334, 0.9791666666666666, 0.9833333333333333, 0.9791666666666666, 0.9375, 0.8958333333333334, 0.9083333333333333, 0.95, 0.9625, 0.9625, 0.9625, 0.9625, 0.9541666666666667, 0.95, 0.9416666666666667, 0.925, 0.9458333333333333, 0.9541666666666667, 0.9541666666666667, 0.9541666666666667, 0.9541666666666667, 0.9541666666666667, 0.95, 0.9333333333333333, 0.9291666666666667, 0.9208333333333333, 0.9083333333333333, 0.9041666666666667, 0.9041666666666667, 0.9041666666666667, 0.9041666666666667, 0.9041666666666667, 0.9166666666666666, 0.9208333333333333, 0.9208333333333333, 0.9125, 0.9083333333333333, 0.9041666666666667, 0.9041666666666667, 0.8958333333333334, 0.8958333333333334, 0.8958333333333334, 0.8916666666666667, 0.8916666666666667, 0.8958333333333334, 0.8958333333333334, 0.8958333333333334, 0.8958333333333334, 0.8916666666666667, 0.8916666666666667, 0.9041666666666667, 0.9083333333333333, 0.9208333333333333, 0.9208333333333333, 0.9208333333333333, 0.9208333333333333, 0.9166666666666666, 0.9083333333333333, 0.9083333333333333, 0.9041666666666667, 0.9, 0.8958333333333334, 0.8958333333333334, 0.9, 0.9041666666666667, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9083333333333333, 0.9, 0.9, 0.8916666666666667, 0.9, 0.9041666666666667, 0.9, 0.9, 0.9, 0.9, 0.9, 0.8916666666666667, 0.8916666666666667, 0.8916666666666667, 0.9041666666666667, 0.925, 0.9291666666666667, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333, 0.9291666666666667, 0.925, 0.925, 0.9208333333333333, 0.9083333333333333, 0.9208333333333333, 0.9291666666666667, 0.9291666666666667, 0.9291666666666667, 0.9291666666666667, 0.925, 0.925, 0.9208333333333333, 0.9041666666666667, 0.925, 0.9333333333333333, 0.9333333333333333, 0.9375, 0.9375, 0.9333333333333333, 0.9291666666666667, 0.9291666666666667, 0.9333333333333333, 0.9375, 0.9375, 0.9375, 0.9333333333333333, 0.9291666666666667, 0.925, 0.9291666666666667, 0.9291666666666667, 0.9291666666666667, 0.9291666666666667, 0.9291666666666667, 0.925, 0.9125, 0.9291666666666667, 0.9375, 0.9458333333333333, 0.95, 0.9416666666666667, 0.9333333333333333, 0.9291666666666667, 0.9166666666666666, 0.9083333333333333, 0.9291666666666667, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333, 0.9291666666666667, 0.9208333333333333, 0.9166666666666666, 0.9166666666666666, 0.925, 0.925, 0.925, 0.9208333333333333, 0.9083333333333333, 0.9083333333333333, 0.9, 0.9083333333333333, 0.9208333333333333, 0.9208333333333333, 0.925, 0.925, 0.9208333333333333, 0.9291666666666667, 0.925, 0.9208333333333333, 0.9083333333333333, 0.9, 0.8916666666666667, 0.9166666666666666, 0.9375, 0.9416666666666667, 0.9458333333333333, 0.9416666666666667, 0.9333333333333333, 0.9333333333333333, 0.9166666666666666, 0.9166666666666666, 0.9041666666666667, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9166666666666666, 0.9041666666666667, 0.9625, 0.9833333333333333, 0.9625, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.975, 0.9833333333333333, 0.9958333333333333, 0.9833333333333333, 0.95, 0.9958333333333333, 0.9958333333333333, 0.9458333333333333, 0.9916666666666667, 0.9958333333333333, 0.9625, 0.8958333333333334, 0.9458333333333333, 0.9541666666666667, 0.9583333333333334, 0.9625, 0.9666666666666667, 0.9708333333333333, 0.9708333333333333, 0.9666666666666667, 0.9666666666666667, 0.975, 0.975, 0.975, 0.9708333333333333, 0.9666666666666667, 0.9666666666666667, 0.9625, 0.9583333333333334, 0.9458333333333333, 0.9708333333333333, 0.975, 0.9791666666666666, 0.9791666666666666, 0.9791666666666666, 0.975, 0.9708333333333333, 0.9666666666666667, 0.9583333333333334, 0.9541666666666667, 0.95, 0.9458333333333333, 0.9458333333333333, 0.9833333333333333, 0.9958333333333333, 0.9916666666666667, 0.95, 0.9625, 0.9833333333333333, 0.9916666666666667, 0.9916666666666667, 0.9916666666666667, 0.9791666666666666, 0.975, 0.9666666666666667, 0.9708333333333333, 0.9708333333333333, 0.9708333333333333, 0.9708333333333333, 0.9708333333333333, 0.9708333333333333, 0.9708333333333333, 0.9708333333333333, 0.9708333333333333, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9625, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9625, 0.9583333333333334, 0.9583333333333334, 0.9541666666666667, 0.95, 0.9666666666666667, 0.975, 0.9875, 0.9875, 0.9708333333333333, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9625, 0.9625, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9583333333333334, 0.9458333333333333, 0.9416666666666667, 0.9416666666666667, 0.9666666666666667, 0.9708333333333333, 0.9666666666666667, 0.9625, 0.9583333333333334, 0.95, 0.95, 0.9458333333333333, 0.9458333333333333, 0.9375, 0.9333333333333333, 0.95, 0.9583333333333334, 0.9583333333333334, 0.9583333333333334, 0.95, 0.95, 0.9458333333333333, 0.9375, 0.9333333333333333, 0.925, 0.9458333333333333, 0.95, 0.9583333333333334, 0.95, 0.95, 0.9458333333333333, 0.9416666666666667, 0.9333333333333333, 0.925, 0.9208333333333333, 0.9166666666666666, 0.9458333333333333, 0.9541666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9583333333333334, 0.9541666666666667, 0.9541666666666667, 0.9833333333333333, 0.9958333333333333, 0.9291666666666667, 0.9958333333333333, 0.9958333333333333, 0.9166666666666666, 0.3541666666666667, 0.5, 0.5, 0.5125, 0.5125, 0.5, 0.5208333333333334, 0.7208333333333333, 0.5916666666666667, 0.7, 0.7333333333333333, 0.7041666666666667, 0.7375, 0.75, 0.75, 0.7375, 0.7333333333333333, 0.7458333333333333, 0.7458333333333333, 0.7333333333333333, 0.7083333333333334, 0.7208333333333333, 0.7416666666666667, 0.7458333333333333, 0.7458333333333333, 0.7458333333333333, 0.7166666666666667, 0.7416666666666667, 0.7458333333333333, 0.7458333333333333, 0.7416666666666667, 0.7416666666666667, 0.7375, 0.7291666666666666, 0.7291666666666666, 0.7291666666666666, 0.7291666666666666, 0.7291666666666666, 0.7208333333333333, 0.7083333333333334, 0.725, 0.7416666666666667, 0.7458333333333333, 0.7458333333333333, 0.7416666666666667, 0.7416666666666667, 0.7291666666666666, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7375, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7333333333333333, 0.7291666666666666, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.7166666666666667, 0.7125, 0.7125, 0.7125, 0.7125, 0.725, 0.7416666666666667, 0.7458333333333333, 0.7458333333333333, 0.7416666666666667, 0.7333333333333333, 0.7208333333333333, 0.725, 0.7333333333333333, 0.7375, 0.7375, 0.7333333333333333, 0.725, 0.725, 0.725, 0.725, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7166666666666667, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.725, 0.725, 0.7208333333333333, 0.725, 0.725, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.725, 0.7416666666666667, 0.7416666666666667, 0.7458333333333333, 0.7458333333333333, 0.7416666666666667, 0.7416666666666667, 0.7375, 0.7333333333333333, 0.7291666666666666, 0.725, 0.725, 0.7291666666666666, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.7333333333333333, 0.725, 0.725, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.7208333333333333, 0.725, 0.725, 0.725, 0.725]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fq5zzuzg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>total train loss</td><td>▂▅▄▃▁▁▁▁▁▁▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▂▁▁▁█▇▆▆▆▆▆▆</td></tr><tr><td>total validation loss</td><td>▁▇▆▅▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▃▂▂▂█▆▆▆▆▆▆▅</td></tr><tr><td>train label accuracy</td><td>▁▅▅▅████████████████████████████▄▅▅▅▅▅▅▅</td></tr><tr><td>train label loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▄▄▄▃▄▄▃</td></tr><tr><td>train triplet loss</td><td>▂▅▄▃▁▁▁▁▁▁▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▂▁▁▁█▇▆▆▆▆▆▆</td></tr><tr><td>validation label accuracy</td><td>▁▆▆▆▇█████▇█▇▇▇▇▇▇██▇▇▇▇▇▇█████▇▃▆▅▅▅▅▆▅</td></tr><tr><td>validation label loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▄▄▄▄▄▄▄</td></tr><tr><td>validation triplet loss</td><td>▁▇▆▅▂▂▂▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▂▂▂█▆▆▆▆▆▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>7134.20955</td></tr><tr><td>total validation loss</td><td>2376.51514</td></tr><tr><td>train label accuracy</td><td>0.75</td></tr><tr><td>train label loss</td><td>1.05647</td></tr><tr><td>train triplet loss</td><td>7133.1532</td></tr><tr><td>validation label accuracy</td><td>0.725</td></tr><tr><td>validation label loss</td><td>1.06779</td></tr><tr><td>validation triplet loss</td><td>2375.44727</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">generous-terrain-479</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/fq5zzuzg' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/fq5zzuzg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230419_171528-fq5zzuzg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fq5zzuzg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c8bcd5f5964c0c8c551086e75d60f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670519618007043, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20230419_172041-02uc2n73</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/02uc2n73' target=\"_blank\">sleek-bush-480</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/02uc2n73' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/02uc2n73</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "num_classes = 4 # Number of unique class labels in the dataset\n",
    "latent_dims = 32\n",
    "epochs = 1000\n",
    "lr = 0.001\n",
    "num_models = 1\n",
    "batch_size = 1024\n",
    "save_dir = save_dir\n",
    "main_code(save_dir, num_models, epochs, num_classes, batch_size,\n",
    "             lr, latent_dims)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75759db008f847b3b2bb062eaa483eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hearty-disco-477</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/ha7isah8' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/ha7isah8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230419_170832-ha7isah8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5ec221b8ce1ddc6eafacfbc77a75e3f09c9fea6e76ac8503f9810425480e77e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
