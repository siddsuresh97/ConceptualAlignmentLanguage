{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = os.path.abspath('../..')\n",
    "save_dir = os.path.join(base_dir,'results')\n",
    "data_dir = os.path.join(base_dir,'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.manual_seed(0)\n",
    "import wandb\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "from torch.utils.data import TensorDataset,Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# from neurora.rdm_corr import rdm_correlation_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLabelModel(nn.Module):\n",
    "    def __init__(self, encoded_space_dim=64, num_classes=4):\n",
    "        super().__init__()\n",
    "        \"\"\n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "    \n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        ### Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            nn.Linear(32*4*4, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, encoded_space_dim)\n",
    "        )\n",
    "\n",
    "        ## triplet projection module\n",
    "        self.decoder_triplet_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 32),\n",
    "            nn.ReLU(True)\n",
    "         \n",
    "        )\n",
    "        ##labeling module\n",
    "        self.decoder_labels_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16, num_classes),\n",
    "        )\n",
    "\n",
    "        ### initialize weights using xavier initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    \n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        batch_s = x.size(0)\n",
    "        img_features = self.encoder_cnn(x)\n",
    "        img_features = self.flatten(img_features)\n",
    "        \n",
    "        enc_latent = self.encoder_lin(img_features)\n",
    "\n",
    "        triplet_latent = self.decoder_triplet_lin(enc_latent)\n",
    "        label = self.decoder_labels_lin(enc_latent)\n",
    "        # label = F.softmax(label,dim=1)\n",
    "        return enc_latent, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### custom loss computing triplet loss and labeling loss\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, margin=10):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, anchor, positive, negative, label, pred_label):\n",
    "        cosine_sim = torch.nn.CosineSimilarity(1)\n",
    "        # distance_positive = torch.tensor(1)-cosine_sim(anchor,positive)\n",
    "   \n",
    "        # distance_negative = torch.tensor(1)-cosine_sim(anchor,negative)\n",
    "\n",
    "        # triplet_loss = torch.maximum(distance_positive - distance_negative + self.margin, torch.tensor(0))\n",
    "        # triplet_loss = torch.sum(triplet_loss)\n",
    "        triplet_loss = (nn.TripletMarginWithDistanceLoss( distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)))\n",
    "        triplet_loss = triplet_loss(anchor, positive, negative)\n",
    "        label_loss = F.binary_cross_entropy_with_logits(pred_label.float(), label.float())\n",
    "        total_loss = triplet_loss + label_loss\n",
    "        return triplet_loss, label_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrainModels(nn.Module):\n",
    "    def __init__(self, latent_dims, num_classes):\n",
    "        super(TrainModels, self).__init__()\n",
    "        self.triplet_lab_model = TripletLabelModel(latent_dims, num_classes)\n",
    "        self.custom_loss = CustomLoss()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, anchor_im, positive_im, negative_im):\n",
    "        anchor_latent, anchor_label = self.triplet_lab_model(anchor_im)\n",
    "        positive_latent, _ = self.triplet_lab_model(positive_im)\n",
    "        negative_latent, _ = self.triplet_lab_model(negative_im)\n",
    "\n",
    "        return anchor_latent, positive_latent, negative_latent, anchor_label\n",
    "\n",
    "    def test_epoch(self, test_data):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "        self.eval()\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            # Define the lists to store the outputs for each batch\n",
    "            test_triplet_loss = []\n",
    "            test_label_loss = []\n",
    "            test_total_loss = []\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for anchor_ims, contrast_ims, labels in test_data:\n",
    "                # Move tensor to the proper device\n",
    "                anchor_ims = anchor_ims.to(device)\n",
    "                contrast_ims = contrast_ims.to(device)\n",
    "                labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "                labels = labels.to(device)\n",
    "                anchor_latent, positive_latent, negative_latent, pred_label = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "                # Append the network output and the original image to the lists\n",
    "                triplet_loss, label_loss, total_loss = self.custom_loss(anchor_latent,\n",
    "                                                                positive_latent, \n",
    "                                                                negative_latent, \n",
    "                                                                labels,\n",
    "                                                                pred_label)\n",
    "                total += labels.size(0)\n",
    "                correct += (torch.argmax(pred_label, dim = 1) == torch.argmax(labels, dim = 1)).sum().item()\n",
    "                test_triplet_loss.append(triplet_loss.item())\n",
    "                test_label_loss.append(label_loss.item())\n",
    "                test_total_loss.append(total_loss.item())\n",
    "        test_triplet_loss = sum(test_triplet_loss)/len(test_triplet_loss)\n",
    "        test_label_loss = sum(test_label_loss)/len(test_label_loss)\n",
    "        test_total_loss = sum(test_total_loss)/len(test_total_loss)\n",
    "        test_accuracy = correct/total\n",
    "        return test_triplet_loss, test_label_loss, test_total_loss, test_accuracy\n",
    "\n",
    "    def test_epoch_calculate_representation_separation(self, test_data):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "        self.eval()\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            accuracies = []\n",
    "            for anchor_ims, contrast_ims, labels in test_data:\n",
    "                # Move tensor to the proper device\n",
    "                anchor_ims = anchor_ims.to(device)\n",
    "                contrast_ims = contrast_ims.to(device)\n",
    "                # labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "                # labels = labels.to(device)\n",
    "                anchor_latent, _, _, _ = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "                # use sklearn to predict labels from anchor_latent\n",
    "                # calculate accuracy\n",
    "                # x's are anchor_latent and y's are labels\n",
    "                # append accuracy to list\n",
    "                # put anchor_latent and labels on cpu and convert to numpy\n",
    "\n",
    "          \n",
    "                anchor_latent = anchor_latent.cpu().numpy()\n",
    "                ### standard scale the data in anchor_latent before fitting to the model\n",
    "                anchor_latent = StandardScaler().fit_transform(anchor_latent)\n",
    "                labels = labels.cpu().numpy()\n",
    "                \n",
    "                lm = linear_model.LogisticRegression()\n",
    "                lm.fit(anchor_latent, labels)\n",
    "                # convert labels to sklearn format\n",
    "                accuracies.append(lm.score(anchor_latent, labels))\n",
    "        accuracy = sum(accuracies)/len(accuracies)\n",
    "        return accuracy\n",
    "\n",
    "    def train_epoch(self, train_data, optimizer, train_mode):\n",
    "        self.train()\n",
    "        train_triplet_loss = []\n",
    "        train_label_loss = []\n",
    "        train_total_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for anchor_ims, contrast_ims, labels in train_data:\n",
    "            \n",
    "            anchor_ims = anchor_ims.to(device)\n",
    "            contrast_ims = contrast_ims.to(device)\n",
    "            labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            anchor_latent, positive_latent, negative_latent, pred_label = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "           \n",
    "           \n",
    "           \n",
    "            triplet_loss, label_loss, total_loss = self.custom_loss(anchor_latent,\n",
    "                                                                positive_latent, \n",
    "                                                                negative_latent, \n",
    "                                                                labels,\n",
    "                                                                pred_label)\n",
    "            \n",
    "            \n",
    "            if train_mode==0:\n",
    "                triplet_loss.backward()\n",
    "            elif train_mode==1:\n",
    "                label_loss.backward()\n",
    "            elif train_mode==2:\n",
    "                total_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_triplet_loss.append(triplet_loss.item())\n",
    "            train_label_loss.append(label_loss.item())\n",
    "            train_total_loss.append(total_loss.item())\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.argmax(pred_label, dim = 1) == torch.argmax(labels, dim = 1)).sum().item()\n",
    "        train_triplet_loss = sum(train_triplet_loss)/len(train_triplet_loss)\n",
    "        train_label_loss = sum(train_label_loss)/len(train_label_loss)\n",
    "        train_total_loss = sum(train_total_loss)/len(train_total_loss)\n",
    "        train_accuracy = correct/total\n",
    "        return train_triplet_loss, train_label_loss, train_total_loss, train_accuracy\n",
    "\n",
    "    def training_loop(self, train_data, test_data,train_mode,\n",
    "                      epochs, optimizer):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_triplet_losses = []\n",
    "        val_triplet_losses = []\n",
    "        train_label_losses = []\n",
    "        val_label_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        latent_separation_accuracy = 0\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "          train_triplet_loss, train_label_loss, train_total_loss, train_accuracy =self.train_epoch(train_data, optimizer, \n",
    "                                             train_mode)\n",
    "          test_triplet_loss, test_label_loss, test_total_loss, test_accuracy = self.test_epoch(test_data)\n",
    "          separation_accuracy = self.test_epoch_calculate_representation_separation(test_data)\n",
    "          train_losses.append(train_total_loss)\n",
    "          val_losses.append(test_total_loss)\n",
    "          train_triplet_losses.append(train_triplet_loss)\n",
    "          val_triplet_losses.append(test_triplet_loss)\n",
    "          train_label_losses.append(train_label_loss)\n",
    "          val_label_losses.append(test_label_loss)\n",
    "          train_accuracies.append(train_accuracy)\n",
    "          val_accuracies.append(test_accuracy)\n",
    "          wandb.log({\"train triplet loss\": train_triplet_loss, \n",
    "            \"train label loss\":train_label_loss, \n",
    "            \"validation triplet loss\":test_triplet_loss, \n",
    "            \"validation label loss\":test_label_loss, \n",
    "            \"total train loss\":train_total_loss, \n",
    "            \"total validation loss\":test_total_loss, \n",
    "            \"train label accuracy\":train_accuracy, \n",
    "            \"validation label accuracy\":test_accuracy,\n",
    "            'latent separation accuracy':separation_accuracy})\n",
    "        return train_triplet_losses, train_label_losses, val_triplet_losses, val_label_losses ,train_losses, val_losses, train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_A_ims = np.load(os.path.join(data_dir, 'set_A.npy'))\n",
    "set_B_ims = np.load(os.path.join(data_dir, 'set_B.npy'))\n",
    "set_C_ims= np.load(os.path.join(data_dir, 'set_C.npy'))\n",
    "set_A_labs = np.load(os.path.join(data_dir, 'set_A_labs.npy'))\n",
    "set_B_labs = np.load(os.path.join(data_dir, 'set_B_labs.npy'))\n",
    "set_C_labs = np.load(os.path.join(data_dir, 'set_C_labs.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###initialize weights and bias tracking\n",
    "def wandb_init(epochs, lr, train_mode, batch_size, model_number,data_set):\n",
    "  wandb.init(project=\"ConceptualAlignment2023\", entity=\"psych-711\",settings=wandb.Settings(start_method=\"thread\"))\n",
    "  wandb.config = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size, \n",
    "    # \"label_ratio\":label_ratio, \n",
    "    \"model_number\": model_number,\n",
    "    \"dataset\": data_set,\n",
    "    \"train_mode\":train_mode,\n",
    "  }\n",
    "  train_mode_dict = {0:'triplet', 1:'label', 2:'label_and_triplet'}\n",
    "  wandb.run.name = f'{data_set}_{train_mode_dict[train_mode]}_{model_number}'\n",
    "  wandb.run.save()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_code(save_dir, num_models, epochs, num_classes, batch_size,\n",
    "             lr, latent_dims):\n",
    "  if os.path.isdir(save_dir):\n",
    "    pass\n",
    "  else:\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "\n",
    "  test_intervals = [(540, 600), (1140, 1200), (1740, 1800), (2340, 2400)]\n",
    "\n",
    "  # initialize an empty list to hold the indices\n",
    "  val_indices = []\n",
    "\n",
    "  # loop through the intervals and append the indices to the list\n",
    "  for start, stop in test_intervals:\n",
    "      val_indices.extend(list(range(start, stop)))\n",
    "\n",
    "  train_indices = (np.setdiff1d(np.arange(2400),np.array(val_indices)))\n",
    "\n",
    "  np.random.seed(56)\n",
    "  contrast_indices  = np.concatenate((np.random.choice(np.arange(start=600, stop=2400), 600, replace=False),\n",
    "                np.random.choice(np.concatenate((np.arange(start=0, stop=600), np.arange(start=1200, stop=2400))), 600, replace=False),\n",
    "                np.random.choice(np.concatenate((np.arange(start=0, stop=1200), np.arange(start=1800, stop=2400))), 600, replace=False),\n",
    "                np.random.choice(np.arange(start=1800, stop=2400), 600, replace=False)))\n",
    "\n",
    "  for data_set in ['set_A','set_B','set_C']:\n",
    "    for train_mode in tqdm(range(3)):\n",
    "     # torch.manual_seed(0)\n",
    "      for model in range(num_models):\n",
    "        wandb_init(epochs, lr, train_mode, batch_size, model,data_set)\n",
    "\n",
    "        if data_set=='set_A':\n",
    "          train_data = TensorDataset(torch.tensor(set_A_ims.transpose(0,3,1,2)/255).float(), torch.tensor(set_A_ims[contrast_indices].transpose(0,3,1,2)/255).float(),\\\n",
    "                                     torch.tensor(set_A_labs).to(torch.int64))\n",
    "        elif data_set=='set_B':\n",
    "          train_data = TensorDataset(torch.tensor(set_B_ims.transpose(0,3,1,2)/255).float(), torch.tensor(set_B_ims[contrast_indices].transpose(0,3,1,2)/255).float(),\\\n",
    "                                     torch.tensor(set_B_labs).to(torch.int64))\n",
    "        elif data_set=='set_C':\n",
    "          train_data = TensorDataset(torch.tensor(set_C_ims.transpose(0,3,1,2)/255).float(), torch.tensor(set_C_ims[contrast_indices].transpose(0,3,1,2)/255).float(),\\\n",
    "                                     torch.tensor(set_C_labs).to(torch.int64))\n",
    "          \n",
    "        val_data = torch.utils.data.Subset(train_data, val_indices)\n",
    "        train_data = torch.utils.data.Subset(train_data, train_indices)\n",
    "       \n",
    "\n",
    "        train_data = torch.utils.data.DataLoader(train_data, \n",
    "                                                batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "        val_data = torch.utils.data.DataLoader(val_data, \n",
    "                                                batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "        \n",
    "     \n",
    "\n",
    "        train_obj = TrainModels(latent_dims, num_classes).to(device) # GPU\n",
    "        optimizer = torch.optim.Adam(train_obj.parameters(), lr=lr, weight_decay=1e-05)\n",
    "        train_triplet_losses, train_label_losses, \\\n",
    "          val_triplet_losses, val_label_losses, \\\n",
    "            train_losses, val_losses, train_accuracies, val_accuracies= train_obj.training_loop(train_data = train_data,\n",
    "                                                            test_data = val_data,\n",
    "                                                            epochs = epochs,\n",
    "                                                            optimizer = optimizer, \n",
    "                                                            train_mode = train_mode)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print('validation triplet loss:',val_triplet_losses,'validation total loss:',val_losses,'validation accuracy:',val_accuracies)\n",
    "        # wandb.log({\"train_img_loss\": train_img_loss, \n",
    "        #           \"train_label_loss\":train_label_loss, \n",
    "        #           \"val_img_loss\":val_img_loss, \n",
    "        #           \"val_label_loss\":val_label_loss, \n",
    "        #           \"train_losses\":train_losses, \n",
    "        #           \"val_losses\":val_losses, \n",
    "        #           \"train_accuracy\":train_accuracy, \n",
    "        #           \"val_accuracy\":val_accuracy})\n",
    "        train_mode_dict = {0:'triplet', 1:'label',2:'label_and_triplet' }\n",
    "        torch.save(train_obj.triplet_lab_model.state_dict(), os.path.join(save_dir,f'{data_set}_{train_mode_dict[train_mode]}_{model}.pth'))\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20230419_233437-s67a4h07</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/s67a4h07' target=\"_blank\">stilted-moon-19</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/s67a4h07' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/s67a4h07</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.38it/s]\n",
      " 33%|███▎      | 1/3 [00:13<00:26, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.8325868248939514, 0.23087778687477112, 0.13166537880897522, 0.11618342995643616, 0.12159474194049835, 0.10982739925384521, 0.1326916664838791, 0.10736203193664551, 0.09900834411382675, 0.09412360191345215] validation total loss: [1.5502538681030273, 0.9072749614715576, 0.790917158126831, 0.7833114862442017, 0.7844045162200928, 0.7281664609909058, 0.7635741233825684, 0.741872251033783, 0.7501118183135986, 0.73979252576828] validation accuracy: [0.19166666666666668, 0.2875, 0.3375, 0.2625, 0.325, 0.3958333333333333, 0.375, 0.4041666666666667, 0.2875, 0.2916666666666667]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:s67a4h07) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb481e6d54b744928eaaeb660b64b91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>█▇▁▇▇▆▆▅▅▅</td></tr><tr><td>total train loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>total validation loss</td><td>█▃▂▁▁▁▁▁▁▁</td></tr><tr><td>train label accuracy</td><td>▅▂▃▁▆▇▄█▅▆</td></tr><tr><td>train label loss</td><td>▇▃▅█▃▄▆▃▁▄</td></tr><tr><td>train triplet loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▁▄▆▃▅█▇█▄▄</td></tr><tr><td>validation label loss</td><td>█▅▄▄▄▁▂▂▃▃</td></tr><tr><td>validation triplet loss</td><td>█▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>0.98333</td></tr><tr><td>total train loss</td><td>0.70036</td></tr><tr><td>total validation loss</td><td>0.73979</td></tr><tr><td>train label accuracy</td><td>0.25741</td></tr><tr><td>train label loss</td><td>0.69452</td></tr><tr><td>train triplet loss</td><td>0.00584</td></tr><tr><td>validation label accuracy</td><td>0.29167</td></tr><tr><td>validation label loss</td><td>0.64567</td></tr><tr><td>validation triplet loss</td><td>0.09412</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stilted-moon-19</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/s67a4h07' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/s67a4h07</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230419_233437-s67a4h07/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:s67a4h07). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20230419_233451-0tkwgmp9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/0tkwgmp9' target=\"_blank\">splendid-plasma-20</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/0tkwgmp9' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/0tkwgmp9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.45it/s]\n",
      " 67%|██████▋   | 2/3 [00:28<00:14, 14.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.8526226282119751, 0.5901137590408325, 0.7049498558044434, 0.5987834334373474, 0.5266598463058472, 0.527089536190033, 0.5096761584281921, 0.5363163352012634, 0.5328738689422607, 0.5599735975265503] validation total loss: [1.5235861539840698, 0.6206023693084717, 1.1005301475524902, 0.6215917468070984, 0.5643811225891113, 0.531183123588562, 0.5219855308532715, 0.5683464407920837, 0.6509128212928772, 0.6315500736236572] validation accuracy: [0.25833333333333336, 0.9958333333333333, 0.8208333333333333, 0.9875, 0.9708333333333333, 1.0, 0.9916666666666667, 0.9833333333333333, 0.9541666666666667, 0.9416666666666667]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:0tkwgmp9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf0047693924e0e813d8ad98079b625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total train loss</td><td>█▄▄▃▂▁▁▁▁▁</td></tr><tr><td>total validation loss</td><td>█▂▅▂▁▁▁▁▂▂</td></tr><tr><td>train label accuracy</td><td>▁▇▇███████</td></tr><tr><td>train label loss</td><td>█▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train triplet loss</td><td>▁▆█▇▆▄▃▃▃▃</td></tr><tr><td>validation label accuracy</td><td>▁█▆██████▇</td></tr><tr><td>validation label loss</td><td>█▁▅▁▁▁▁▁▂▂</td></tr><tr><td>validation triplet loss</td><td>█▃▅▃▁▁▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.53727</td></tr><tr><td>total validation loss</td><td>0.63155</td></tr><tr><td>train label accuracy</td><td>0.99722</td></tr><tr><td>train label loss</td><td>0.00349</td></tr><tr><td>train triplet loss</td><td>0.53377</td></tr><tr><td>validation label accuracy</td><td>0.94167</td></tr><tr><td>validation label loss</td><td>0.07158</td></tr><tr><td>validation triplet loss</td><td>0.55997</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">splendid-plasma-20</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/0tkwgmp9' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/0tkwgmp9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230419_233451-0tkwgmp9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:0tkwgmp9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20230419_233506-nn13a56q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nn13a56q' target=\"_blank\">woven-deluge-21</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nn13a56q' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nn13a56q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.81it/s]\n",
      "100%|██████████| 3/3 [00:44<00:00, 14.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.8466342091560364, 0.16003479063510895, 0.10295314341783524, 0.06509903818368912, 0.06900302320718765, 0.07121720910072327, 0.06519393622875214, 0.06111343204975128, 0.055598046630620956, 0.037688929587602615] validation total loss: [1.4192085266113281, 0.3218994140625, 0.15820565819740295, 0.07530447840690613, 0.3620893955230713, 0.09107503294944763, 0.09839673340320587, 0.14086729288101196, 0.09662124514579773, 0.04296433925628662] validation accuracy: [0.25, 0.9625, 0.9791666666666666, 1.0, 0.7541666666666667, 0.9875, 0.9791666666666666, 0.9333333333333333, 0.9833333333333333, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:nn13a56q) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁▆████████</td></tr><tr><td>total train loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr><tr><td>total validation loss</td><td>█▂▂▁▃▁▁▁▁▁</td></tr><tr><td>train label accuracy</td><td>▁▆▇███████</td></tr><tr><td>train label loss</td><td>█▃▂▁▁▁▁▁▁▁</td></tr><tr><td>train triplet loss</td><td>█▅▄▃▂▂▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▁███▆██▇██</td></tr><tr><td>validation label loss</td><td>█▃▂▁▅▁▁▂▁▁</td></tr><tr><td>validation triplet loss</td><td>█▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.021</td></tr><tr><td>total validation loss</td><td>0.04296</td></tr><tr><td>train label accuracy</td><td>0.99907</td></tr><tr><td>train label loss</td><td>0.00179</td></tr><tr><td>train triplet loss</td><td>0.01921</td></tr><tr><td>validation label accuracy</td><td>1.0</td></tr><tr><td>validation label loss</td><td>0.00528</td></tr><tr><td>validation triplet loss</td><td>0.03769</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">woven-deluge-21</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nn13a56q' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nn13a56q</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230419_233506-nn13a56q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:nn13a56q). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20230419_233522-qg9p0u3p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/qg9p0u3p' target=\"_blank\">gentle-blaze-22</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/qg9p0u3p' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/qg9p0u3p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.91it/s]\n",
      " 33%|███▎      | 1/3 [00:18<00:37, 18.56s/it]/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (nn13a56q) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.6752011179924011, 0.14804071187973022, 0.12982496619224548, 0.1358948051929474, 0.0696510300040245, 0.0751727893948555, 0.08015242218971252, 0.07770232111215591, 0.07047251611948013, 0.06824380904436111] validation total loss: [1.2601779699325562, 0.7799314856529236, 0.7683043479919434, 0.7677327394485474, 0.7064144015312195, 0.7065895199775696, 0.7220096588134766, 0.7078208923339844, 0.7080687284469604, 0.703364372253418] validation accuracy: [0.3125, 0.19583333333333333, 0.1875, 0.20416666666666666, 0.22083333333333333, 0.2125, 0.14166666666666666, 0.1875, 0.17083333333333334, 0.17083333333333334]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qg9p0u3p) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aacb963de454fafb307bf4998bd1130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁▅▅██▅█▅██</td></tr><tr><td>total train loss</td><td>█▅▃▂▁▁▁▁▁▁</td></tr><tr><td>total validation loss</td><td>█▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train label accuracy</td><td>▄▁▄█▆▅▂▃▆▅</td></tr><tr><td>train label loss</td><td>▂█▃▁▁▃▆▆▅▆</td></tr><tr><td>train triplet loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>█▃▃▄▄▄▁▃▂▂</td></tr><tr><td>validation label loss</td><td>▁▇█▇▇▇█▇▇▇</td></tr><tr><td>validation triplet loss</td><td>█▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>0.99583</td></tr><tr><td>total train loss</td><td>0.65009</td></tr><tr><td>total validation loss</td><td>0.70336</td></tr><tr><td>train label accuracy</td><td>0.25231</td></tr><tr><td>train label loss</td><td>0.64392</td></tr><tr><td>train triplet loss</td><td>0.00617</td></tr><tr><td>validation label accuracy</td><td>0.17083</td></tr><tr><td>validation label loss</td><td>0.63512</td></tr><tr><td>validation triplet loss</td><td>0.06824</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-blaze-22</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/qg9p0u3p' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/qg9p0u3p</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230419_233522-qg9p0u3p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:qg9p0u3p). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7eadb5c01d34db596b12f28ca96a9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666917058173567, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20230419_233540-omgq504c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/omgq504c' target=\"_blank\">avid-butterfly-23</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/omgq504c' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/omgq504c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.12it/s]\n",
      " 67%|██████▋   | 2/3 [00:33<00:16, 16.33s/it]/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (nn13a56q) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.8811678886413574, 0.5942180752754211, 0.49946144223213196, 0.5024130344390869, 0.539466142654419, 0.4892040491104126, 0.5311083793640137, 0.4631440043449402, 0.4345147907733917, 0.4212682247161865] validation total loss: [1.5198936462402344, 1.1016626358032227, 0.6611021757125854, 0.5140487551689148, 0.5476621985435486, 0.5193187594413757, 0.6619729995727539, 0.4979228079319, 0.46300455927848816, 0.4213007986545563] validation accuracy: [0.2708333333333333, 0.4875, 0.8875, 0.9958333333333333, 0.9916666666666667, 0.9791666666666666, 0.9375, 0.9833333333333333, 0.9791666666666666, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:omgq504c) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6790fd262574d29b1d6c42cd1ac9033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁▅████████</td></tr><tr><td>total train loss</td><td>█▆▄▂▂▂▂▁▁▁</td></tr><tr><td>total validation loss</td><td>█▅▃▂▂▂▃▁▁▁</td></tr><tr><td>train label accuracy</td><td>▁▃▇███████</td></tr><tr><td>train label loss</td><td>█▅▃▁▁▁▁▁▁▁</td></tr><tr><td>train triplet loss</td><td>▁▆██▇▇▆▆▅▄</td></tr><tr><td>validation label accuracy</td><td>▁▃▇███▇███</td></tr><tr><td>validation label loss</td><td>█▇▃▁▁▁▂▁▁▁</td></tr><tr><td>validation triplet loss</td><td>█▄▂▂▃▂▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.46114</td></tr><tr><td>total validation loss</td><td>0.4213</td></tr><tr><td>train label accuracy</td><td>0.9963</td></tr><tr><td>train label loss</td><td>0.00396</td></tr><tr><td>train triplet loss</td><td>0.45718</td></tr><tr><td>validation label accuracy</td><td>1.0</td></tr><tr><td>validation label loss</td><td>3e-05</td></tr><tr><td>validation triplet loss</td><td>0.42127</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">avid-butterfly-23</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/omgq504c' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/omgq504c</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230419_233540-omgq504c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:omgq504c). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b443ce7de54869904e1c467802c5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670827070871988, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20230419_233555-gerjyg4s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/gerjyg4s' target=\"_blank\">soft-meadow-24</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/gerjyg4s' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/gerjyg4s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.12it/s]\n",
      "100%|██████████| 3/3 [00:53<00:00, 18.31s/it]/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (nn13a56q) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "100%|██████████| 3/3 [00:53<00:00, 18.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.8028333187103271, 0.13609068095684052, 0.12557488679885864, 0.08941909670829773, 0.10547396540641785, 0.08442936837673187, 0.06388864666223526, 0.06577518582344055, 0.07241745293140411, 0.06480774283409119] validation total loss: [1.3778021335601807, 0.20809820294380188, 0.26130321621894836, 0.09969512373209, 0.11797698587179184, 0.09200815856456757, 0.06567082554101944, 0.06679460406303406, 0.0747700035572052, 0.06721905618906021] validation accuracy: [0.3, 0.9875, 0.9166666666666666, 0.9916666666666667, 0.9875, 0.9958333333333333, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gerjyg4s) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17323ebb6803472c832220a5f7d7702b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total train loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr><tr><td>total validation loss</td><td>█▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train label accuracy</td><td>▁▆████████</td></tr><tr><td>train label loss</td><td>█▃▂▁▁▁▁▁▁▁</td></tr><tr><td>train triplet loss</td><td>█▅▃▂▂▂▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▁█▇███████</td></tr><tr><td>validation label loss</td><td>█▂▃▁▁▁▁▁▁▁</td></tr><tr><td>validation triplet loss</td><td>█▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.01274</td></tr><tr><td>total validation loss</td><td>0.06722</td></tr><tr><td>train label accuracy</td><td>1.0</td></tr><tr><td>train label loss</td><td>0.00069</td></tr><tr><td>train triplet loss</td><td>0.01205</td></tr><tr><td>validation label accuracy</td><td>1.0</td></tr><tr><td>validation label loss</td><td>0.00241</td></tr><tr><td>validation triplet loss</td><td>0.06481</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-meadow-24</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/gerjyg4s' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/gerjyg4s</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230419_233555-gerjyg4s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gerjyg4s). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd2ac8ab7494b29bf0ea4d416090b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670353322600324, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20230419_233616-32i0vcha</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/32i0vcha' target=\"_blank\">zesty-donkey-25</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/32i0vcha' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/32i0vcha</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.74it/s]\n",
      " 33%|███▎      | 1/3 [00:16<00:32, 16.37s/it]/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (gerjyg4s) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.6906775236129761, 0.34843140840530396, 0.23155581951141357, 0.07811810821294785, 0.09335731714963913, 0.11232209950685501, 0.09029556065797806, 0.10467633605003357, 0.05787605419754982, 0.08762605488300323] validation total loss: [1.3100078105926514, 0.9633222222328186, 0.8903534412384033, 0.7502300143241882, 0.7766430974006653, 0.763231635093689, 0.7351425886154175, 0.7551455497741699, 0.7184464335441589, 0.7308791875839233] validation accuracy: [0.19583333333333333, 0.3416666666666667, 0.25416666666666665, 0.25833333333333336, 0.22083333333333333, 0.2875, 0.2625, 0.26666666666666666, 0.25833333333333336, 0.2125]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:32i0vcha) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>██▅▆▅▅▆▆▁▅</td></tr><tr><td>total train loss</td><td>█▅▄▃▃▂▁▁▁▂</td></tr><tr><td>total validation loss</td><td>█▄▃▁▂▂▁▁▁▁</td></tr><tr><td>train label accuracy</td><td>▄▇▅▃▁▂▆█▇▂</td></tr><tr><td>train label loss</td><td>▁▂▃█▆▅▃▂▃▆</td></tr><tr><td>train triplet loss</td><td>█▅▄▃▂▁▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▁█▄▄▂▅▄▄▄▂</td></tr><tr><td>validation label loss</td><td>▁▁▅▇█▅▄▅▆▄</td></tr><tr><td>validation triplet loss</td><td>█▄▃▁▁▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>0.99167</td></tr><tr><td>total train loss</td><td>0.72329</td></tr><tr><td>total validation loss</td><td>0.73088</td></tr><tr><td>train label accuracy</td><td>0.22685</td></tr><tr><td>train label loss</td><td>0.71614</td></tr><tr><td>train triplet loss</td><td>0.00715</td></tr><tr><td>validation label accuracy</td><td>0.2125</td></tr><tr><td>validation label loss</td><td>0.64325</td></tr><tr><td>validation triplet loss</td><td>0.08763</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-donkey-25</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/32i0vcha' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/32i0vcha</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230419_233616-32i0vcha/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:32i0vcha). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bf1c8b3a4748e9a7ebafadf41bf792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016671004837068418, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20230419_233632-j9q04roy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/j9q04roy' target=\"_blank\">resilient-dew-26</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/j9q04roy' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/j9q04roy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.17it/s]\n",
      " 67%|██████▋   | 2/3 [00:33<00:17, 17.06s/it]/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (gerjyg4s) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.7550578117370605, 0.6421393752098083, 0.6172167658805847, 0.5402910113334656, 0.5789048671722412, 0.5647134184837341, 0.5232914686203003, 0.5422380566596985, 0.5176268815994263, 0.5174731016159058] validation total loss: [1.11830735206604, 0.7451427578926086, 0.8025115728378296, 0.5412409901618958, 0.6013852953910828, 0.5834274291992188, 0.5234407782554626, 0.5433118939399719, 0.5177183747291565, 0.5175617933273315] validation accuracy: [0.575, 0.9875, 0.8791666666666667, 1.0, 0.9833333333333333, 0.9958333333333333, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:j9q04roy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁█████████</td></tr><tr><td>total train loss</td><td>█▆▃▂▂▂▂▁▁▁</td></tr><tr><td>total validation loss</td><td>█▄▄▁▂▂▁▁▁▁</td></tr><tr><td>train label accuracy</td><td>▁▆████████</td></tr><tr><td>train label loss</td><td>█▄▂▁▁▁▁▁▁▁</td></tr><tr><td>train triplet loss</td><td>▁▇█▇▆▆▆▅▄▄</td></tr><tr><td>validation label accuracy</td><td>▁█▆███████</td></tr><tr><td>validation label loss</td><td>█▃▅▁▁▁▁▁▁▁</td></tr><tr><td>validation triplet loss</td><td>█▅▄▂▃▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.53714</td></tr><tr><td>total validation loss</td><td>0.51756</td></tr><tr><td>train label accuracy</td><td>0.99815</td></tr><tr><td>train label loss</td><td>0.0034</td></tr><tr><td>train triplet loss</td><td>0.53374</td></tr><tr><td>validation label accuracy</td><td>1.0</td></tr><tr><td>validation label loss</td><td>9e-05</td></tr><tr><td>validation triplet loss</td><td>0.51747</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resilient-dew-26</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/j9q04roy' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/j9q04roy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230419_233632-j9q04roy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:j9q04roy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700aa4766240476c883915bb86d5dcf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670724532256525, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20230419_233650-1jr0wq6o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/1jr0wq6o' target=\"_blank\">bumbling-glade-27</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/1jr0wq6o' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/1jr0wq6o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.72it/s]\n",
      "100%|██████████| 3/3 [00:55<00:00, 19.00s/it]/home/kushinm/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (gerjyg4s) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "100%|██████████| 3/3 [00:55<00:00, 18.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.7328863143920898, 0.12930816411972046, 0.09532110393047333, 0.046415071934461594, 0.04093194007873535, 0.05198988318443298, 0.0608380101621151, 0.04322240874171257, 0.031808458268642426, 0.06013976410031319] validation total loss: [1.2565560340881348, 0.2879713773727417, 0.11753387749195099, 0.06095512956380844, 0.04550860449671745, 0.052376218140125275, 0.08768932521343231, 0.045577868819236755, 0.04412916675209999, 0.12293985486030579] validation accuracy: [0.4375, 0.875, 0.9791666666666666, 0.9958333333333333, 1.0, 1.0, 0.9875, 1.0, 0.9958333333333333, 0.95]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁█████████</td></tr><tr><td>total train loss</td><td>█▄▂▁▁▁▁▁▁▁</td></tr><tr><td>total validation loss</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train label accuracy</td><td>▁▆▇███████</td></tr><tr><td>train label loss</td><td>█▃▂▁▁▁▁▁▁▁</td></tr><tr><td>train triplet loss</td><td>█▅▃▂▂▁▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▁▆███████▇</td></tr><tr><td>validation label loss</td><td>█▃▁▁▁▁▁▁▁▂</td></tr><tr><td>validation triplet loss</td><td>█▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.03143</td></tr><tr><td>total validation loss</td><td>0.12294</td></tr><tr><td>train label accuracy</td><td>0.99259</td></tr><tr><td>train label loss</td><td>0.01059</td></tr><tr><td>train triplet loss</td><td>0.02084</td></tr><tr><td>validation label accuracy</td><td>0.95</td></tr><tr><td>validation label loss</td><td>0.0628</td></tr><tr><td>validation triplet loss</td><td>0.06014</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bumbling-glade-27</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/1jr0wq6o' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/1jr0wq6o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230419_233650-1jr0wq6o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "\n",
    "num_classes = 4 # Number of unique class labels in the dataset\n",
    "latent_dims = 64\n",
    "epochs = 10\n",
    "lr = 0.005\n",
    "num_models = 1\n",
    "batch_size = 256\n",
    "save_dir = save_dir\n",
    "main_code(save_dir, num_models, epochs, num_classes, batch_size,\n",
    "             lr, latent_dims)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5ec221b8ce1ddc6eafacfbc77a75e3f09c9fea6e76ac8503f9810425480e77e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
