{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# base_dir = os.path.abspath('/mnt/ws/home/xyu/ConceptualAlignmentLanguage/tripletNCE')\n",
    "base_dir = os.path.abspath('/mnt/dv/wid/projects3/Rogers-nsf-ind-diff/sid/Projects/ConceptualAlignmentLanguage/tripletNCE')\n",
    "save_dir = os.path.join(base_dir,'results')\n",
    "data_dir = os.path.join(base_dir,'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.manual_seed(0)\n",
    "# import wandb\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "from torch.utils.data import TensorDataset,Dataset, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# from neurora.rdm_corr import rdm_correlation_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLabelModel(nn.Module):\n",
    "    def __init__(self, encoded_space_dim=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        \"\"\n",
    "        ### Convolutional section\n",
    "       ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "    \n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        ### Linear section\n",
    "        ## changed 32*4*4 to 32*2*2\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            nn.Linear(32*2*2, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, encoded_space_dim)\n",
    "        )\n",
    "\n",
    "        ## triplet projection module\n",
    "        self.decoder_triplet_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 32),\n",
    "            nn.ReLU(True)\n",
    "         \n",
    "        )\n",
    "        ##labeling module\n",
    "        self.decoder_labels_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16, num_classes),\n",
    "        )\n",
    "\n",
    "        ### initialize weights using xavier initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        batch_s = x.size(0)\n",
    "        img_features = self.encoder_cnn(x)\n",
    "        img_features = self.flatten(img_features)\n",
    "        \n",
    "        enc_latent = self.encoder_lin(img_features)\n",
    "\n",
    "        triplet_latent = self.decoder_triplet_lin(enc_latent)\n",
    "        label = self.decoder_labels_lin(enc_latent)\n",
    "        # label = F.softmax(label,dim=1)\n",
    "        return enc_latent, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### custom loss computing triplet loss and labeling loss\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, margin=10):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, anchor, positive, negative, label, pred_label):\n",
    "        cosine_sim = torch.nn.CosineSimilarity(1)\n",
    "        # distance_positive = torch.tensor(1)-cosine_sim(anchor,positive)\n",
    "   \n",
    "        # distance_negative = torch.tensor(1)-cosine_sim(anchor,negative)\n",
    "\n",
    "        # triplet_loss = torch.maximum(distance_positive - distance_negative + self.margin, torch.tensor(0))\n",
    "        # triplet_loss = torch.sum(triplet_loss)\n",
    "        triplet_loss = (nn.TripletMarginWithDistanceLoss( distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)))\n",
    "        triplet_loss = triplet_loss(anchor, positive, negative)\n",
    "        label_loss = F.binary_cross_entropy_with_logits(pred_label.float(), label.float())\n",
    "        total_loss = triplet_loss + label_loss\n",
    "        return triplet_loss, label_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = TripletLabelModel()\n",
    "# cifar_model_path = '../../data/CIFAR10_NCE_i_1e-05_50.pth'\n",
    "# t.load_state_dict(torch.load(cifar_model_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrainModels(nn.Module):\n",
    "    def __init__(self, latent_dims, num_classes, weights_path=None):\n",
    "        super(TrainModels, self).__init__()\n",
    "        self.triplet_lab_model = TripletLabelModel(latent_dims, 10) ### load cifar model\n",
    "        if weights_path!=None:\n",
    "            cifar_model_path = '/mnt/ws/home/xyu/ConceptualAlignmentLanguage/tripletNCE/data/CIFAR10_NCE_i_1e-05_50.pth'\n",
    "            self.triplet_lab_model.load_state_dict(torch.load(cifar_model_path))\n",
    "            self.triplet_lab_model.decoder_labels_lin[4] = nn.Linear(16, num_classes)\n",
    "        self.custom_loss = CustomLoss()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, anchor_im, positive_im, negative_im):\n",
    "        anchor_latent, anchor_label = self.triplet_lab_model(anchor_im)\n",
    "        positive_latent, _ = self.triplet_lab_model(positive_im)\n",
    "        negative_latent, _ = self.triplet_lab_model(negative_im)\n",
    "\n",
    "        return anchor_latent, positive_latent, negative_latent, anchor_label\n",
    "\n",
    "    def test_epoch(self, test_data):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "        self.eval()\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            # Define the lists to store the outputs for each batch\n",
    "            test_triplet_loss = []\n",
    "            test_label_loss = []\n",
    "            test_total_loss = []\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for anchor_ims, contrast_ims, labels in test_data:\n",
    "                # Move tensor to the proper device\n",
    "                anchor_ims = anchor_ims.to(device)\n",
    "                contrast_ims = contrast_ims.to(device)\n",
    "                labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "                labels = labels.to(device)\n",
    "                anchor_latent, positive_latent, negative_latent, pred_label = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "                # Append the network output and the original image to the lists\n",
    "                triplet_loss, label_loss, total_loss = self.custom_loss(anchor_latent,\n",
    "                                                                positive_latent, \n",
    "                                                                negative_latent, \n",
    "                                                                labels,\n",
    "                                                                pred_label)\n",
    "                total += labels.size(0)\n",
    "                correct += (torch.argmax(pred_label, dim = 1) == torch.argmax(labels, dim = 1)).sum().item()\n",
    "                test_triplet_loss.append(triplet_loss.item())\n",
    "                test_label_loss.append(label_loss.item())\n",
    "                test_total_loss.append(total_loss.item())\n",
    "        test_triplet_loss = sum(test_triplet_loss)/len(test_triplet_loss)\n",
    "        test_label_loss = sum(test_label_loss)/len(test_label_loss)\n",
    "        test_total_loss = sum(test_total_loss)/len(test_total_loss)\n",
    "        test_accuracy = correct/total\n",
    "        return test_triplet_loss, test_label_loss, test_total_loss, test_accuracy\n",
    "\n",
    "    def test_epoch_calculate_representation_separation(self, test_data):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "        self.eval()\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            accuracies = []\n",
    "            for anchor_ims, contrast_ims, labels in test_data:\n",
    "                # Move tensor to the proper device\n",
    "                anchor_ims = anchor_ims.to(device)\n",
    "                contrast_ims = contrast_ims.to(device)\n",
    "                # labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "                # labels = labels.to(device)\n",
    "                anchor_latent, _, _, _ = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "                # use sklearn to predict labels from anchor_latent\n",
    "                # calculate accuracy\n",
    "                # x's are anchor_latent and y's are labels\n",
    "                # append accuracy to list\n",
    "                # put anchor_latent and labels on cpu and convert to numpy\n",
    "\n",
    "          \n",
    "                anchor_latent = anchor_latent.cpu().numpy()\n",
    "                ### standard scale the data in anchor_latent before fitting to the model\n",
    "                anchor_latent = StandardScaler().fit_transform(anchor_latent)\n",
    "                labels = labels.cpu().numpy()\n",
    "                \n",
    "                lm = linear_model.LogisticRegression()\n",
    "                lm.fit(anchor_latent, labels)\n",
    "                # convert labels to sklearn format\n",
    "                accuracies.append(lm.score(anchor_latent, labels))\n",
    "        accuracy = sum(accuracies)/len(accuracies)\n",
    "        return accuracy\n",
    "\n",
    "    def train_epoch(self, train_data, optimizer, train_mode):\n",
    "        self.train()\n",
    "        train_triplet_loss = []\n",
    "        train_label_loss = []\n",
    "        train_total_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for anchor_ims, contrast_ims, labels in train_data:\n",
    "            \n",
    "            anchor_ims = anchor_ims.to(device)\n",
    "            contrast_ims = contrast_ims.to(device)\n",
    "            labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            anchor_latent, positive_latent, negative_latent, pred_label = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "           \n",
    "           \n",
    "           \n",
    "            triplet_loss, label_loss, total_loss = self.custom_loss(anchor_latent,\n",
    "                                                                positive_latent, \n",
    "                                                                negative_latent, \n",
    "                                                                labels,\n",
    "                                                                pred_label)\n",
    "            \n",
    "            \n",
    "            if train_mode==0:\n",
    "                triplet_loss.backward()\n",
    "            elif train_mode==1:\n",
    "                label_loss.backward()\n",
    "            elif train_mode==2:\n",
    "                total_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_triplet_loss.append(triplet_loss.item())\n",
    "            train_label_loss.append(label_loss.item())\n",
    "            train_total_loss.append(total_loss.item())\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.argmax(pred_label, dim = 1) == torch.argmax(labels, dim = 1)).sum().item()\n",
    "        train_triplet_loss = sum(train_triplet_loss)/len(train_triplet_loss)\n",
    "        train_label_loss = sum(train_label_loss)/len(train_label_loss)\n",
    "        train_total_loss = sum(train_total_loss)/len(train_total_loss)\n",
    "        train_accuracy = correct/total\n",
    "        return train_triplet_loss, train_label_loss, train_total_loss, train_accuracy\n",
    "\n",
    "    def training_loop(self, train_data, test_data,train_mode,\n",
    "                      epochs, optimizer):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_triplet_losses = []\n",
    "        val_triplet_losses = []\n",
    "        train_label_losses = []\n",
    "        val_label_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        latent_separation_accuracy = 0\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "          train_triplet_loss, train_label_loss, train_total_loss, train_accuracy =self.train_epoch(train_data, optimizer, \n",
    "                                             train_mode)\n",
    "          test_triplet_loss, test_label_loss, test_total_loss, test_accuracy = self.test_epoch(test_data)\n",
    "          separation_accuracy = self.test_epoch_calculate_representation_separation(test_data)\n",
    "          train_losses.append(train_total_loss)\n",
    "          val_losses.append(test_total_loss)\n",
    "          train_triplet_losses.append(train_triplet_loss)\n",
    "          val_triplet_losses.append(test_triplet_loss)\n",
    "          train_label_losses.append(train_label_loss)\n",
    "          val_label_losses.append(test_label_loss)\n",
    "          train_accuracies.append(train_accuracy)\n",
    "          val_accuracies.append(test_accuracy)\n",
    "          wandb.log({\"train triplet loss\": train_triplet_loss, \n",
    "            \"train label loss\":train_label_loss, \n",
    "            \"validation triplet loss\":test_triplet_loss, \n",
    "            \"validation label loss\":test_label_loss, \n",
    "            \"total train loss\":train_total_loss, \n",
    "            \"total validation loss\":test_total_loss, \n",
    "            \"train label accuracy\":train_accuracy, \n",
    "            \"validation label accuracy\":test_accuracy,\n",
    "            'latent separation accuracy':separation_accuracy})\n",
    "        return train_triplet_losses, train_label_losses, val_triplet_losses, val_label_losses ,train_losses, val_losses, train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_A_ims = np.load(os.path.join(data_dir, 'set_A.npy'))\n",
    "set_B_ims = np.load(os.path.join(data_dir, 'set_B.npy'))\n",
    "set_C_ims = np.load(os.path.join(data_dir, 'set_C.npy'))\n",
    "set_A_labs = np.load(os.path.join(data_dir, 'set_A_labs.npy'))\n",
    "set_B_labs = np.load(os.path.join(data_dir, 'set_B_labs.npy'))\n",
    "set_C_labs = np.load(os.path.join(data_dir, 'set_C_labs.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_A_sub_ims =[]\n",
    "set_B_sub_ims =[]\n",
    "\n",
    "set_C_sub_ims =[]\n",
    "\n",
    "set_A_sub_labs =[]\n",
    "set_B_sub_labs =[]\n",
    "set_C_sub_labs =[]\n",
    "\n",
    "\n",
    "for i in range (4):\n",
    "    sub_main = set_A_ims[i*600:(i*600)+600]\n",
    "    labels_main = set_A_labs[i*600:(i*600)+600]\n",
    "    np.random.seed(711)\n",
    "    np.random.shuffle(sub_main)\n",
    "    np.random.seed(711)\n",
    "    np.random.shuffle(labels_main)\n",
    "\n",
    "    set_A_sub_ims.append(sub_main[:30])\n",
    "    set_B_sub_ims.append(sub_main[:15])\n",
    "    set_B_sub_ims.append(sub_main[30:45])\n",
    "    set_C_sub_ims.append(sub_main[35:65])\n",
    "\n",
    "    set_A_sub_labs.append(labels_main[:30])\n",
    "    set_B_sub_labs.append(labels_main[:15])\n",
    "    set_B_sub_labs.append(labels_main[30:45])\n",
    "    set_C_sub_labs.append(labels_main[35:65])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##flatten set_A_sub_ims into an array of shape 120,64,64,3\n",
    "set_A_sub_ims = np.concatenate(set_A_sub_ims)\n",
    "set_B_sub_ims = np.concatenate(set_B_sub_ims)\n",
    "set_C_sub_ims = np.concatenate(set_C_sub_ims)\n",
    "\n",
    "set_A_sub_labs = np.concatenate(set_A_sub_labs)\n",
    "set_B_sub_labs = np.concatenate(set_B_sub_labs)\n",
    "set_C_sub_labs = np.concatenate(set_C_sub_labs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A-B: 50% \\\n",
    "A-C: 0% \\\n",
    "B-C: 33.33%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###initialize weights and bias tracking\n",
    "def wandb_init(epochs, lr, train_mode, batch_size, model_number,data_set):\n",
    "    wandb.init(project=\"ConceptualAlignment\", settings=wandb.Settings(start_method=\"thread\"))\n",
    "    wandb.config = {\n",
    "      \"learning_rate\": lr,\n",
    "      \"epochs\": epochs,\n",
    "      \"batch_size\": batch_size, \n",
    "      # \"label_ratio\":label_ratio, \n",
    "      \"model_number\": model_number,\n",
    "      \"dataset\": data_set,\n",
    "      \"train_mode\":train_mode,\n",
    "    }\n",
    "    train_mode_dict = {0:'triplet', 1:'label', 2:'label_and_triplet'}\n",
    "    wandb.run.name = f'{data_set}_{train_mode_dict[train_mode]}_{model_number}'\n",
    "    wandb.run.save()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_code(save_dir, num_models, epochs, num_classes, batch_size,\n",
    "             lr, latent_dims):\n",
    "  if os.path.isdir(save_dir):\n",
    "    pass\n",
    "  else:\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "  np.random.seed(42)\n",
    "  torch.manual_seed(42)\n",
    "  \n",
    "  # test_intervals = [(540, 600), (1140, 1200), (1740, 1800), (2340, 2400)]\n",
    "  test_intervals = [(25, 30), (55, 60), (85, 90), (115, 120)]\n",
    "  # initialize an empty list to hold the indices\n",
    "  val_indices = []\n",
    "\n",
    "  # loop through the intervals and append the indices to the list\n",
    "  for start, stop in test_intervals:\n",
    "      val_indices.extend(list(range(start, stop)))\n",
    "\n",
    "  # train_indices = (np.setdiff1d(np.arange(2400),np.array(val_indices)))\n",
    "  train_indices = (np.setdiff1d(np.arange(120),np.array(val_indices)))\n",
    "\n",
    "  # np.random.seed(56)\n",
    "  # contrast_indices  = np.concatenate((np.random.choice(np.arange(start=600, stop=2400), 600, replace=False),\n",
    "  #               np.random.choice(np.concatenate((np.arange(start=0, stop=600), np.arange(start=1200, stop=2400))), 600, replace=False),\n",
    "  #               np.random.choice(np.concatenate((np.arange(start=0, stop=1200), np.arange(start=1800, stop=2400))), 600, replace=False),\n",
    "  #               np.random.choice(np.arange(start=1800, stop=2400), 600, replace=False)))\n",
    "  contrast_indices  = np.concatenate((np.random.choice(np.arange(start=30, stop=120), 30, replace=False),\n",
    "                np.random.choice(np.concatenate((np.arange(start=0, stop=30), np.arange(start=60, stop=120))), 30, replace=False),\n",
    "                np.random.choice(np.concatenate((np.arange(start=0, stop=60), np.arange(start=90, stop=120))), 30, replace=False),\n",
    "                np.random.choice(np.arange(start=0, stop=90), 30, replace=False)))\n",
    "\n",
    "  # for data_set in ['set_A','set_A2','set_B','set_C']:\n",
    "  for data_set in ['set_A']:\n",
    "    for train_mode in tqdm(range(2, 3)):\n",
    "     # torch.manual_seed(0)\n",
    "      for model in range(num_models):\n",
    "        wandb_init(epochs, lr, train_mode, batch_size, model,data_set)\n",
    "        weights_path = f'../../data/cifar_models/m{model}.pth'\n",
    "\n",
    "        # if data_set=='set_A':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_A_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_A_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_A_labs).to(torch.int64))\n",
    "        # elif data_set=='set_B':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_B_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_B_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_B_labs).to(torch.int64))\n",
    "        # elif data_set=='set_C':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_C_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_C_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_C_labs).to(torch.int64))\n",
    "        # if data_set=='set_A2':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_A_sub_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_A_sub_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_A_sub_labs).to(torch.int64))\n",
    "        if data_set=='set_A':\n",
    "          train_data = TensorDataset(Resize(32)(torch.tensor(set_A_sub_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_A_sub_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "                                     torch.tensor(set_A_sub_labs).to(torch.int64))\n",
    "        # elif data_set=='set_B':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_B_sub_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_B_sub_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_B_sub_labs).to(torch.int64))\n",
    "        # elif data_set=='set_C':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_C_sub_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_C_sub_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_C_sub_labs).to(torch.int64))\n",
    "        \n",
    "        train_size = int(0.7 * len(train_data))\n",
    "        val_size = len(train_data) - train_size\n",
    "        \n",
    "        train_data, val_data = random_split(train_data, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "       \n",
    "\n",
    "        train_data = torch.utils.data.DataLoader(train_data, \n",
    "                                                batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "        val_data = torch.utils.data.DataLoader(val_data, \n",
    "                                                batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "        \n",
    "\n",
    "\n",
    "        train_obj = TrainModels(latent_dims, num_classes, weights_path).to(device) # GPU\n",
    "        optimizer = torch.optim.Adam(train_obj.parameters(), lr=lr, weight_decay=1e-05)\n",
    "        train_triplet_losses, train_label_losses, \\\n",
    "          val_triplet_losses, val_label_losses, \\\n",
    "            train_losses, val_losses, train_accuracies, val_accuracies= train_obj.training_loop(train_data = train_data,\n",
    "                                                            test_data = val_data,\n",
    "                                                            epochs = epochs,\n",
    "                                                            optimizer = optimizer, \n",
    "                                                            train_mode = train_mode)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        print('validation triplet loss:',val_triplet_losses[-1],'validation total loss:',val_losses[-1],'validation accuracy:',val_accuracies[-1])\n",
    "        # wandb.log({\"train_img_loss\": train_img_loss, \n",
    "        #           \"train_label_loss\":train_label_loss, \n",
    "        #           \"val_img_loss\":val_img_loss, \n",
    "        #           \"val_label_loss\":val_label_loss, \n",
    "        #           \"train_losses\":train_losses, \n",
    "        #           \"val_losses\":val_losses, \n",
    "        #           \"train_accuracy\":train_accuracy, \n",
    "        #           \"val_accuracy\":val_accuracy})\n",
    "        train_mode_dict = {0:'triplet', 1:'label',2:'label_and_triplet' }\n",
    "        torch.save(train_obj.triplet_lab_model.state_dict(), os.path.join(save_dir,f'{model}_{data_set}_{train_mode_dict[train_mode]}_{round(val_accuracies[-1], 3)}.pth'))\n",
    "  return val_data\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuxizheng\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/xyu/ConceptualAlignmentLanguage/tripletNCE/wandb/run-20240123_200008-6kfk0npu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yuxizheng/ConceptualAlignment/runs/6kfk0npu' target=\"_blank\">comfy-water-52</a></strong> to <a href='https://wandb.ai/yuxizheng/ConceptualAlignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yuxizheng/ConceptualAlignment' target=\"_blank\">https://wandb.ai/yuxizheng/ConceptualAlignment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yuxizheng/ConceptualAlignment/runs/6kfk0npu' target=\"_blank\">https://wandb.ai/yuxizheng/ConceptualAlignment/runs/6kfk0npu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "100%|██████████| 1000/1000 [00:27<00:00, 36.85it/s]\n",
      "100%|██████████| 1/1 [00:32<00:00, 32.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: 0.10054220259189606 validation total loss: 0.995544970035553 validation accuracy: 0.8611111111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>██████████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total train loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total validation loss</td><td>█▃▇▆▆▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▃▁▂▅█▇▆▆▆▆▆▅▅▅▅▅</td></tr><tr><td>train label accuracy</td><td>▁███████████████████████████████████████</td></tr><tr><td>train label loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train triplet loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▁▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇████████████</td></tr><tr><td>validation label loss</td><td>▂▃▆▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▃▃▃▁▃▆█▇▇▆▆▆▆▆▆▆▆▅</td></tr><tr><td>validation triplet loss</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>0.97222</td></tr><tr><td>total train loss</td><td>0.0</td></tr><tr><td>total validation loss</td><td>0.99554</td></tr><tr><td>train label accuracy</td><td>1.0</td></tr><tr><td>train label loss</td><td>0.0</td></tr><tr><td>train triplet loss</td><td>0.0</td></tr><tr><td>validation label accuracy</td><td>0.86111</td></tr><tr><td>validation label loss</td><td>0.895</td></tr><tr><td>validation triplet loss</td><td>0.10054</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comfy-water-52</strong> at: <a href='https://wandb.ai/yuxizheng/ConceptualAlignment/runs/6kfk0npu' target=\"_blank\">https://wandb.ai/yuxizheng/ConceptualAlignment/runs/6kfk0npu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240123_200008-6kfk0npu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "\n",
    "num_classes = 4 # Number of unique class labels in the dataset\n",
    "latent_dims = 64\n",
    "epochs = 1000\n",
    "lr = 0.005\n",
    "num_models = 1\n",
    "batch_size = 256\n",
    "save_dir = save_dir\n",
    "val_data = main_code(save_dir, num_models, epochs, num_classes, batch_size,\n",
    "             lr, latent_dims)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     31\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m anchor_ims, contrast_ims, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mval_data\u001b[49m:\n\u001b[1;32m     33\u001b[0m     anchor_ims \u001b[38;5;241m=\u001b[39m anchor_ims\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     34\u001b[0m     contrast_ims \u001b[38;5;241m=\u001b[39m contrast_ims\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_data' is not defined"
     ]
    }
   ],
   "source": [
    "class MaxLogitsLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxLogitsLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        max_logits = torch.sum(logits * targets, dim=1)\n",
    "        loss = -max_logits.mean()\n",
    "        return loss\n",
    "\n",
    "model_path = {40: \"./results/0_set_A_label_and_triplet_0.417.pth\",\n",
    "              50: \"./results/0_set_A_label_and_triplet_0.528.pth\",\n",
    "              60: \"./results/0_set_A_label_and_triplet_0.639.pth\",\n",
    "              70: \"./results/0_set_A_label_and_triplet_0.722.pth\",\n",
    "              80: \"./results/0_set_A_label_and_triplet_0.806.pth\"}\n",
    "\n",
    "chooses = [40, 50, 60, 70, 80]\n",
    "\n",
    "for choose in chooses:\n",
    "    triplet_lab_model = TripletLabelModel(latent_dims, 4)\n",
    "    triplet_lab_model.load_state_dict(torch.load(model_path[choose]))\n",
    "    triplet_lab_model = triplet_lab_model.to(device)\n",
    "    criterion = MaxLogitsLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(triplet_lab_model.parameters(), lr=0.001)\n",
    "    num_epochs = 50\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for anchor_ims, contrast_ims, labels in val_data:\n",
    "            anchor_ims = anchor_ims.to(device)\n",
    "            contrast_ims = contrast_ims.to(device)\n",
    "            labels = F.one_hot(labels, num_classes=4)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            _, label_logits = triplet_lab_model(anchor_ims)\n",
    "            loss = criterion(label_logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(label_logits.data, 1)\n",
    "            _, labels_max = torch.max(labels.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels_max).sum().item()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "        \n",
    "        # print(f\"Epoch {epoch}, Loss: {loss.item()}, Accuracy: {accuracy}%\")\n",
    "\n",
    "    print(f\"Pretrained Model: {choose}, Epoch {epoch}, Loss: {loss.item()}, Accuracy: {accuracy}%\")\n",
    "    save_path = os.path.join(save_dir, f'0_set_A_maxlogits_acc_{choose}.pth')\n",
    "    torch.save(triplet_lab_model.state_dict(), save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want the final layer logits , compute error based on the maximum logits ground truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data_to_save = []\n",
    "\n",
    "for batch in val_data:\n",
    "    # Here, we assume each batch contains data and labels\n",
    "    anchor_ims, contrast_ims, labels = batch\n",
    "    # Convert tensors to numpy arrays or another suitable format for saving\n",
    "    data_to_save.append((anchor_ims.numpy(), contrast_ims.numpy(), labels.numpy()))\n",
    "\n",
    "# Save the data to a file\n",
    "with open('val_data.pkl', 'wb') as file:\n",
    "    pickle.dump(data_to_save, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sid changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pickle.load(open('val_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_21655/377933159.py\u001b[0m(42)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     41 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 42 \u001b[0;31m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     43 \u001b[0;31m            \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3493e-01, 3.8855e-01, 2.5582e-02, 4.5094e-01],\n",
      "        [3.6291e-01, 1.2532e-01, 6.3376e-02, 4.4839e-01],\n",
      "        [3.7697e-01, 1.3558e-01, 1.0663e-02, 4.7679e-01],\n",
      "        [2.7803e-01, 3.3652e-01, 5.9843e-02, 3.2560e-01],\n",
      "        [8.8920e-02, 1.7551e-01, 2.1351e-02, 7.1422e-01],\n",
      "        [2.7628e-01, 4.2909e-01, 9.8371e-02, 1.9626e-01],\n",
      "        [1.1462e-01, 1.6420e-01, 5.8876e-01, 1.3242e-01],\n",
      "        [6.9496e-02, 5.9394e-01, 4.7910e-02, 2.8865e-01],\n",
      "        [4.1053e-02, 3.1276e-02, 8.5095e-03, 9.1916e-01],\n",
      "        [3.2236e-01, 8.0509e-02, 2.3023e-02, 5.7411e-01],\n",
      "        [3.7139e-02, 1.9197e-01, 5.6107e-01, 2.0982e-01],\n",
      "        [4.8098e-02, 1.5658e-02, 7.6736e-03, 9.2857e-01],\n",
      "        [3.3845e-01, 6.9600e-02, 7.1299e-03, 5.8482e-01],\n",
      "        [2.2335e-01, 3.9030e-01, 8.9676e-02, 2.9668e-01],\n",
      "        [3.4253e-04, 9.6428e-03, 9.8696e-01, 3.0547e-03],\n",
      "        [7.3291e-02, 2.6609e-01, 1.8252e-01, 4.7810e-01],\n",
      "        [1.2007e-04, 4.2824e-03, 9.9516e-01, 4.3293e-04],\n",
      "        [1.3500e-04, 5.1713e-03, 9.9442e-01, 2.6886e-04],\n",
      "        [4.3171e-01, 1.1248e-01, 1.6485e-02, 4.3933e-01],\n",
      "        [3.2522e-01, 4.2272e-01, 3.7246e-02, 2.1482e-01],\n",
      "        [3.9540e-01, 2.9503e-01, 2.7573e-02, 2.8200e-01],\n",
      "        [1.0027e-03, 4.0008e-03, 9.9349e-01, 1.5059e-03],\n",
      "        [1.8667e-01, 2.1813e-01, 1.4309e-01, 4.5212e-01],\n",
      "        [1.6560e-01, 6.0016e-01, 8.5551e-03, 2.2569e-01],\n",
      "        [9.9056e-03, 4.3596e-02, 9.3673e-01, 9.7726e-03],\n",
      "        [4.7298e-03, 1.3931e-01, 8.3403e-01, 2.1931e-02],\n",
      "        [3.4448e-02, 4.4108e-02, 4.1342e-03, 9.1731e-01],\n",
      "        [2.3546e-01, 4.3861e-01, 1.6547e-01, 1.6046e-01],\n",
      "        [2.5699e-01, 4.7802e-01, 2.1780e-02, 2.4321e-01],\n",
      "        [3.1011e-01, 3.9839e-01, 4.8058e-03, 2.8669e-01],\n",
      "        [1.5351e-02, 5.6997e-01, 7.7021e-02, 3.3765e-01],\n",
      "        [3.7560e-03, 6.5152e-04, 1.3511e-05, 9.9558e-01],\n",
      "        [1.0718e-01, 2.5483e-01, 5.2945e-01, 1.0854e-01],\n",
      "        [1.7687e-01, 4.6180e-01, 1.7155e-01, 1.8978e-01],\n",
      "        [1.5601e-01, 5.5078e-01, 1.2915e-01, 1.6406e-01],\n",
      "        [2.5270e-01, 4.7667e-01, 1.0894e-01, 1.6170e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.3493e-01, 3.8855e-01, 2.5582e-02, 4.5094e-01],\n",
      "        [3.6291e-01, 1.2532e-01, 6.3376e-02, 4.4839e-01],\n",
      "        [3.7697e-01, 1.3558e-01, 1.0663e-02, 4.7679e-01],\n",
      "        [2.7803e-01, 3.3652e-01, 5.9843e-02, 3.2560e-01],\n",
      "        [8.8920e-02, 1.7551e-01, 2.1351e-02, 7.1422e-01],\n",
      "        [2.7628e-01, 4.2909e-01, 9.8371e-02, 1.9626e-01],\n",
      "        [1.1462e-01, 1.6420e-01, 5.8876e-01, 1.3242e-01],\n",
      "        [6.9496e-02, 5.9394e-01, 4.7910e-02, 2.8865e-01],\n",
      "        [4.1053e-02, 3.1276e-02, 8.5095e-03, 9.1916e-01],\n",
      "        [3.2236e-01, 8.0509e-02, 2.3023e-02, 5.7411e-01],\n",
      "        [3.7139e-02, 1.9197e-01, 5.6107e-01, 2.0982e-01],\n",
      "        [4.8098e-02, 1.5658e-02, 7.6736e-03, 9.2857e-01],\n",
      "        [3.3845e-01, 6.9600e-02, 7.1299e-03, 5.8482e-01],\n",
      "        [2.2335e-01, 3.9030e-01, 8.9676e-02, 2.9668e-01],\n",
      "        [3.4253e-04, 9.6428e-03, 9.8696e-01, 3.0547e-03],\n",
      "        [7.3291e-02, 2.6609e-01, 1.8252e-01, 4.7810e-01],\n",
      "        [1.2007e-04, 4.2824e-03, 9.9516e-01, 4.3293e-04],\n",
      "        [1.3500e-04, 5.1713e-03, 9.9442e-01, 2.6886e-04],\n",
      "        [4.3171e-01, 1.1248e-01, 1.6485e-02, 4.3933e-01],\n",
      "        [3.2522e-01, 4.2272e-01, 3.7246e-02, 2.1482e-01],\n",
      "        [3.9540e-01, 2.9503e-01, 2.7573e-02, 2.8200e-01],\n",
      "        [1.0027e-03, 4.0008e-03, 9.9349e-01, 1.5059e-03],\n",
      "        [1.8667e-01, 2.1813e-01, 1.4309e-01, 4.5212e-01],\n",
      "        [1.6560e-01, 6.0016e-01, 8.5551e-03, 2.2569e-01],\n",
      "        [9.9056e-03, 4.3596e-02, 9.3673e-01, 9.7726e-03],\n",
      "        [4.7298e-03, 1.3931e-01, 8.3403e-01, 2.1931e-02],\n",
      "        [3.4448e-02, 4.4108e-02, 4.1342e-03, 9.1731e-01],\n",
      "        [2.3546e-01, 4.3861e-01, 1.6547e-01, 1.6046e-01],\n",
      "        [2.5699e-01, 4.7802e-01, 2.1780e-02, 2.4321e-01],\n",
      "        [3.1011e-01, 3.9839e-01, 4.8058e-03, 2.8669e-01],\n",
      "        [1.5351e-02, 5.6997e-01, 7.7021e-02, 3.3765e-01],\n",
      "        [3.7560e-03, 6.5152e-04, 1.3511e-05, 9.9558e-01],\n",
      "        [1.0718e-01, 2.5483e-01, 5.2945e-01, 1.0854e-01],\n",
      "        [1.7687e-01, 4.6180e-01, 1.7155e-01, 1.8978e-01],\n",
      "        [1.5601e-01, 5.5078e-01, 1.2915e-01, 1.6406e-01],\n",
      "        [2.5270e-01, 4.7667e-01, 1.0894e-01, 1.6170e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-1.6973e+00, -6.3964e-01, -3.3602e+00, -4.9074e-01],\n",
      "        [-8.0995e-01, -1.8733e+00, -2.5550e+00, -5.9845e-01],\n",
      "        [-3.9266e-01, -1.4153e+00, -3.9581e+00, -1.5776e-01],\n",
      "        [-8.3698e-01, -6.4606e-01, -2.3730e+00, -6.7904e-01],\n",
      "        [-1.4490e+00, -7.6907e-01, -2.8756e+00,  6.3445e-01],\n",
      "        [-8.8928e-01, -4.4903e-01, -1.9219e+00, -1.2312e+00],\n",
      "        [-1.6336e+00, -1.2741e+00,  2.8072e-03, -1.4893e+00],\n",
      "        [-1.9553e+00,  1.9022e-01, -2.3272e+00, -5.3135e-01],\n",
      "        [-2.8680e+00, -3.1400e+00, -4.4417e+00,  2.4057e-01],\n",
      "        [-5.1710e-01, -1.9044e+00, -3.1563e+00,  6.0054e-02],\n",
      "        [-2.5801e+00, -9.3749e-01,  1.3505e-01, -8.4855e-01],\n",
      "        [-2.0942e+00, -3.2165e+00, -3.9297e+00,  8.6618e-01],\n",
      "        [ 3.1276e-03, -1.5785e+00, -3.8570e+00,  5.5007e-01],\n",
      "        [-1.0026e+00, -4.4438e-01, -1.9151e+00, -7.1865e-01],\n",
      "        [-6.3027e+00, -2.9651e+00,  1.6633e+00, -4.1146e+00],\n",
      "        [-2.2186e+00, -9.2922e-01, -1.3062e+00, -3.4324e-01],\n",
      "        [-6.4282e+00, -2.8540e+00,  2.5944e+00, -5.1457e+00],\n",
      "        [-6.0404e+00, -2.3948e+00,  2.8643e+00, -5.3514e+00],\n",
      "        [ 1.0501e-01, -1.2400e+00, -3.1603e+00,  1.2250e-01],\n",
      "        [-7.9055e-01, -5.2834e-01, -2.9575e+00, -1.2053e+00],\n",
      "        [-5.5855e-01, -8.5134e-01, -3.2216e+00, -8.9654e-01],\n",
      "        [-5.0512e+00, -3.6673e+00,  1.8474e+00, -4.6445e+00],\n",
      "        [-1.0077e+00, -8.5199e-01, -1.2736e+00, -1.2312e-01],\n",
      "        [-1.6100e+00, -3.2243e-01, -4.5731e+00, -1.3005e+00],\n",
      "        [-3.0929e+00, -1.6110e+00,  1.4564e+00, -3.1064e+00],\n",
      "        [-4.1067e+00, -7.2382e-01,  1.0657e+00, -2.5726e+00],\n",
      "        [-2.1995e+00, -1.9523e+00, -4.3196e+00,  1.0825e+00],\n",
      "        [-1.0189e+00, -3.9680e-01, -1.3716e+00, -1.4024e+00],\n",
      "        [-9.8839e-01, -3.6778e-01, -3.4564e+00, -1.0435e+00],\n",
      "        [-7.8995e-01, -5.3944e-01, -4.9570e+00, -8.6846e-01],\n",
      "        [-4.0231e+00, -4.0874e-01, -2.4102e+00, -9.3231e-01],\n",
      "        [-2.8745e+00, -4.6263e+00, -8.5021e+00,  2.7055e+00],\n",
      "        [-1.8591e+00, -9.9295e-01, -2.6173e-01, -1.8464e+00],\n",
      "        [-1.2668e+00, -3.0708e-01, -1.2973e+00, -1.1964e+00],\n",
      "        [-1.3202e+00, -5.8833e-02, -1.5092e+00, -1.2700e+00],\n",
      "        [-7.3104e-01, -9.6417e-02, -1.5725e+00, -1.1775e+00]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "*** TypeError: softmax() received an invalid combination of arguments - got (Tensor), but expected one of:\n",
      " * (Tensor input, int dim, torch.dtype dtype, *, Tensor out)\n",
      " * (Tensor input, name dim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "# class MaxLogitsLoss(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MaxLogitsLoss, self).__init__()\n",
    "\n",
    "#     def forward(self, logits, targets):\n",
    "#         max_logits = torch.sum(logits * targets, dim=1)\n",
    "#         loss = -max_logits.mean()\n",
    "#         return loss\n",
    "latent_dims = 64\n",
    "\n",
    "model_path = {40: \"./results/0_set_A_label_and_triplet_0.417.pth\",\n",
    "              50: \"./results/0_set_A_label_and_triplet_0.528.pth\",\n",
    "              60: \"./results/0_set_A_label_and_triplet_0.639.pth\",\n",
    "              70: \"./results/0_set_A_label_and_triplet_0.722.pth\",\n",
    "              80: \"./results/0_set_A_label_and_triplet_0.806.pth\"}\n",
    "\n",
    "chooses = [40, 50, 60, 70, 80]\n",
    "\n",
    "for choose in chooses:\n",
    "    triplet_lab_model = TripletLabelModel(latent_dims, 4)\n",
    "    triplet_lab_model.load_state_dict(torch.load(model_path[choose]))\n",
    "    triplet_lab_model = triplet_lab_model.to(device)\n",
    "    # criterion = MaxLogitsLoss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(triplet_lab_model.parameters(), lr=0.001)\n",
    "    num_epochs = 50\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for anchor_ims, contrast_ims, labels in val_data:\n",
    "            anchor_ims = torch.from_numpy(anchor_ims).to(device)\n",
    "            contrast_ims = torch.from_numpy(contrast_ims).to(device)\n",
    "            # labels = F.one_hot(labels, num_classes=4)\n",
    "            # labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            _, label_logits = triplet_lab_model(anchor_ims)\n",
    "            lab_softmax = F.softmax(label_logits, dim=1)\n",
    "            import ipdb; ipdb.set_trace()\n",
    "            loss = criterion(label_logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(label_logits.data, 1)\n",
    "            _, labels_max = torch.max(labels.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels_max).sum().item()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "        \n",
    "        # print(f\"Epoch {epoch}, Loss: {loss.item()}, Accuracy: {accuracy}%\")\n",
    "\n",
    "    print(f\"Pretrained Model: {choose}, Epoch {epoch}, Loss: {loss.item()}, Accuracy: {accuracy}%\")\n",
    "    save_path = os.path.join(save_dir, f'0_set_A_maxlogits_acc_{choose}.pth')\n",
    "    torch.save(triplet_lab_model.state_dict(), save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5ec221b8ce1ddc6eafacfbc77a75e3f09c9fea6e76ac8503f9810425480e77e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
