{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = os.path.abspath('../..')\n",
    "save_dir = os.path.join(base_dir,'results')\n",
    "data_dir = os.path.join(base_dir,'data')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 3,
>>>>>>> f8078c1f1f8abf351c8ffaf316e0405fc2ab2c67
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.manual_seed(0)\n",
    "import wandb\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "from torch.utils.data import TensorDataset,Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "# from neurora.rdm_corr import rdm_correlation_spearman"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 4,
>>>>>>> f8078c1f1f8abf351c8ffaf316e0405fc2ab2c67
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 5,
>>>>>>> f8078c1f1f8abf351c8ffaf316e0405fc2ab2c67
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLabelModel(nn.Module):\n",
    "    def __init__(self, encoded_space_dim=10, num_classes=4):\n",
    "        super().__init__()\n",
    "        \"\"\n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "    \n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        ### Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            nn.Linear(32*4*4, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, encoded_space_dim)\n",
    "        )\n",
    "        ##labeling module\n",
    "        self.decoder_labels_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, num_classes),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        # initialise all weights using xavier normal\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        batch_s = x.size(0)\n",
    "        img_features = self.encoder_cnn(x)\n",
    "        img_features = self.flatten(img_features)\n",
    "        \n",
    "        out_latent = self.encoder_lin(img_features)\n",
    "\n",
    "        label = self.decoder_labels_lin(out_latent)\n",
    "        return out_latent, label"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 6,
>>>>>>> f8078c1f1f8abf351c8ffaf316e0405fc2ab2c67
   "metadata": {},
   "outputs": [],
   "source": [
    "### custom loss computing triplet loss and labeling loss\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, margin=1):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        # self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, anchor, positive, negative, label, pred_label):\n",
    "        cosine_sim = torch.nn.CosineSimilarity(1)\n",
    "        # distance_positive = torch.tensor(1)-cosine_sim(anchor,positive)\n",
    "   \n",
    "        # distance_negative = torch.tensor(1)-cosine_sim(anchor,negative)\n",
    "\n",
    "        # triplet_loss = torch.maximum(distance_positive - distance_negative + self.margin, torch.tensor(0))\n",
    "        # triplet_loss = torch.sum(triplet_loss)\n",
    "\n",
    "\n",
    "        triplet_loss = (nn.TripletMarginWithDistanceLoss( distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)))\n",
    "        triplet_loss = triplet_loss(anchor, positive, negative)\n",
    "        label_loss = F.binary_cross_entropy_with_logits(pred_label.float(), label.float())\n",
    "        total_loss = triplet_loss + label_loss\n",
    "        return triplet_loss, label_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 50,
>>>>>>> f8078c1f1f8abf351c8ffaf316e0405fc2ab2c67
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrainModels(nn.Module):\n",
    "    def __init__(self, latent_dims, num_classes):\n",
    "        super(TrainModels, self).__init__()\n",
    "        self.triplet_lab_model = TripletLabelModel(latent_dims, num_classes)\n",
    "        self.custom_loss = CustomLoss()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, anchor_im, positive_im, negative_im):\n",
    "        anchor_latent, anchor_label = self.triplet_lab_model(anchor_im)\n",
    "        positive_latent, _ = self.triplet_lab_model(positive_im)\n",
    "        negative_latent, _ = self.triplet_lab_model(negative_im)\n",
    "\n",
    "        return anchor_latent, positive_latent, negative_latent, anchor_label\n",
    "\n",
    "    def test_epoch(self, test_data):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "        self.eval()\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            # Define the lists to store the outputs for each batch\n",
    "            test_triplet_loss = []\n",
    "            test_label_loss = []\n",
    "            test_total_loss = []\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for anchor_ims, contrast_ims, labels in test_data:\n",
    "                # Move tensor to the proper device\n",
    "                anchor_ims = anchor_ims.to(device)\n",
    "                contrast_ims = contrast_ims.to(device)\n",
    "                labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "                labels = labels.to(device)\n",
    "                anchor_latent, positive_latent, negative_latent, pred_label = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "                # Append the network output and the original image to the lists\n",
    "                triplet_loss, label_loss, total_loss = self.custom_loss(anchor_latent,\n",
    "                                                                positive_latent, \n",
    "                                                                negative_latent, \n",
    "                                                                labels,\n",
    "                                                                pred_label)\n",
    "                total += labels.size(0)\n",
    "                correct += (torch.argmax(pred_label, dim = 1) == torch.argmax(labels, dim = 1)).sum().item()\n",
    "                test_triplet_loss.append(triplet_loss.item())\n",
    "                test_label_loss.append(label_loss.item())\n",
    "                test_total_loss.append(total_loss.item())\n",
    "        test_triplet_loss = sum(test_triplet_loss)/len(test_triplet_loss)\n",
    "        test_label_loss = sum(test_label_loss)/len(test_label_loss)\n",
    "        test_total_loss = sum(test_total_loss)/len(test_total_loss)\n",
    "        test_accuracy = correct/total\n",
    "        return test_triplet_loss, test_label_loss, test_total_loss, test_accuracy\n",
    "\n",
    "    def test_epoch_calculate_representation_separation(self, test_data):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "        self.eval()\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            accuracies = []\n",
    "            for anchor_ims, contrast_ims, labels in test_data:\n",
    "                # Move tensor to the proper device\n",
    "                anchor_ims = anchor_ims.to(device)\n",
    "                contrast_ims = contrast_ims.to(device)\n",
    "                # labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "                # labels = labels.to(device)\n",
    "                anchor_latent, _, _, _ = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "                # use sklearn to predict labels from anchor_latent\n",
    "                # calculate accuracy\n",
    "                # x's are anchor_latent and y's are labels\n",
    "                # append accuracy to list\n",
    "                # put anchor_latent and labels on cpu and convert to numpy\n",
    "                anchor_latent = anchor_latent.cpu().numpy()\n",
    "                lm = linear_model.LogisticRegression(multi_class='ovr', solver='liblinear')\n",
    "                lm.fit(anchor_latent, labels)\n",
    "                # convert labels to sklearn format\n",
    "                accuracies.append(lm.score(anchor_latent, labels))\n",
    "        accuracy = sum(accuracies)/len(accuracies)\n",
    "        return accuracy\n",
    "\n",
    "    def train_epoch(self, train_data, optimizer, train_mode):\n",
    "        self.train()\n",
    "        train_triplet_loss = []\n",
    "        train_label_loss = []\n",
    "        train_total_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for anchor_ims, contrast_ims, labels in train_data:\n",
    "            \n",
    "            anchor_ims = anchor_ims.to(device)\n",
    "            contrast_ims = contrast_ims.to(device)\n",
    "            labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            anchor_latent, positive_latent, negative_latent, pred_label = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "           \n",
    "           \n",
    "           \n",
    "            triplet_loss, label_loss, total_loss = self.custom_loss(anchor_latent,\n",
    "                                                                positive_latent, \n",
    "                                                                negative_latent, \n",
    "                                                                labels,\n",
    "                                                                pred_label)\n",
    "            \n",
    "            \n",
    "            if train_mode==0:\n",
    "                triplet_loss.backward()\n",
    "            elif train_mode==1:\n",
    "                total_loss.backward()\n",
    "            elif train_mode==2:\n",
    "                label_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_triplet_loss.append(triplet_loss.item())\n",
    "            train_label_loss.append(label_loss.item())\n",
    "            train_total_loss.append(total_loss.item())\n",
    "            ##### check if the predicted labels in pred_label are the same as the labels\n",
    "            \n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (torch.argmax(pred_label, dim = 1) == torch.argmax(labels, dim = 1)).sum().item()\n",
    "        train_triplet_loss = sum(train_triplet_loss)/len(train_triplet_loss)\n",
    "        train_label_loss = sum(train_label_loss)/len(train_label_loss)\n",
    "        train_total_loss = sum(train_total_loss)/len(train_total_loss)\n",
    "        train_accuracy = correct/total\n",
    "        return train_triplet_loss, train_label_loss, train_total_loss, train_accuracy\n",
    "\n",
    "    def training_loop(self, train_data, test_data,train_mode,\n",
    "                      epochs, optimizer):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_triplet_losses = []\n",
    "        val_triplet_losses = []\n",
    "        train_label_losses = []\n",
    "        val_label_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        latent_separation_accuracy = 0\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "          train_triplet_loss, train_label_loss, train_total_loss, train_accuracy =self.train_epoch(train_data, optimizer, \n",
    "                                             train_mode)\n",
    "          test_triplet_loss, test_label_loss, test_total_loss, test_accuracy = self.test_epoch(test_data)\n",
    "          separation_accuracy = self.test_epoch_calculate_representation_separation(test_data)\n",
    "          train_losses.append(train_total_loss)\n",
    "          val_losses.append(test_total_loss)\n",
    "          train_triplet_losses.append(train_triplet_loss)\n",
    "          val_triplet_losses.append(test_triplet_loss)\n",
    "          train_label_losses.append(train_label_loss)\n",
    "          val_label_losses.append(test_label_loss)\n",
    "          train_accuracies.append(train_accuracy)\n",
    "          val_accuracies.append(test_accuracy)\n",
    "          wandb.log({\"train triplet loss\": train_triplet_loss, \n",
    "            \"train label loss\":train_label_loss, \n",
    "            \"validation triplet loss\":test_triplet_loss, \n",
    "            \"validation label loss\":test_label_loss, \n",
    "            \"total train loss\":train_total_loss, \n",
    "            \"total validation loss\":test_total_loss, \n",
    "            \"train label accuracy\":train_accuracy, \n",
    "            \"validation label accuracy\":test_accuracy,\n",
    "            'latent separation accuracy':separation_accuracy})\n",
    "        return train_triplet_losses, train_label_losses, val_triplet_losses, val_label_losses ,train_losses, val_losses, train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 45,
>>>>>>> f8078c1f1f8abf351c8ffaf316e0405fc2ab2c67
   "metadata": {},
   "outputs": [],
   "source": [
    "set_A_ims = np.load(os.path.join(data_dir, 'set_A.npy'))\n",
    "set_B_ims = np.load(os.path.join(data_dir, 'set_B.npy'))\n",
    "set_C_ims= np.load(os.path.join(data_dir, 'set_C.npy'))\n",
    "set_A_labs = np.load(os.path.join(data_dir, 'set_A_labs.npy'))\n",
    "set_B_labs = np.load(os.path.join(data_dir, 'set_B_labs.npy'))\n",
    "set_C_labs = np.load(os.path.join(data_dir, 'set_C_labs.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
=======
   "execution_count": 46,
>>>>>>> f8078c1f1f8abf351c8ffaf316e0405fc2ab2c67
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###initialize weights and bias tracking\n",
    "def wandb_init(epochs, lr, train_mode, batch_size, model_number,data_set):\n",
    "  wandb.init(project=\"ConceptualAlignmentLanguage\", entity=\"psych-711\",settings=wandb.Settings(start_method=\"thread\"))\n",
    "  wandb.config = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size, \n",
    "    # \"label_ratio\":label_ratio, \n",
    "    \"model_number\": model_number,\n",
    "    \"dataset\": data_set,\n",
    "    \"train_mode\":train_mode,\n",
    "  }\n",
    "  train_mode_dict = {0:'no label', 1:'label + triplet', 2:'only label' }\n",
    "  wandb.run.name = f'{data_set}_{train_mode_dict[train_mode]}_{model_number}'\n",
    "  wandb.run.save()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 47,
>>>>>>> f8078c1f1f8abf351c8ffaf316e0405fc2ab2c67
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_code(save_dir, num_models, epochs, num_classes, batch_size,\n",
    "             lr, latent_dims):\n",
    "  if os.path.isdir(save_dir):\n",
    "    pass\n",
    "  else:\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "\n",
    "  test_intervals = [(540, 600), (1140, 1200), (1740, 1800), (2340, 2400)]\n",
    "\n",
    "  # initialize an empty list to hold the indices\n",
    "  val_indices = []\n",
    "\n",
    "  # loop through the intervals and append the indices to the list\n",
    "  for start, stop in test_intervals:\n",
    "      val_indices.extend(list(range(start, stop)))\n",
    "\n",
    "  train_indices = (np.setdiff1d(np.arange(2400),np.array(val_indices)))\n",
    "\n",
    "  np.random.seed(56)\n",
    "  contrast_indices  = np.concatenate((np.random.choice(np.arange(start=600, stop=2400), 600, replace=False),\n",
    "                np.random.choice(np.concatenate((np.arange(start=0, stop=600), np.arange(start=1200, stop=2400))), 600, replace=False),\n",
    "                np.random.choice(np.concatenate((np.arange(start=0, stop=1200), np.arange(start=1800, stop=2400))), 600, replace=False),\n",
    "                np.random.choice(np.arange(start=1800, stop=2400), 600, replace=False)))\n",
    "\n",
    "  for data_set in ['set_A','set_B','set_C']:\n",
    "    for train_mode in tqdm(range(3)):\n",
    "      torch.manual_seed(56)\n",
    "      for model in range(num_models):\n",
    "        wandb_init(epochs, lr, train_mode, batch_size, model,data_set)\n",
    "\n",
    "        if data_set=='set_A':\n",
    "          train_data = TensorDataset(torch.tensor(set_A_ims.transpose(0,3,1,2)/255).float(), torch.tensor(set_A_ims[contrast_indices].transpose(0,3,1,2)/255).float(),\\\n",
    "                                     torch.tensor(set_A_labs).to(torch.int64))\n",
    "        elif data_set=='set_B':\n",
    "          train_data = TensorDataset(torch.tensor(set_B_ims.transpose(0,3,1,2)/255).float(), torch.tensor(set_B_ims[contrast_indices].transpose(0,3,1,2)/255).float(),\\\n",
    "                                     torch.tensor(set_B_labs).to(torch.int64))\n",
    "        elif data_set=='set_C':\n",
    "          train_data = TensorDataset(torch.tensor(set_C_ims.transpose(0,3,1,2)/255).float(), torch.tensor(set_C_ims[contrast_indices].transpose(0,3,1,2)/255).float(),\\\n",
    "                                     torch.tensor(set_C_labs).to(torch.int64))\n",
    "          \n",
    "        val_data = torch.utils.data.Subset(train_data, val_indices)\n",
    "        train_data = torch.utils.data.Subset(train_data, train_indices)\n",
    "       \n",
    "\n",
    "        train_data = torch.utils.data.DataLoader(train_data, \n",
    "                                                batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "        val_data = torch.utils.data.DataLoader(val_data, \n",
    "                                                batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "        \n",
    "     \n",
    "\n",
    "        train_obj = TrainModels(latent_dims, num_classes).to(device) # GPU\n",
    "        optimizer = torch.optim.Adam(train_obj.parameters(), lr=lr, weight_decay=1e-05)\n",
    "        train_triplet_losses, train_label_losses, \\\n",
    "          val_triplet_losses, val_label_losses, \\\n",
    "            train_losses, val_losses, train_accuracies, val_accuracies= train_obj.training_loop(train_data = train_data,\n",
    "                                                            test_data = val_data,\n",
    "                                                            epochs = epochs,\n",
    "                                                            optimizer = optimizer, \n",
    "                                                            train_mode = train_mode)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print('validation triplet loss:',val_triplet_losses,'validation total loss:',val_losses,'validation accuracy:',val_accuracies)\n",
    "        # wandb.log({\"train_img_loss\": train_img_loss, \n",
    "        #           \"train_label_loss\":train_label_loss, \n",
    "        #           \"val_img_loss\":val_img_loss, \n",
    "        #           \"val_label_loss\":val_label_loss, \n",
    "        #           \"train_losses\":train_losses, \n",
    "        #           \"val_losses\":val_losses, \n",
    "        #           \"train_accuracy\":train_accuracy, \n",
    "        #           \"val_accuracy\":val_accuracy})\n",
    "        train_mode_dict = {0:'no label', 1:'label + triplet', 2:'only label' }\n",
    "        torch.save(train_obj.state_dict(), os.path.join(save_dir,f'{data_set}_{train_mode_dict[train_mode]}_{model}'))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 51,
>>>>>>> f8078c1f1f8abf351c8ffaf316e0405fc2ab2c67
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkushinm\u001b[0m (\u001b[33mpsych-711\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:r37tvvdi) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>█▃▃▃▂▄▂▃▃▄▂▄▃▁▃▃▁▂▂▃▃▂▂▂▂▂▂▂▃▁▂▂▂▃▂▃▂▂▃▃</td></tr><tr><td>total train loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total validation loss</td><td>▁███████████████████████████████████████</td></tr><tr><td>train label accuracy</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train label loss</td><td>▁▃██████████████████████████████████████</td></tr><tr><td>train triplet loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▁███████████████████████████████████████</td></tr><tr><td>validation label loss</td><td>█▁▃▃▄▃▂▄▃▄▄▃▄▃▄▅▃▄▂▅▂▄▄▃▄▄▃▃▃▄▄▄▃▂▄▄▅▄▄▃</td></tr><tr><td>validation triplet loss</td><td>▁███████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>0.32161</td></tr><tr><td>total train loss</td><td>571.08397</td></tr><tr><td>total validation loss</td><td>539.38829</td></tr><tr><td>train label accuracy</td><td>0.25509</td></tr><tr><td>train label loss</td><td>1.43684</td></tr><tr><td>train triplet loss</td><td>569.64713</td></tr><tr><td>validation label accuracy</td><td>0.25</td></tr><tr><td>validation label loss</td><td>1.38771</td></tr><tr><td>validation triplet loss</td><td>538.00058</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lilac-resonance-456</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/r37tvvdi' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/r37tvvdi</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230418_102527-r37tvvdi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:r37tvvdi). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
<<<<<<< HEAD
       "Run data is saved locally in <code>/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20230417_143604-l8p3kc33</code>"
=======
       "Run data is saved locally in <code>/home/siddsuresh97/Projects/ConceptualAlignmentLanguage/code/python/wandb/run-20230418_102801-b4gquzgp</code>"
>>>>>>> f8078c1f1f8abf351c8ffaf316e0405fc2ab2c67
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
<<<<<<< HEAD
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/l8p3kc33' target=\"_blank\">ancient-butterfly-422</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
=======
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/b4gquzgp' target=\"_blank\">likely-cloud-457</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
>>>>>>> f8078c1f1f8abf351c8ffaf316e0405fc2ab2c67
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
<<<<<<< HEAD
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/l8p3kc33' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/l8p3kc33</a>"
=======
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/b4gquzgp' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/b4gquzgp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:28<00:00,  1.13it/s]\n",
      " 50%|█████     | 1/2 [01:43<01:43, 103.04s/it]/home/siddsuresh97/packages/anaconda3/envs/vision/lib/python3.9/site-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (r37tvvdi) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [537.8956298828125, 540.7371520996094, 550.0711822509766, 550.5347595214844, 550.5024948120117, 542.5106887817383, 537.9639511108398, 538.0134582519531, 538.0176696777344, 538.0164566040039, 538.0181121826172, 538.0259017944336, 538.0434341430664, 538.0134429931641, 538.0133361816406, 538.0137939453125, 538.0138549804688, 538.014892578125, 538.0154876708984, 538.0152435302734, 538.0167083740234, 538.0187759399414, 538.023811340332, 538.0251083374023, 538.0262069702148, 538.0315246582031, 538.0399475097656, 538.0457534790039, 538.0734786987305, 538.1262359619141, 538.1044235229492, 538.1719055175781, 538.2761993408203, 538.0081939697266, 538.0043334960938, 538.0055694580078, 538.0090179443359, 538.0189437866211, 538.0442504882812, 538.1355972290039, 538.1344146728516, 538.1132507324219, 538.2172393798828, 538.4537811279297, 548.6541213989258, 530.4475173950195, 538.0268173217773, 550.5130157470703, 550.5003356933594, 550.5000915527344, 550.5000686645508, 550.5000610351562, 550.5000305175781, 550.5000610351562, 550.5000457763672, 550.5000305175781, 550.5000381469727, 550.5000152587891, 550.5, 550.5000228881836, 550.5000228881836, 550.5000152587891, 550.5000305175781, 550.5000152587891, 550.5000305175781, 550.5000305175781, 550.5000305175781, 550.5000152587891, 550.5000152587891, 550.5, 550.5000152587891, 550.5000152587891, 550.5, 550.5000076293945, 550.5, 550.5, 550.5000076293945, 550.5, 550.5000152587891, 550.5000152587891, 550.5000076293945, 550.5, 550.5000152587891, 550.5, 550.5, 550.5, 550.5, 550.5, 550.5000076293945, 550.5000152587891, 550.5000152587891, 550.5, 550.5, 550.5000076293945, 550.5, 550.5, 550.5000076293945, 550.5, 550.5, 550.5] validation total loss: [539.2819213867188, 542.1234436035156, 551.4574737548828, 551.9210510253906, 551.888786315918, 543.9070816040039, 539.3566665649414, 539.414665222168, 539.4213485717773, 539.4168548583984, 539.4200439453125, 539.4280853271484, 539.4424209594727, 539.4094161987305, 539.4093780517578, 539.4098663330078, 539.4101791381836, 539.412483215332, 539.4112319946289, 539.4133071899414, 539.4135208129883, 539.4183654785156, 539.4227676391602, 539.4232711791992, 539.4235000610352, 539.4300155639648, 539.4374465942383, 539.4457168579102, 539.4720001220703, 539.5267868041992, 539.5049133300781, 539.5732803344727, 539.6820220947266, 539.4041213989258, 539.4039993286133, 539.4067916870117, 539.406852722168, 539.419319152832, 539.4468460083008, 539.537109375, 539.5352172851562, 539.510986328125, 539.6171035766602, 539.8547897338867, 550.0634155273438, 531.852424621582, 539.4883880615234, 552.0077285766602, 551.9902725219727, 551.9915161132812, 551.989128112793, 551.9952545166016, 551.9891738891602, 551.9914474487305, 551.9864044189453, 551.9927139282227, 551.9961547851562, 551.9947280883789, 551.9923553466797, 551.9963073730469, 551.9895782470703, 551.9887924194336, 551.9923934936523, 551.9952239990234, 551.9983291625977, 551.9859085083008, 551.9935836791992, 551.9903793334961, 551.9885330200195, 551.9885559082031, 551.9900970458984, 551.9933624267578, 551.9921035766602, 551.9960098266602, 551.9890899658203, 551.989387512207, 551.9882431030273, 551.991455078125, 551.9855041503906, 551.9913558959961, 551.9945220947266, 551.9906387329102, 551.9900665283203, 551.9923629760742, 551.9904708862305, 551.9926986694336, 551.9909744262695, 551.9905395507812, 551.9883880615234, 551.9853744506836, 551.9944534301758, 551.9880676269531, 551.9946212768555, 551.9856033325195, 551.9934616088867, 551.9869079589844, 551.9819412231445, 551.9948654174805, 551.9954223632812, 551.9905090332031] validation accuracy: [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.23333333333333334, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:b4gquzgp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▄▄▄▃▂▄▃▄▂▃▄▃▂▅▄▃▂▄█▂▂▁▂▃▃▂▃▁▂▃▂▂▂▂▃▁▂▂▂▃</td></tr><tr><td>total train loss</td><td>█▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>total validation loss</td><td>▄█▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁█████████████████████</td></tr><tr><td>train label accuracy</td><td>█▅▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>train label loss</td><td>▁▂▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████████████</td></tr><tr><td>train triplet loss</td><td>█▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>validation label accuracy</td><td>██████████████████▁█████████████████████</td></tr><tr><td>validation label loss</td><td>▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂████████████▇████▇▇▇█</td></tr><tr><td>validation triplet loss</td><td>▄█▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁█████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>0.32161</td></tr><tr><td>total train loss</td><td>572.36885</td></tr><tr><td>total validation loss</td><td>551.99051</td></tr><tr><td>train label accuracy</td><td>0.25509</td></tr><tr><td>train label loss</td><td>1.4865</td></tr><tr><td>train triplet loss</td><td>570.88235</td></tr><tr><td>validation label accuracy</td><td>0.25</td></tr><tr><td>validation label loss</td><td>1.49051</td></tr><tr><td>validation triplet loss</td><td>550.5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">likely-cloud-457</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/b4gquzgp' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/b4gquzgp</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230418_102801-b4gquzgp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:b4gquzgp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/siddsuresh97/Projects/ConceptualAlignmentLanguage/code/python/wandb/run-20230418_102944-y5mp4uki</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/y5mp4uki' target=\"_blank\">twilight-star-458</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/y5mp4uki' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignmentLanguage/runs/y5mp4uki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:31<00:00,  1.09it/s]\n",
      "100%|██████████| 2/2 [03:31<00:00, 106.27s/it]/home/siddsuresh97/packages/anaconda3/envs/vision/lib/python3.9/site-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (r37tvvdi) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "100%|██████████| 2/2 [03:31<00:00, 105.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [535.0172119140625, 535.0050811767578, 535.0071411132812, 535.0059356689453, 535.0047454833984, 535.0042266845703, 535.0039825439453, 535.0066833496094, 535.0094604492188, 535.0078353881836, 535.0054931640625, 535.0084228515625, 535.0074844360352, 540.915397644043, 541.0280838012695, 541.0005722045898, 540.9992065429688, 540.9991302490234, 540.9992218017578, 540.9992218017578, 540.9992599487305, 540.9993133544922, 540.9993667602539, 540.9994735717773, 540.9994049072266, 540.9995269775391, 540.99951171875, 540.9994888305664, 540.99951171875, 540.9995574951172, 540.9995727539062, 540.999626159668, 540.9996795654297, 540.9996566772461, 540.9996643066406, 540.9997406005859, 540.9997329711914, 540.9998016357422, 540.9998931884766, 540.9999389648438, 540.9999465942383, 540.9999237060547, 540.9999694824219, 540.9999847412109, 540.9999694824219, 540.9999923706055, 540.9999694824219, 540.9999847412109, 540.9999923706055, 540.9999923706055, 540.9999771118164, 541.0, 541.0000076293945, 540.9999694824219, 541.0000152587891, 541.0000152587891, 540.9999847412109, 541.0, 541.0, 540.9999847412109, 541.0000152587891, 541.0, 541.0, 541.0, 541.0000152587891, 541.0, 540.9999847412109, 541.0000839233398, 541.0, 599.5920715332031, 599.9931182861328, 548.24658203125, 546.0099639892578, 546.0014495849609, 546.0004043579102, 535.0194702148438, 535.0032348632812, 535.002082824707, 535.0020599365234, 535.0020523071289, 535.0018615722656, 535.0020599365234, 535.0018997192383, 535.0018692016602, 535.001838684082, 535.0018157958984, 535.0019149780273, 535.0017776489258, 535.0018920898438, 535.001953125, 535.0018615722656, 535.0017318725586, 535.001708984375, 535.0017700195312, 535.0017395019531, 535.0016174316406, 535.0015487670898, 535.0015640258789, 535.0016937255859, 535.0014419555664] validation total loss: [536.4044570922852, 536.3913726806641, 536.3940582275391, 536.3923187255859, 536.3913803100586, 536.3908309936523, 536.3907623291016, 536.3932037353516, 536.3967132568359, 536.3946533203125, 536.3922424316406, 536.3958435058594, 536.3938827514648, 542.3018188476562, 542.4145584106445, 542.3872985839844, 542.3858795166016, 542.3853607177734, 542.3856964111328, 542.3854675292969, 542.3856582641602, 542.3857040405273, 542.3856201171875, 542.385986328125, 542.3857421875, 542.3858947753906, 542.3858871459961, 542.3855056762695, 542.3859329223633, 542.3859176635742, 542.3858947753906, 542.3859405517578, 542.3858413696289, 542.3861236572266, 542.3859405517578, 542.386116027832, 542.3859100341797, 542.3860473632812, 542.3863220214844, 542.3862380981445, 542.3862915039062, 542.3861541748047, 542.3863220214844, 542.386360168457, 542.3863296508789, 542.3862762451172, 542.3862380981445, 542.3863143920898, 542.3862686157227, 542.3863220214844, 542.3862533569336, 542.386360168457, 542.3861465454102, 542.3862609863281, 542.3862991333008, 542.3863372802734, 542.3861923217773, 542.3863143920898, 542.3862228393555, 542.3862533569336, 542.3863220214844, 542.3862915039062, 542.3862380981445, 542.3863830566406, 542.3863296508789, 542.3863830566406, 542.3864059448242, 542.386360168457, 542.3862991333008, 600.9781951904297, 601.3794097900391, 549.6853332519531, 547.3962554931641, 547.3877410888672, 547.3866958618164, 536.40576171875, 536.3895263671875, 536.3883743286133, 536.3883514404297, 536.3883438110352, 536.3881530761719, 536.3883514404297, 536.3881912231445, 536.3881607055664, 536.3881301879883, 536.3881072998047, 536.3882064819336, 536.388069152832, 536.38818359375, 536.3882446289062, 536.3881530761719, 536.3880233764648, 536.3880004882812, 536.3880615234375, 536.3880310058594, 536.3879089355469, 536.3878402709961, 536.3878555297852, 536.3879852294922, 536.3877334594727] validation accuracy: [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.24583333333333332, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25416666666666665, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:y5mp4uki) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
>>>>>>> f8078c1f1f8abf351c8ffaf316e0405fc2ab2c67
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
      "  0%|          | 0/3 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m save_dir \u001b[39m=\u001b[39m save_dir\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m main_code(save_dir, num_models, epochs, num_classes, batch_size,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m              lr, latent_dims)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n",
      "\u001b[1;32m/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb Cell 15\u001b[0m in \u001b[0;36mmain_code\u001b[0;34m(save_dir, num_models, epochs, num_classes, batch_size, lr, latent_dims)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m train_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(train_data, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m                                         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m                                       shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m val_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(val_data, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m                                         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m                                       shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m train_obj \u001b[39m=\u001b[39m TrainModels(latent_dims, num_classes)\u001b[39m.\u001b[39;49mto(device) \u001b[39m# GPU\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(train_obj\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr, weight_decay\u001b[39m=\u001b[39m\u001b[39m1e-05\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m train_triplet_losses, train_label_losses, \\\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m   val_triplet_losses, val_label_losses, \\\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     train_losses, val_losses, train_accuracies, val_accuracies\u001b[39m=\u001b[39m train_obj\u001b[39m.\u001b[39mtraining_loop(train_data \u001b[39m=\u001b[39m train_data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m                                                     optimizer \u001b[39m=\u001b[39m optimizer, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnowak-gpu-2.morgridge.net/home/kushinm/repos/ConceptualAlignmentLanguage/code/python/triplet_model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m                                                     train_mode \u001b[39m=\u001b[39m train_mode)\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "num_classes = 4 # Number of unique class labels in the dataset\n",
    "latent_dims = 32\n",
    "epochs = 50\n",
    "lr = 0.0001\n",
    "num_models = 1\n",
    "batch_size = 128\n",
    "save_dir = save_dir\n",
    "main_code(save_dir, num_models, epochs, num_classes, batch_size,\n",
    "             lr, latent_dims)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5ec221b8ce1ddc6eafacfbc77a75e3f09c9fea6e76ac8503f9810425480e77e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
