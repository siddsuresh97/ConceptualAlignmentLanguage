{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = os.path.abspath('/mnt/ws/home/xyu/ConceptualAlignmentLanguage/tripletNCE')\n",
    "save_dir = os.path.join(base_dir,'results')\n",
    "data_dir = os.path.join(base_dir,'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.manual_seed(0)\n",
    "import wandb\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "from torch.utils.data import TensorDataset,Dataset, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# from neurora.rdm_corr import rdm_correlation_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLabelModel(nn.Module):\n",
    "    def __init__(self, encoded_space_dim=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        \"\"\n",
    "        ### Convolutional section\n",
    "       ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "    \n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        ### Linear section\n",
    "        ## changed 32*4*4 to 32*2*2\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            nn.Linear(32*2*2, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, encoded_space_dim)\n",
    "        )\n",
    "\n",
    "        ## triplet projection module\n",
    "        self.decoder_triplet_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 32),\n",
    "            nn.ReLU(True)\n",
    "         \n",
    "        )\n",
    "        ##labeling module\n",
    "        self.decoder_labels_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16, num_classes),\n",
    "        )\n",
    "\n",
    "        ### initialize weights using xavier initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        batch_s = x.size(0)\n",
    "        img_features = self.encoder_cnn(x)\n",
    "        img_features = self.flatten(img_features)\n",
    "        \n",
    "        enc_latent = self.encoder_lin(img_features)\n",
    "\n",
    "        triplet_latent = self.decoder_triplet_lin(enc_latent)\n",
    "        label = self.decoder_labels_lin(enc_latent)\n",
    "        # label = F.softmax(label,dim=1)\n",
    "        return enc_latent, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### custom loss computing triplet loss and labeling loss\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, margin=10):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, anchor, positive, negative, label, pred_label):\n",
    "        cosine_sim = torch.nn.CosineSimilarity(1)\n",
    "        # distance_positive = torch.tensor(1)-cosine_sim(anchor,positive)\n",
    "   \n",
    "        # distance_negative = torch.tensor(1)-cosine_sim(anchor,negative)\n",
    "\n",
    "        # triplet_loss = torch.maximum(distance_positive - distance_negative + self.margin, torch.tensor(0))\n",
    "        # triplet_loss = torch.sum(triplet_loss)\n",
    "        triplet_loss = (nn.TripletMarginWithDistanceLoss( distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)))\n",
    "        triplet_loss = triplet_loss(anchor, positive, negative)\n",
    "        label_loss = F.binary_cross_entropy_with_logits(pred_label.float(), label.float())\n",
    "        total_loss = triplet_loss + label_loss\n",
    "        return triplet_loss, label_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = TripletLabelModel()\n",
    "# cifar_model_path = '../../data/CIFAR10_NCE_i_1e-05_50.pth'\n",
    "# t.load_state_dict(torch.load(cifar_model_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrainModels(nn.Module):\n",
    "    def __init__(self, latent_dims, num_classes, weights_path=None):\n",
    "        super(TrainModels, self).__init__()\n",
    "        self.triplet_lab_model = TripletLabelModel(latent_dims, 10) ### load cifar model\n",
    "        if weights_path!=None:\n",
    "            cifar_model_path = '/mnt/ws/home/xyu/ConceptualAlignmentLanguage/tripletNCE/data/CIFAR10_NCE_i_1e-05_50.pth'\n",
    "            self.triplet_lab_model.load_state_dict(torch.load(cifar_model_path))\n",
    "            self.triplet_lab_model.decoder_labels_lin[4] = nn.Linear(16, num_classes)\n",
    "        self.custom_loss = CustomLoss()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, anchor_im, positive_im, negative_im):\n",
    "        anchor_latent, anchor_label = self.triplet_lab_model(anchor_im)\n",
    "        positive_latent, _ = self.triplet_lab_model(positive_im)\n",
    "        negative_latent, _ = self.triplet_lab_model(negative_im)\n",
    "\n",
    "        return anchor_latent, positive_latent, negative_latent, anchor_label\n",
    "\n",
    "    def test_epoch(self, test_data):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "        self.eval()\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            # Define the lists to store the outputs for each batch\n",
    "            test_triplet_loss = []\n",
    "            test_label_loss = []\n",
    "            test_total_loss = []\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for anchor_ims, contrast_ims, labels in test_data:\n",
    "                # Move tensor to the proper device\n",
    "                anchor_ims = anchor_ims.to(device)\n",
    "                contrast_ims = contrast_ims.to(device)\n",
    "                labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "                labels = labels.to(device)\n",
    "                anchor_latent, positive_latent, negative_latent, pred_label = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "                # Append the network output and the original image to the lists\n",
    "                triplet_loss, label_loss, total_loss = self.custom_loss(anchor_latent,\n",
    "                                                                positive_latent, \n",
    "                                                                negative_latent, \n",
    "                                                                labels,\n",
    "                                                                pred_label)\n",
    "                total += labels.size(0)\n",
    "                correct += (torch.argmax(pred_label, dim = 1) == torch.argmax(labels, dim = 1)).sum().item()\n",
    "                test_triplet_loss.append(triplet_loss.item())\n",
    "                test_label_loss.append(label_loss.item())\n",
    "                test_total_loss.append(total_loss.item())\n",
    "        test_triplet_loss = sum(test_triplet_loss)/len(test_triplet_loss)\n",
    "        test_label_loss = sum(test_label_loss)/len(test_label_loss)\n",
    "        test_total_loss = sum(test_total_loss)/len(test_total_loss)\n",
    "        test_accuracy = correct/total\n",
    "        return test_triplet_loss, test_label_loss, test_total_loss, test_accuracy\n",
    "\n",
    "    def test_epoch_calculate_representation_separation(self, test_data):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "        self.eval()\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            accuracies = []\n",
    "            for anchor_ims, contrast_ims, labels in test_data:\n",
    "                # Move tensor to the proper device\n",
    "                anchor_ims = anchor_ims.to(device)\n",
    "                contrast_ims = contrast_ims.to(device)\n",
    "                # labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "                # labels = labels.to(device)\n",
    "                anchor_latent, _, _, _ = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "                # use sklearn to predict labels from anchor_latent\n",
    "                # calculate accuracy\n",
    "                # x's are anchor_latent and y's are labels\n",
    "                # append accuracy to list\n",
    "                # put anchor_latent and labels on cpu and convert to numpy\n",
    "\n",
    "          \n",
    "                anchor_latent = anchor_latent.cpu().numpy()\n",
    "                ### standard scale the data in anchor_latent before fitting to the model\n",
    "                anchor_latent = StandardScaler().fit_transform(anchor_latent)\n",
    "                labels = labels.cpu().numpy()\n",
    "                \n",
    "                lm = linear_model.LogisticRegression()\n",
    "                lm.fit(anchor_latent, labels)\n",
    "                # convert labels to sklearn format\n",
    "                accuracies.append(lm.score(anchor_latent, labels))\n",
    "        accuracy = sum(accuracies)/len(accuracies)\n",
    "        return accuracy\n",
    "\n",
    "    def train_epoch(self, train_data, optimizer, train_mode):\n",
    "        self.train()\n",
    "        train_triplet_loss = []\n",
    "        train_label_loss = []\n",
    "        train_total_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for anchor_ims, contrast_ims, labels in train_data:\n",
    "            \n",
    "            anchor_ims = anchor_ims.to(device)\n",
    "            contrast_ims = contrast_ims.to(device)\n",
    "            labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            anchor_latent, positive_latent, negative_latent, pred_label = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "           \n",
    "           \n",
    "           \n",
    "            triplet_loss, label_loss, total_loss = self.custom_loss(anchor_latent,\n",
    "                                                                positive_latent, \n",
    "                                                                negative_latent, \n",
    "                                                                labels,\n",
    "                                                                pred_label)\n",
    "            \n",
    "            \n",
    "            if train_mode==0:\n",
    "                triplet_loss.backward()\n",
    "            elif train_mode==1:\n",
    "                label_loss.backward()\n",
    "            elif train_mode==2:\n",
    "                total_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_triplet_loss.append(triplet_loss.item())\n",
    "            train_label_loss.append(label_loss.item())\n",
    "            train_total_loss.append(total_loss.item())\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.argmax(pred_label, dim = 1) == torch.argmax(labels, dim = 1)).sum().item()\n",
    "        train_triplet_loss = sum(train_triplet_loss)/len(train_triplet_loss)\n",
    "        train_label_loss = sum(train_label_loss)/len(train_label_loss)\n",
    "        train_total_loss = sum(train_total_loss)/len(train_total_loss)\n",
    "        train_accuracy = correct/total\n",
    "        return train_triplet_loss, train_label_loss, train_total_loss, train_accuracy\n",
    "\n",
    "    def training_loop(self, train_data, test_data,train_mode,\n",
    "                      epochs, optimizer):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_triplet_losses = []\n",
    "        val_triplet_losses = []\n",
    "        train_label_losses = []\n",
    "        val_label_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        latent_separation_accuracy = 0\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "          train_triplet_loss, train_label_loss, train_total_loss, train_accuracy =self.train_epoch(train_data, optimizer, \n",
    "                                             train_mode)\n",
    "          test_triplet_loss, test_label_loss, test_total_loss, test_accuracy = self.test_epoch(test_data)\n",
    "          separation_accuracy = self.test_epoch_calculate_representation_separation(test_data)\n",
    "          train_losses.append(train_total_loss)\n",
    "          val_losses.append(test_total_loss)\n",
    "          train_triplet_losses.append(train_triplet_loss)\n",
    "          val_triplet_losses.append(test_triplet_loss)\n",
    "          train_label_losses.append(train_label_loss)\n",
    "          val_label_losses.append(test_label_loss)\n",
    "          train_accuracies.append(train_accuracy)\n",
    "          val_accuracies.append(test_accuracy)\n",
    "          wandb.log({\"train triplet loss\": train_triplet_loss, \n",
    "            \"train label loss\":train_label_loss, \n",
    "            \"validation triplet loss\":test_triplet_loss, \n",
    "            \"validation label loss\":test_label_loss, \n",
    "            \"total train loss\":train_total_loss, \n",
    "            \"total validation loss\":test_total_loss, \n",
    "            \"train label accuracy\":train_accuracy, \n",
    "            \"validation label accuracy\":test_accuracy,\n",
    "            'latent separation accuracy':separation_accuracy})\n",
    "        return train_triplet_losses, train_label_losses, val_triplet_losses, val_label_losses ,train_losses, val_losses, train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_A_ims = np.load(os.path.join(data_dir, 'set_A.npy'))\n",
    "set_B_ims = np.load(os.path.join(data_dir, 'set_B.npy'))\n",
    "set_C_ims = np.load(os.path.join(data_dir, 'set_C.npy'))\n",
    "set_A_labs = np.load(os.path.join(data_dir, 'set_A_labs.npy'))\n",
    "set_B_labs = np.load(os.path.join(data_dir, 'set_B_labs.npy'))\n",
    "set_C_labs = np.load(os.path.join(data_dir, 'set_C_labs.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40:60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_A_sub_ims =[]\n",
    "set_B_sub_ims =[]\n",
    "\n",
    "set_C_sub_ims =[]\n",
    "\n",
    "set_A_sub_labs =[]\n",
    "set_B_sub_labs =[]\n",
    "set_C_sub_labs =[]\n",
    "\n",
    "\n",
    "for i in range (4):\n",
    "    sub_main = set_A_ims[i*600:(i*600)+600]\n",
    "    labels_main = set_A_labs[i*600:(i*600)+600]\n",
    "    np.random.seed(711)\n",
    "    np.random.shuffle(sub_main)\n",
    "    np.random.seed(711)\n",
    "    np.random.shuffle(labels_main)\n",
    "\n",
    "    set_A_sub_ims.append(sub_main[:30])\n",
    "    set_B_sub_ims.append(sub_main[:15])\n",
    "    set_B_sub_ims.append(sub_main[30:45])\n",
    "    set_C_sub_ims.append(sub_main[35:65])\n",
    "\n",
    "    set_A_sub_labs.append(labels_main[:30])\n",
    "    set_B_sub_labs.append(labels_main[:15])\n",
    "    set_B_sub_labs.append(labels_main[30:45])\n",
    "    set_C_sub_labs.append(labels_main[35:65])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##flatten set_A_sub_ims into an array of shape 120,64,64,3\n",
    "set_A_sub_ims = np.concatenate(set_A_sub_ims)\n",
    "set_B_sub_ims = np.concatenate(set_B_sub_ims)\n",
    "set_C_sub_ims = np.concatenate(set_C_sub_ims)\n",
    "\n",
    "set_A_sub_labs = np.concatenate(set_A_sub_labs)\n",
    "set_B_sub_labs = np.concatenate(set_B_sub_labs)\n",
    "set_C_sub_labs = np.concatenate(set_C_sub_labs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A-B: 50% \\\n",
    "A-C: 0% \\\n",
    "B-C: 33.33%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###initialize weights and bias tracking\n",
    "def wandb_init(epochs, lr, train_mode, batch_size, model_number,data_set):\n",
    "    wandb.init(project=\"ConceptualAlignment\", settings=wandb.Settings(start_method=\"thread\"))\n",
    "    wandb.config = {\n",
    "      \"learning_rate\": lr,\n",
    "      \"epochs\": epochs,\n",
    "      \"batch_size\": batch_size, \n",
    "      # \"label_ratio\":label_ratio, \n",
    "      \"model_number\": model_number,\n",
    "      \"dataset\": data_set,\n",
    "      \"train_mode\":train_mode,\n",
    "    }\n",
    "    train_mode_dict = {0:'triplet', 1:'label', 2:'label_and_triplet'}\n",
    "    wandb.run.name = f'{data_set}_{train_mode_dict[train_mode]}_{model_number}'\n",
    "    wandb.run.save()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_code(save_dir, num_models, epochs, num_classes, batch_size,\n",
    "             lr, latent_dims):\n",
    "  if os.path.isdir(save_dir):\n",
    "    pass\n",
    "  else:\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "  np.random.seed(42)\n",
    "  torch.manual_seed(42)\n",
    "  \n",
    "  # test_intervals = [(540, 600), (1140, 1200), (1740, 1800), (2340, 2400)]\n",
    "  test_intervals = [(25, 30), (55, 60), (85, 90), (115, 120)]\n",
    "  # initialize an empty list to hold the indices\n",
    "  val_indices = []\n",
    "\n",
    "  # loop through the intervals and append the indices to the list\n",
    "  for start, stop in test_intervals:\n",
    "      val_indices.extend(list(range(start, stop)))\n",
    "\n",
    "  # train_indices = (np.setdiff1d(np.arange(2400),np.array(val_indices)))\n",
    "  train_indices = (np.setdiff1d(np.arange(120),np.array(val_indices)))\n",
    "\n",
    "  # np.random.seed(56)\n",
    "  # contrast_indices  = np.concatenate((np.random.choice(np.arange(start=600, stop=2400), 600, replace=False),\n",
    "  #               np.random.choice(np.concatenate((np.arange(start=0, stop=600), np.arange(start=1200, stop=2400))), 600, replace=False),\n",
    "  #               np.random.choice(np.concatenate((np.arange(start=0, stop=1200), np.arange(start=1800, stop=2400))), 600, replace=False),\n",
    "  #               np.random.choice(np.arange(start=1800, stop=2400), 600, replace=False)))\n",
    "  contrast_indices  = np.concatenate((np.random.choice(np.arange(start=30, stop=120), 30, replace=False),\n",
    "                np.random.choice(np.concatenate((np.arange(start=0, stop=30), np.arange(start=60, stop=120))), 30, replace=False),\n",
    "                np.random.choice(np.concatenate((np.arange(start=0, stop=60), np.arange(start=90, stop=120))), 30, replace=False),\n",
    "                np.random.choice(np.arange(start=0, stop=90), 30, replace=False)))\n",
    "\n",
    "  # for data_set in ['set_A','set_A2','set_B','set_C']:\n",
    "  for data_set in ['set_A']:\n",
    "    for train_mode in tqdm(range(2, 3)):\n",
    "     # torch.manual_seed(0)\n",
    "      for model in range(num_models):\n",
    "        wandb_init(epochs, lr, train_mode, batch_size, model,data_set)\n",
    "        weights_path = f'../../data/cifar_models/m{model}.pth'\n",
    "\n",
    "        # if data_set=='set_A':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_A_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_A_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_A_labs).to(torch.int64))\n",
    "        # elif data_set=='set_B':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_B_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_B_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_B_labs).to(torch.int64))\n",
    "        # elif data_set=='set_C':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_C_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_C_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_C_labs).to(torch.int64))\n",
    "        # if data_set=='set_A2':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_A_sub_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_A_sub_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_A_sub_labs).to(torch.int64))\n",
    "        if data_set=='set_A':\n",
    "          train_data = TensorDataset(Resize(32)(torch.tensor(set_A_sub_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_A_sub_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "                                     torch.tensor(set_A_sub_labs).to(torch.int64))\n",
    "        # elif data_set=='set_B':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_B_sub_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_B_sub_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_B_sub_labs).to(torch.int64))\n",
    "        # elif data_set=='set_C':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_C_sub_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_C_sub_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_C_sub_labs).to(torch.int64))\n",
    "        \n",
    "        train_size = int(0.7 * len(train_data))\n",
    "        val_size = len(train_data) - train_size\n",
    "        \n",
    "        train_data, val_data = random_split(train_data, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "       \n",
    "\n",
    "        train_data = torch.utils.data.DataLoader(train_data, \n",
    "                                                batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "        val_data = torch.utils.data.DataLoader(val_data, \n",
    "                                                batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "        \n",
    "     \n",
    "\n",
    "        train_obj = TrainModels(latent_dims, num_classes, weights_path).to(device) # GPU\n",
    "        optimizer = torch.optim.Adam(train_obj.parameters(), lr=lr, weight_decay=1e-05)\n",
    "        train_triplet_losses, train_label_losses, \\\n",
    "          val_triplet_losses, val_label_losses, \\\n",
    "            train_losses, val_losses, train_accuracies, val_accuracies= train_obj.training_loop(train_data = train_data,\n",
    "                                                            test_data = val_data,\n",
    "                                                            epochs = epochs,\n",
    "                                                            optimizer = optimizer, \n",
    "                                                            train_mode = train_mode)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print('validation triplet loss:',val_triplet_losses[-1],'validation total loss:',val_losses[-1],'validation accuracy:',val_accuracies[-1])\n",
    "        # wandb.log({\"train_img_loss\": train_img_loss, \n",
    "        #           \"train_label_loss\":train_label_loss, \n",
    "        #           \"val_img_loss\":val_img_loss, \n",
    "        #           \"val_label_loss\":val_label_loss, \n",
    "        #           \"train_losses\":train_losses, \n",
    "        #           \"val_losses\":val_losses, \n",
    "        #           \"train_accuracy\":train_accuracy, \n",
    "        #           \"val_accuracy\":val_accuracy})\n",
    "        train_mode_dict = {0:'triplet', 1:'label',2:'label_and_triplet' }\n",
    "        torch.save(train_obj.triplet_lab_model.state_dict(), os.path.join(save_dir,f'{model}_{data_set}_{train_mode_dict[train_mode]}_{round(val_accuracies[-1], 3)}.pth'))\n",
    "  return val_data\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/xyu/ConceptualAlignmentLanguage/tripletNCE/wandb/run-20240123_181111-e9kdzjkg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yuxizheng/ConceptualAlignment/runs/e9kdzjkg' target=\"_blank\">laced-lake-51</a></strong> to <a href='https://wandb.ai/yuxizheng/ConceptualAlignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yuxizheng/ConceptualAlignment' target=\"_blank\">https://wandb.ai/yuxizheng/ConceptualAlignment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yuxizheng/ConceptualAlignment/runs/e9kdzjkg' target=\"_blank\">https://wandb.ai/yuxizheng/ConceptualAlignment/runs/e9kdzjkg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/ws/home/xyu/miniconda3/envs/get_responses/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "100%|██████████| 1000/1000 [00:24<00:00, 40.69it/s]\n",
      "100%|██████████| 1/1 [00:27<00:00, 27.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: 0.16304899752140045 validation total loss: 1.5399528741836548 validation accuracy: 0.6944444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>█████████████████▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total train loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total validation loss</td><td>▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▄▃▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>train label accuracy</td><td>▁████████████████▇██████████████████████</td></tr><tr><td>train label loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train triplet loss</td><td>▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▁███▇▇▇██████████▃▅█████████████████████</td></tr><tr><td>validation label loss</td><td>▁▁▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃</td></tr><tr><td>validation triplet loss</td><td>█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▇▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>0.97222</td></tr><tr><td>total train loss</td><td>1e-05</td></tr><tr><td>total validation loss</td><td>1.53995</td></tr><tr><td>train label accuracy</td><td>1.0</td></tr><tr><td>train label loss</td><td>1e-05</td></tr><tr><td>train triplet loss</td><td>0.0</td></tr><tr><td>validation label accuracy</td><td>0.69444</td></tr><tr><td>validation label loss</td><td>1.3769</td></tr><tr><td>validation triplet loss</td><td>0.16305</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-lake-51</strong> at: <a href='https://wandb.ai/yuxizheng/ConceptualAlignment/runs/e9kdzjkg' target=\"_blank\">https://wandb.ai/yuxizheng/ConceptualAlignment/runs/e9kdzjkg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240123_181111-e9kdzjkg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "\n",
    "num_classes = 4 # Number of unique class labels in the dataset\n",
    "latent_dims = 64\n",
    "epochs = 1000\n",
    "lr = 0.005\n",
    "num_models = 1\n",
    "batch_size = 256\n",
    "save_dir = save_dir\n",
    "val_data = main_code(save_dir, num_models, epochs, num_classes, batch_size,\n",
    "             lr, latent_dims)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3970279395580292\n",
      "Epoch 1, Loss: -0.3470607101917267\n",
      "Epoch 2, Loss: -0.8885486125946045\n",
      "Epoch 3, Loss: -1.2550053596496582\n",
      "Epoch 4, Loss: -1.5770169496536255\n",
      "Epoch 5, Loss: -1.8885822296142578\n",
      "Epoch 6, Loss: -2.199861764907837\n",
      "Epoch 7, Loss: -2.5175230503082275\n",
      "Epoch 8, Loss: -2.8484487533569336\n",
      "Epoch 9, Loss: -3.202425241470337\n",
      "Epoch 10, Loss: -3.579545259475708\n",
      "Epoch 11, Loss: -3.973787546157837\n",
      "Epoch 12, Loss: -4.392509937286377\n",
      "Epoch 13, Loss: -4.839850425720215\n",
      "Epoch 14, Loss: -5.323063373565674\n",
      "Epoch 15, Loss: -5.8380913734436035\n",
      "Epoch 16, Loss: -6.380001544952393\n",
      "Epoch 17, Loss: -6.950700283050537\n",
      "Epoch 18, Loss: -7.567458152770996\n",
      "Epoch 19, Loss: -8.225098609924316\n",
      "Epoch 20, Loss: -8.920822143554688\n",
      "Epoch 21, Loss: -9.664689064025879\n",
      "Epoch 22, Loss: -10.46700382232666\n",
      "Epoch 23, Loss: -11.330839157104492\n",
      "Epoch 24, Loss: -12.253902435302734\n",
      "Epoch 25, Loss: -13.228592872619629\n",
      "Epoch 26, Loss: -14.265116691589355\n",
      "Epoch 27, Loss: -15.374069213867188\n",
      "Epoch 28, Loss: -16.558380126953125\n",
      "Epoch 29, Loss: -17.81340980529785\n",
      "Epoch 30, Loss: -19.1505126953125\n",
      "Epoch 31, Loss: -20.56419563293457\n",
      "Epoch 32, Loss: -22.065589904785156\n",
      "Epoch 33, Loss: -23.66683006286621\n",
      "Epoch 34, Loss: -25.370800018310547\n",
      "Epoch 35, Loss: -27.179615020751953\n",
      "Epoch 36, Loss: -29.098846435546875\n",
      "Epoch 37, Loss: -31.13543128967285\n",
      "Epoch 38, Loss: -33.29158401489258\n",
      "Epoch 39, Loss: -35.57671356201172\n",
      "Epoch 40, Loss: -38.00414276123047\n",
      "Epoch 41, Loss: -40.572418212890625\n",
      "Epoch 42, Loss: -43.28718948364258\n",
      "Epoch 43, Loss: -46.156272888183594\n",
      "Epoch 44, Loss: -49.191654205322266\n",
      "Epoch 45, Loss: -52.39830780029297\n",
      "Epoch 46, Loss: -55.77975845336914\n",
      "Epoch 47, Loss: -59.3529052734375\n",
      "Epoch 48, Loss: -63.11805725097656\n",
      "Epoch 49, Loss: -67.09955596923828\n"
     ]
    }
   ],
   "source": [
    "class MaxLogitsLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxLogitsLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        max_logits = torch.sum(logits * targets, dim=1)\n",
    "        loss = -max_logits.mean()\n",
    "        return loss\n",
    "\n",
    "model_path = {40: \"./results/0_set_A_label_and_triplet_0.417.pth\",\n",
    "              50: \"./results/0_set_A_label_and_triplet_0.528.pth\",\n",
    "              60: \"./results/0_set_A_label_and_triplet_0.639.pth\",\n",
    "              70: \"./results/0_set_A_label_and_triplet_0.722.pth\",\n",
    "              80: \"./results/0_set_A_label_and_triplet_0.806.pth\"}\n",
    "\n",
    "choose = 40\n",
    "\n",
    "triplet_lab_model = TripletLabelModel(latent_dims, 4)\n",
    "triplet_lab_model.load_state_dict(torch.load(model_path[choose]))\n",
    "triplet_lab_model = triplet_lab_model.to(device)\n",
    "criterion = MaxLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(triplet_lab_model.parameters(), lr=0.001)\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for anchor_ims, contrast_ims, labels in val_data:\n",
    "        anchor_ims = anchor_ims.to(device)\n",
    "        contrast_ims = contrast_ims.to(device)\n",
    "        labels = F.one_hot(labels, num_classes=4)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        _, label_logits = triplet_lab_model(anchor_ims)\n",
    "        loss = criterion(label_logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "save_path = os.path.join(save_dir, f'0_set_A_maxlogits_acc_{choose}.pth')\n",
    "torch.save(triplet_lab_model.state_dict(), save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want the final layer logits , compute error based on the maximum logits ground truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5ec221b8ce1ddc6eafacfbc77a75e3f09c9fea6e76ac8503f9810425480e77e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
