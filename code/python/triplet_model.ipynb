{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = os.path.abspath('../..')\n",
    "save_dir = os.path.join(base_dir,'results')\n",
    "data_dir = os.path.join(base_dir,'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.manual_seed(0)\n",
    "import wandb\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "from torch.utils.data import TensorDataset,Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# from neurora.rdm_corr import rdm_correlation_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLabelModel(nn.Module):\n",
    "    def __init__(self, encoded_space_dim=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        \"\"\n",
    "        ### Convolutional section\n",
    "       ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "    \n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        ### Linear section\n",
    "        ## changed 32*4*4 to 32*2*2\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            nn.Linear(32*2*2, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, encoded_space_dim)\n",
    "        )\n",
    "\n",
    "        ## triplet projection module\n",
    "        self.decoder_triplet_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 32),\n",
    "            nn.ReLU(True)\n",
    "         \n",
    "        )\n",
    "        ##labeling module\n",
    "        self.decoder_labels_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16, num_classes),\n",
    "        )\n",
    "\n",
    "        ### initialize weights using xavier initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        batch_s = x.size(0)\n",
    "        img_features = self.encoder_cnn(x)\n",
    "        img_features = self.flatten(img_features)\n",
    "        \n",
    "        enc_latent = self.encoder_lin(img_features)\n",
    "\n",
    "        triplet_latent = self.decoder_triplet_lin(enc_latent)\n",
    "        label = self.decoder_labels_lin(enc_latent)\n",
    "        # label = F.softmax(label,dim=1)\n",
    "        return enc_latent, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### custom loss computing triplet loss and labeling loss\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, margin=10):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, anchor, positive, negative, label, pred_label):\n",
    "        cosine_sim = torch.nn.CosineSimilarity(1)\n",
    "        # distance_positive = torch.tensor(1)-cosine_sim(anchor,positive)\n",
    "   \n",
    "        # distance_negative = torch.tensor(1)-cosine_sim(anchor,negative)\n",
    "\n",
    "        # triplet_loss = torch.maximum(distance_positive - distance_negative + self.margin, torch.tensor(0))\n",
    "        # triplet_loss = torch.sum(triplet_loss)\n",
    "        triplet_loss = (nn.TripletMarginWithDistanceLoss( distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)))\n",
    "        triplet_loss = triplet_loss(anchor, positive, negative)\n",
    "        label_loss = F.binary_cross_entropy_with_logits(pred_label.float(), label.float())\n",
    "        total_loss = triplet_loss + label_loss\n",
    "        return triplet_loss, label_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TripletLabelModel()\n",
    "cifar_model_path = '../../data/CIFAR10_NCE_i_1e-05_50.pth'\n",
    "t.load_state_dict(torch.load(cifar_model_path))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrainModels(nn.Module):\n",
    "    def __init__(self, latent_dims, num_classes):\n",
    "        super(TrainModels, self).__init__()\n",
    "        self.triplet_lab_model = TripletLabelModel(latent_dims, 10) ### load cifar model\n",
    "        cifar_model_path = '../../data/CIFAR10_NCE_i_1e-05_50.pth'\n",
    "        self.triplet_lab_model.load_state_dict(torch.load(cifar_model_path))\n",
    "        self.triplet_lab_model.decoder_labels_lin[4] = nn.Linear(16, num_classes)\n",
    "        self.custom_loss = CustomLoss()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, anchor_im, positive_im, negative_im):\n",
    "        anchor_latent, anchor_label = self.triplet_lab_model(anchor_im)\n",
    "        positive_latent, _ = self.triplet_lab_model(positive_im)\n",
    "        negative_latent, _ = self.triplet_lab_model(negative_im)\n",
    "\n",
    "        return anchor_latent, positive_latent, negative_latent, anchor_label\n",
    "\n",
    "    def test_epoch(self, test_data):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "        self.eval()\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            # Define the lists to store the outputs for each batch\n",
    "            test_triplet_loss = []\n",
    "            test_label_loss = []\n",
    "            test_total_loss = []\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for anchor_ims, contrast_ims, labels in test_data:\n",
    "                # Move tensor to the proper device\n",
    "                anchor_ims = anchor_ims.to(device)\n",
    "                contrast_ims = contrast_ims.to(device)\n",
    "                labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "                labels = labels.to(device)\n",
    "                anchor_latent, positive_latent, negative_latent, pred_label = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "                # Append the network output and the original image to the lists\n",
    "                triplet_loss, label_loss, total_loss = self.custom_loss(anchor_latent,\n",
    "                                                                positive_latent, \n",
    "                                                                negative_latent, \n",
    "                                                                labels,\n",
    "                                                                pred_label)\n",
    "                total += labels.size(0)\n",
    "                correct += (torch.argmax(pred_label, dim = 1) == torch.argmax(labels, dim = 1)).sum().item()\n",
    "                test_triplet_loss.append(triplet_loss.item())\n",
    "                test_label_loss.append(label_loss.item())\n",
    "                test_total_loss.append(total_loss.item())\n",
    "        test_triplet_loss = sum(test_triplet_loss)/len(test_triplet_loss)\n",
    "        test_label_loss = sum(test_label_loss)/len(test_label_loss)\n",
    "        test_total_loss = sum(test_total_loss)/len(test_total_loss)\n",
    "        test_accuracy = correct/total\n",
    "        return test_triplet_loss, test_label_loss, test_total_loss, test_accuracy\n",
    "\n",
    "    def test_epoch_calculate_representation_separation(self, test_data):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "        self.eval()\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            accuracies = []\n",
    "            for anchor_ims, contrast_ims, labels in test_data:\n",
    "                # Move tensor to the proper device\n",
    "                anchor_ims = anchor_ims.to(device)\n",
    "                contrast_ims = contrast_ims.to(device)\n",
    "                # labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "                # labels = labels.to(device)\n",
    "                anchor_latent, _, _, _ = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "                # use sklearn to predict labels from anchor_latent\n",
    "                # calculate accuracy\n",
    "                # x's are anchor_latent and y's are labels\n",
    "                # append accuracy to list\n",
    "                # put anchor_latent and labels on cpu and convert to numpy\n",
    "\n",
    "          \n",
    "                anchor_latent = anchor_latent.cpu().numpy()\n",
    "                ### standard scale the data in anchor_latent before fitting to the model\n",
    "                anchor_latent = StandardScaler().fit_transform(anchor_latent)\n",
    "                labels = labels.cpu().numpy()\n",
    "                \n",
    "                lm = linear_model.LogisticRegression()\n",
    "                lm.fit(anchor_latent, labels)\n",
    "                # convert labels to sklearn format\n",
    "                accuracies.append(lm.score(anchor_latent, labels))\n",
    "        accuracy = sum(accuracies)/len(accuracies)\n",
    "        return accuracy\n",
    "\n",
    "    def train_epoch(self, train_data, optimizer, train_mode):\n",
    "        self.train()\n",
    "        train_triplet_loss = []\n",
    "        train_label_loss = []\n",
    "        train_total_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for anchor_ims, contrast_ims, labels in train_data:\n",
    "            \n",
    "            anchor_ims = anchor_ims.to(device)\n",
    "            contrast_ims = contrast_ims.to(device)\n",
    "            labels = F.one_hot(labels, num_classes=self.num_classes)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            anchor_latent, positive_latent, negative_latent, pred_label = self.forward(anchor_ims, anchor_ims,contrast_ims) \n",
    "           \n",
    "           \n",
    "           \n",
    "            triplet_loss, label_loss, total_loss = self.custom_loss(anchor_latent,\n",
    "                                                                positive_latent, \n",
    "                                                                negative_latent, \n",
    "                                                                labels,\n",
    "                                                                pred_label)\n",
    "            \n",
    "            \n",
    "            if train_mode==0:\n",
    "                triplet_loss.backward()\n",
    "            elif train_mode==1:\n",
    "                label_loss.backward()\n",
    "            elif train_mode==2:\n",
    "                total_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_triplet_loss.append(triplet_loss.item())\n",
    "            train_label_loss.append(label_loss.item())\n",
    "            train_total_loss.append(total_loss.item())\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.argmax(pred_label, dim = 1) == torch.argmax(labels, dim = 1)).sum().item()\n",
    "        train_triplet_loss = sum(train_triplet_loss)/len(train_triplet_loss)\n",
    "        train_label_loss = sum(train_label_loss)/len(train_label_loss)\n",
    "        train_total_loss = sum(train_total_loss)/len(train_total_loss)\n",
    "        train_accuracy = correct/total\n",
    "        return train_triplet_loss, train_label_loss, train_total_loss, train_accuracy\n",
    "\n",
    "    def training_loop(self, train_data, test_data,train_mode,\n",
    "                      epochs, optimizer):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_triplet_losses = []\n",
    "        val_triplet_losses = []\n",
    "        train_label_losses = []\n",
    "        val_label_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        latent_separation_accuracy = 0\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "          train_triplet_loss, train_label_loss, train_total_loss, train_accuracy =self.train_epoch(train_data, optimizer, \n",
    "                                             train_mode)\n",
    "          test_triplet_loss, test_label_loss, test_total_loss, test_accuracy = self.test_epoch(test_data)\n",
    "          separation_accuracy = self.test_epoch_calculate_representation_separation(test_data)\n",
    "          train_losses.append(train_total_loss)\n",
    "          val_losses.append(test_total_loss)\n",
    "          train_triplet_losses.append(train_triplet_loss)\n",
    "          val_triplet_losses.append(test_triplet_loss)\n",
    "          train_label_losses.append(train_label_loss)\n",
    "          val_label_losses.append(test_label_loss)\n",
    "          train_accuracies.append(train_accuracy)\n",
    "          val_accuracies.append(test_accuracy)\n",
    "          wandb.log({\"train triplet loss\": train_triplet_loss, \n",
    "            \"train label loss\":train_label_loss, \n",
    "            \"validation triplet loss\":test_triplet_loss, \n",
    "            \"validation label loss\":test_label_loss, \n",
    "            \"total train loss\":train_total_loss, \n",
    "            \"total validation loss\":test_total_loss, \n",
    "            \"train label accuracy\":train_accuracy, \n",
    "            \"validation label accuracy\":test_accuracy,\n",
    "            'latent separation accuracy':separation_accuracy})\n",
    "        return train_triplet_losses, train_label_losses, val_triplet_losses, val_label_losses ,train_losses, val_losses, train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_A_ims = np.load(os.path.join(data_dir, 'set_A.npy'))\n",
    "set_B_ims = np.load(os.path.join(data_dir, 'set_B.npy'))\n",
    "set_C_ims = np.load(os.path.join(data_dir, 'set_C.npy'))\n",
    "set_A_labs = np.load(os.path.join(data_dir, 'set_A_labs.npy'))\n",
    "set_B_labs = np.load(os.path.join(data_dir, 'set_B_labs.npy'))\n",
    "set_C_labs = np.load(os.path.join(data_dir, 'set_C_labs.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "150/600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40:60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_A_sub_ims =[]\n",
    "set_B_sub_ims =[]\n",
    "set_C_sub_ims =[]\n",
    "\n",
    "set_A_sub_labs =[]\n",
    "set_B_sub_labs =[]\n",
    "set_C_sub_labs =[]\n",
    "\n",
    "\n",
    "for i in range (4):\n",
    "    sub_main = set_A_ims[i*600:(i*600)+600]\n",
    "    labels_main = set_A_labs[i*600:(i*600)+600]\n",
    "    np.random.seed(711)\n",
    "    np.random.shuffle(sub_main)\n",
    "    np.random.seed(711)\n",
    "    np.random.shuffle(labels_main)\n",
    "\n",
    "    set_A_sub_ims.append(sub_main[:30])\n",
    "    set_B_sub_ims.append(sub_main[:15])\n",
    "    set_B_sub_ims.append(sub_main[30:45])\n",
    "    set_C_sub_ims.append(sub_main[35:65])\n",
    "\n",
    "    set_A_sub_labs.append(labels_main[:30])\n",
    "    set_B_sub_labs.append(labels_main[:15])\n",
    "    set_B_sub_labs.append(labels_main[30:45])\n",
    "    set_C_sub_labs.append(labels_main[35:65])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##flatten set_A_sub_ims into an array of shape 120,64,64,3\n",
    "set_A_sub_ims = np.concatenate(set_A_sub_ims)\n",
    "set_B_sub_ims = np.concatenate(set_B_sub_ims)\n",
    "set_C_sub_ims = np.concatenate(set_C_sub_ims)\n",
    "\n",
    "set_A_sub_labs = np.concatenate(set_A_sub_labs)\n",
    "set_B_sub_labs = np.concatenate(set_B_sub_labs)\n",
    "set_C_sub_labs = np.concatenate(set_C_sub_labs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A-B: 50% \\\n",
    "A-C: 0% \\\n",
    "B-C: 33.33%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###initialize weights and bias tracking\n",
    "def wandb_init(epochs, lr, train_mode, batch_size, model_number,data_set):\n",
    "  wandb.init(project=\"ConceptualAlignment2023\", entity=\"psych-711\",settings=wandb.Settings(start_method=\"thread\"))\n",
    "  wandb.config = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size, \n",
    "    # \"label_ratio\":label_ratio, \n",
    "    \"model_number\": model_number,\n",
    "    \"dataset\": data_set,\n",
    "    \"train_mode\":train_mode,\n",
    "  }\n",
    "  train_mode_dict = {0:'triplet', 1:'label', 2:'label_and_triplet'}\n",
    "  wandb.run.name = f'{data_set}_{train_mode_dict[train_mode]}_{model_number}'\n",
    "  wandb.run.save()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_code(save_dir, num_models, epochs, num_classes, batch_size,\n",
    "             lr, latent_dims):\n",
    "  if os.path.isdir(save_dir):\n",
    "    pass\n",
    "  else:\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "\n",
    "  # test_intervals = [(540, 600), (1140, 1200), (1740, 1800), (2340, 2400)]\n",
    "  test_intervals = [(25, 30), (55, 60), (85, 90), (115, 120)]\n",
    "  # initialize an empty list to hold the indices\n",
    "  val_indices = []\n",
    "\n",
    "  # loop through the intervals and append the indices to the list\n",
    "  for start, stop in test_intervals:\n",
    "      val_indices.extend(list(range(start, stop)))\n",
    "\n",
    "  # train_indices = (np.setdiff1d(np.arange(2400),np.array(val_indices)))\n",
    "  train_indices = (np.setdiff1d(np.arange(120),np.array(val_indices)))\n",
    "\n",
    "  # np.random.seed(56)\n",
    "  # contrast_indices  = np.concatenate((np.random.choice(np.arange(start=600, stop=2400), 600, replace=False),\n",
    "  #               np.random.choice(np.concatenate((np.arange(start=0, stop=600), np.arange(start=1200, stop=2400))), 600, replace=False),\n",
    "  #               np.random.choice(np.concatenate((np.arange(start=0, stop=1200), np.arange(start=1800, stop=2400))), 600, replace=False),\n",
    "  #               np.random.choice(np.arange(start=1800, stop=2400), 600, replace=False)))\n",
    "  contrast_indices  = np.concatenate((np.random.choice(np.arange(start=30, stop=120), 30, replace=False),\n",
    "                np.random.choice(np.concatenate((np.arange(start=0, stop=30), np.arange(start=60, stop=120))), 30, replace=False),\n",
    "                np.random.choice(np.concatenate((np.arange(start=0, stop=60), np.arange(start=90, stop=120))), 30, replace=False),\n",
    "                np.random.choice(np.arange(start=0, stop=90), 30, replace=False)))\n",
    "\n",
    "  for data_set in ['set_A','set_B','set_C']:\n",
    "    for train_mode in tqdm(range(3)):\n",
    "     # torch.manual_seed(0)\n",
    "      for model in range(num_models):\n",
    "        wandb_init(epochs, lr, train_mode, batch_size, model,data_set)\n",
    "\n",
    "        # if data_set=='set_A':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_A_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_A_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_A_labs).to(torch.int64))\n",
    "        # elif data_set=='set_B':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_B_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_B_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_B_labs).to(torch.int64))\n",
    "        # elif data_set=='set_C':\n",
    "        #   train_data = TensorDataset(Resize(32)(torch.tensor(set_C_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_C_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "        #                              torch.tensor(set_C_labs).to(torch.int64))\n",
    "\n",
    "        if data_set=='set_A':\n",
    "          train_data = TensorDataset(Resize(32)(torch.tensor(set_A_sub_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_A_sub_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "                                     torch.tensor(set_A_sub_labs).to(torch.int64))\n",
    "        elif data_set=='set_B':\n",
    "          train_data = TensorDataset(Resize(32)(torch.tensor(set_B_sub_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_B_sub_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "                                     torch.tensor(set_B_sub_labs).to(torch.int64))\n",
    "        elif data_set=='set_C':\n",
    "          train_data = TensorDataset(Resize(32)(torch.tensor(set_C_sub_ims.transpose(0,3,1,2)/255).float()), Resize(32)(torch.tensor(set_C_sub_ims[contrast_indices].transpose(0,3,1,2)/255).float()),\\\n",
    "                                     torch.tensor(set_C_sub_labs).to(torch.int64))\n",
    "          \n",
    "        val_data = torch.utils.data.Subset(train_data, val_indices)\n",
    "        train_data = torch.utils.data.Subset(train_data, train_indices)\n",
    "       \n",
    "\n",
    "        train_data = torch.utils.data.DataLoader(train_data, \n",
    "                                                batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "        val_data = torch.utils.data.DataLoader(val_data, \n",
    "                                                batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "        \n",
    "     \n",
    "\n",
    "        train_obj = TrainModels(latent_dims, num_classes).to(device) # GPU\n",
    "        optimizer = torch.optim.Adam(train_obj.parameters(), lr=lr, weight_decay=1e-05)\n",
    "        train_triplet_losses, train_label_losses, \\\n",
    "          val_triplet_losses, val_label_losses, \\\n",
    "            train_losses, val_losses, train_accuracies, val_accuracies= train_obj.training_loop(train_data = train_data,\n",
    "                                                            test_data = val_data,\n",
    "                                                            epochs = epochs,\n",
    "                                                            optimizer = optimizer, \n",
    "                                                            train_mode = train_mode)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print('validation triplet loss:',val_triplet_losses,'validation total loss:',val_losses,'validation accuracy:',val_accuracies)\n",
    "        # wandb.log({\"train_img_loss\": train_img_loss, \n",
    "        #           \"train_label_loss\":train_label_loss, \n",
    "        #           \"val_img_loss\":val_img_loss, \n",
    "        #           \"val_label_loss\":val_label_loss, \n",
    "        #           \"train_losses\":train_losses, \n",
    "        #           \"val_losses\":val_losses, \n",
    "        #           \"train_accuracy\":train_accuracy, \n",
    "        #           \"val_accuracy\":val_accuracy})\n",
    "        train_mode_dict = {0:'triplet', 1:'label',2:'label_and_triplet' }\n",
    "        torch.save(train_obj.triplet_lab_model.state_dict(), os.path.join(save_dir,f'{data_set}_{train_mode_dict[train_mode]}_{model}.pth'))\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fc5bf9227f4056979b12d35ace387a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666875653900206, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/kmukherjee/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20231005_031933-n417sgdr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/n417sgdr' target=\"_blank\">jolly-terrain-230</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/n417sgdr' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/n417sgdr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:06<00:00, 45.84it/s]\n",
      " 33%|███▎      | 1/3 [00:10<00:21, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.6863194108009338, 0.7443161010742188, 0.8009245991706848, 0.7991935014724731, 0.7619277834892273, 0.6999549865722656, 0.6141796112060547, 0.550216019153595, 0.48668718338012695, 0.4324496388435364, 0.39540576934814453, 0.36474737524986267, 0.31269606947898865, 0.2687753736972809, 0.23903194069862366, 0.21765242516994476, 0.19601447880268097, 0.1802210658788681, 0.17262884974479675, 0.16479651629924774, 0.15351535379886627, 0.14423082768917084, 0.139659583568573, 0.1307387351989746, 0.12366408109664917, 0.11973211914300919, 0.11286409199237823, 0.10424084961414337, 0.09789971262216568, 0.09331382811069489, 0.090701162815094, 0.08917014300823212, 0.08837150782346725, 0.08392061293125153, 0.08462650328874588, 0.0861627385020256, 0.08798081427812576, 0.08968912810087204, 0.09120944887399673, 0.09260817617177963, 0.09376010298728943, 0.09473182260990143, 0.09560619294643402, 0.09643100947141647, 0.09725721925497055, 0.09817173331975937, 0.09917701780796051, 0.10017559677362442, 0.10115323215723038, 0.10210669040679932, 0.1029430404305458, 0.10367977619171143, 0.10432883352041245, 0.10484303534030914, 0.10657898336648941, 0.10792350769042969, 0.10888145118951797, 0.10952933877706528, 0.10991312563419342, 0.11009638756513596, 0.11016710102558136, 0.11016013473272324, 0.11004676669836044, 0.10994645208120346, 0.11134248226881027, 0.11250536888837814, 0.11351398378610611, 0.11463043838739395, 0.1156972199678421, 0.11671551316976547, 0.1175919771194458, 0.11823663860559464, 0.11867691576480865, 0.11901314556598663, 0.12002614885568619, 0.12119295448064804, 0.12251400202512741, 0.12329369783401489, 0.12484778463840485, 0.12684744596481323, 0.1297525018453598, 0.13232062757015228, 0.13344967365264893, 0.13267265260219574, 0.13114240765571594, 0.1294325292110443, 0.1276099532842636, 0.125802144408226, 0.12289603054523468, 0.12091070413589478, 0.11946932226419449, 0.11824502795934677, 0.11316005140542984, 0.11031817644834518, 0.10876678675413132, 0.10812175273895264, 0.10648705065250397, 0.10726560652256012, 0.10471802949905396, 0.10389115661382675, 0.10364490002393723, 0.10300680249929428, 0.10268491506576538, 0.10256029665470123, 0.10304076969623566, 0.10362543165683746, 0.10426061600446701, 0.10501112788915634, 0.10574369877576828, 0.10638244450092316, 0.1069665476679802, 0.10759563744068146, 0.11571665853261948, 0.12090511620044708, 0.12511347234249115, 0.12801669538021088, 0.1301322728395462, 0.1315971314907074, 0.12661057710647583, 0.12525303661823273, 0.12575651705265045, 0.12413480132818222, 0.12197104841470718, 0.1199788972735405, 0.1181923970580101, 0.11604773998260498, 0.11408638209104538, 0.11226164549589157, 0.11069794744253159, 0.10921911150217056, 0.10765416920185089, 0.1074809804558754, 0.10717598348855972, 0.1063205748796463, 0.1129366084933281, 0.11692936718463898, 0.11256237328052521, 0.10859741270542145, 0.0982682853937149, 0.09726505726575851, 0.09996473044157028, 0.10255417972803116, 0.10597354173660278, 0.1102486401796341, 0.10953821241855621, 0.11014070361852646, 0.11189103126525879, 0.11627259105443954, 0.12029612064361572, 0.12373001873493195, 0.12608954310417175, 0.12783779203891754, 0.1297953575849533, 0.1316928118467331, 0.13341684639453888, 0.13590483367443085, 0.1364470273256302, 0.13569621741771698, 0.12173878401517868, 0.12111258506774902, 0.11938589066267014, 0.11766355484724045, 0.11610288918018341, 0.11775752156972885, 0.11660845577716827, 0.11889302730560303, 0.12898461520671844, 0.13686850666999817, 0.14348529279232025, 0.1563294380903244, 0.1593470573425293, 0.13712431490421295, 0.12659011781215668, 0.11831040680408478, 0.11103947460651398, 0.10261434316635132, 0.09566213190555573, 0.0894186869263649, 0.08315271139144897, 0.07720514386892319, 0.07167398929595947, 0.06681974977254868, 0.06411176174879074, 0.06196191906929016, 0.0603671632707119, 0.056257545948028564, 0.05443406105041504, 0.053555022925138474, 0.053758859634399414, 0.05396559461951256, 0.05402263626456261, 0.054050542414188385, 0.05415384843945503, 0.05422414466738701, 0.05432647466659546, 0.05457673221826553, 0.05521545559167862, 0.05773000791668892, 0.05975068733096123, 0.06107328087091446, 0.0620298758149147, 0.067938432097435, 0.07010343670845032, 0.0722348690032959, 0.07398178428411484, 0.07537303119897842, 0.0764785185456276, 0.07278092950582504, 0.07417044043540955, 0.07838167995214462, 0.0809527188539505, 0.08182986825704575, 0.0827271118760109, 0.08433189243078232, 0.08758906275033951, 0.09128602594137192, 0.0901574119925499, 0.08729860931634903, 0.08424732089042664, 0.0817994624376297, 0.07978001236915588, 0.07813260704278946, 0.07661834359169006, 0.07511135190725327, 0.07390718907117844, 0.07267246395349503, 0.07156568020582199, 0.07055249065160751, 0.06966280192136765, 0.06879883259534836, 0.0679439827799797, 0.06972572952508926, 0.07086584717035294, 0.07163858413696289, 0.07208936661481857, 0.0723206028342247, 0.07267149537801743, 0.07288406044244766, 0.07296907156705856, 0.07301012426614761, 0.07301779836416245, 0.07286236435174942, 0.07272735983133316, 0.07260046154260635, 0.07248921692371368, 0.07241016626358032, 0.07234527915716171, 0.0722789317369461, 0.07218892872333527, 0.07209981977939606, 0.07200366258621216, 0.07193320989608765, 0.07188647240400314, 0.0730869397521019, 0.0775148794054985, 0.08192853629589081, 0.08504699915647507, 0.08732961863279343, 0.08900319784879684, 0.09063615649938583, 0.09202521294355392, 0.08552498370409012, 0.08255527913570404, 0.0809849351644516, 0.08005966246128082, 0.08229712396860123, 0.08431269973516464, 0.08492270857095718, 0.08535002917051315, 0.08611052483320236, 0.08702605217695236, 0.08795434981584549, 0.08904203027486801, 0.09045912325382233, 0.09174542874097824, 0.0930413082242012, 0.09414193779230118, 0.09496992826461792, 0.09578525274991989, 0.09655731916427612, 0.097264863550663, 0.09786032140254974, 0.0984763652086258, 0.09904894977807999, 0.0995858684182167, 0.10008623450994492, 0.1005239263176918, 0.10110540688037872, 0.10179760307073593, 0.10240554809570312, 0.10298514366149902, 0.10389983654022217, 0.10521640628576279, 0.10641217231750488, 0.10871835052967072, 0.11282600462436676, 0.1198742613196373, 0.12269280105829239, 0.12399175018072128, 0.12548546493053436] validation total loss: [1.4195199012756348, 1.4910279512405396, 1.5544564723968506, 1.5603010654449463, 1.521153450012207, 1.4497696161270142, 1.351963758468628, 1.286850929260254, 1.2256135940551758, 1.1722517013549805, 1.1341991424560547, 1.105047345161438, 1.0568695068359375, 1.0176379680633545, 0.9911915063858032, 0.9722950458526611, 0.9530348181724548, 0.938426673412323, 0.931097149848938, 0.9239147901535034, 0.912816047668457, 0.9038376808166504, 0.8995776772499084, 0.8895995616912842, 0.8820801973342896, 0.8776096105575562, 0.870481550693512, 0.8613003492355347, 0.8546048998832703, 0.8497563004493713, 0.8469780683517456, 0.8452038764953613, 0.8442485332489014, 0.838330090045929, 0.8383200764656067, 0.8394711017608643, 0.8411406874656677, 0.842775285243988, 0.8443337678909302, 0.8457714319229126, 0.8469866514205933, 0.848038375377655, 0.8490352630615234, 0.8499466776847839, 0.8508487343788147, 0.8518189191818237, 0.8528764843940735, 0.8539016246795654, 0.8549239635467529, 0.855918824672699, 0.8567898869514465, 0.857550859451294, 0.8582143783569336, 0.8587242364883423, 0.860110878944397, 0.861126184463501, 0.8617222309112549, 0.8619314432144165, 0.8619925379753113, 0.8619409203529358, 0.8617915511131287, 0.8615880608558655, 0.8612667918205261, 0.860974907875061, 0.8623672723770142, 0.8633434176445007, 0.8643267750740051, 0.8654919862747192, 0.8668108582496643, 0.868241548538208, 0.8701750040054321, 0.8715382218360901, 0.8724465370178223, 0.8730225563049316, 0.8742750287055969, 0.8756303191184998, 0.8767966032028198, 0.8778932690620422, 0.880139946937561, 0.8828001618385315, 0.8863808512687683, 0.8895953893661499, 0.8912490010261536, 0.8903799057006836, 0.8886010646820068, 0.8867580890655518, 0.8850113153457642, 0.8833169341087341, 0.8811864852905273, 0.8796989917755127, 0.8785086870193481, 0.877322256565094, 0.873337984085083, 0.8711739778518677, 0.8697192668914795, 0.8687232136726379, 0.8658924698829651, 0.8658732175827026, 0.8631173968315125, 0.8621456027030945, 0.8618363738059998, 0.8615187406539917, 0.8613337278366089, 0.8611918687820435, 0.8614270091056824, 0.8618476390838623, 0.8622748255729675, 0.8627391457557678, 0.8631939888000488, 0.8635761737823486, 0.8639331459999084, 0.8643933534622192, 0.8731393814086914, 0.8781021237373352, 0.8804597854614258, 0.8822629451751709, 0.8833788633346558, 0.8842427730560303, 0.8776307106018066, 0.8757280111312866, 0.8757959604263306, 0.8742107152938843, 0.871768593788147, 0.8685047030448914, 0.8654763102531433, 0.8618776798248291, 0.8586649298667908, 0.8559200167655945, 0.8530906438827515, 0.8517914414405823, 0.8504029512405396, 0.8498160243034363, 0.8487982153892517, 0.8472622036933899, 0.8524428009986877, 0.8552234768867493, 0.8490816354751587, 0.8435114622116089, 0.8337419033050537, 0.8324971199035645, 0.8361542820930481, 0.839591920375824, 0.8436441421508789, 0.8482530117034912, 0.847655177116394, 0.8484097719192505, 0.8510381579399109, 0.8558071255683899, 0.8605831861495972, 0.8648222088813782, 0.868274450302124, 0.8713658452033997, 0.8745200634002686, 0.8778916597366333, 0.8809002637863159, 0.8833270072937012, 0.8840246796607971, 0.8836514949798584, 0.8704829216003418, 0.87266606092453, 0.871307909488678, 0.8702322244644165, 0.8713418245315552, 0.8772748708724976, 0.8785314559936523, 0.8846371173858643, 0.8966754078865051, 0.9053817987442017, 0.91213458776474, 0.9271414279937744, 0.9267865419387817, 0.9016045928001404, 0.8888091444969177, 0.8790940046310425, 0.8702670335769653, 0.8616809248924255, 0.8545622229576111, 0.8479154109954834, 0.8408100008964539, 0.8337150812149048, 0.8271285891532898, 0.8212606310844421, 0.8176649808883667, 0.814661979675293, 0.8123528957366943, 0.8063845038414001, 0.803576648235321, 0.8021706938743591, 0.8020206093788147, 0.80186527967453, 0.8015828132629395, 0.8013501167297363, 0.80128014087677, 0.8012253642082214, 0.8011744022369385, 0.8012380003929138, 0.8031297922134399, 0.8067577481269836, 0.8099188208580017, 0.8122795820236206, 0.8141261339187622, 0.8214109539985657, 0.8248976469039917, 0.8274007439613342, 0.8292180895805359, 0.8304045796394348, 0.8313283324241638, 0.8249249458312988, 0.8226710557937622, 0.8250306248664856, 0.8260251879692078, 0.8257625699043274, 0.8260815143585205, 0.8275790214538574, 0.8313641548156738, 0.8357067704200745, 0.8342229723930359, 0.8316386938095093, 0.8294107913970947, 0.8277381658554077, 0.8264278173446655, 0.8255124688148499, 0.8245389461517334, 0.8236585855484009, 0.8231242299079895, 0.8224710822105408, 0.8218325972557068, 0.8212400078773499, 0.8206517696380615, 0.8199947476387024, 0.8193789720535278, 0.8219357132911682, 0.8237442374229431, 0.8251804709434509, 0.8263533115386963, 0.8272612690925598, 0.828208327293396, 0.8289313912391663, 0.8294664025306702, 0.8298981189727783, 0.8301700353622437, 0.8302420973777771, 0.8303443193435669, 0.8304386734962463, 0.8305230140686035, 0.8306053876876831, 0.8306819796562195, 0.8307497501373291, 0.8307801485061646, 0.830804169178009, 0.8308238387107849, 0.8308404088020325, 0.8308651447296143, 0.8333221673965454, 0.8391477465629578, 0.8446283340454102, 0.8490557074546814, 0.852461576461792, 0.8551245331764221, 0.8576293587684631, 0.8597282767295837, 0.8562532663345337, 0.8549625277519226, 0.8544078469276428, 0.8551325798034668, 0.8590970039367676, 0.8620006442070007, 0.8630053997039795, 0.8635454773902893, 0.8641632795333862, 0.8647250533103943, 0.8649641871452332, 0.8652649521827698, 0.8660925030708313, 0.866862952709198, 0.8674826622009277, 0.8679949641227722, 0.8682725429534912, 0.8686281442642212, 0.8690061569213867, 0.8693901896476746, 0.8696637153625488, 0.8699898719787598, 0.8703134655952454, 0.8706393837928772, 0.8709738850593567, 0.8712561726570129, 0.8717096447944641, 0.8722943067550659, 0.8728048205375671, 0.8732932806015015, 0.8751684427261353, 0.8770045042037964, 0.8785644173622131, 0.8808553218841553, 0.8846533894538879, 0.8912364840507507, 0.8929659724235535, 0.8931827545166016, 0.8935160636901855] validation accuracy: [0.3, 0.25, 0.25, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25, 0.2, 0.2, 0.15, 0.25, 0.2, 0.2, 0.15, 0.15, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.3, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.3, 0.35, 0.3, 0.25, 0.2, 0.25, 0.2, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.3, 0.3, 0.25, 0.25, 0.2, 0.2, 0.2, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.3, 0.3, 0.35, 0.35, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:n417sgdr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>██████▅▅▅███▁███████████████████████████</td></tr><tr><td>total train loss</td><td>█▃▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▃▁▁▂▁▃▄▄▂▂▁▂▂▂▂▃▃▃▅▆▆▆▅</td></tr><tr><td>total validation loss</td><td>█▅▃▂▁▁▁▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂</td></tr><tr><td>train label accuracy</td><td>▅▁▂▃▃▂▃▃▄▃▃▄▄▄▂▃▅▅█▆▆▇▅▅▆▅▇▇▅▄▅▆▅▅▅▄▃▄▄▄</td></tr><tr><td>train label loss</td><td>▁▄▃▃▃▃▃▃▃▄▄▅▄▅▅▅▆▄▁▂▂▂▄▅▅▃▃▂▃▃▂▃▄▅▄▆███▆</td></tr><tr><td>train triplet loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▅▅▁▁▃▅▅▃▅▆▆▅▃▅▃▃▅█████▃▆▁▃▃▆▆▃▃▃▃▃▁▅▅▅▅▃</td></tr><tr><td>validation label loss</td><td>▅▁▄▅▄▄▄▄▄▄▄▅▅▅▅▄▃▂▁▂▂▄▆▅▄▃▄▄▃▃▄▄▅▅▆█▇▇▇▆</td></tr><tr><td>validation triplet loss</td><td>█▆▃▂▁▁▁▂▂▂▂▂▂▁▁▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.74456</td></tr><tr><td>total validation loss</td><td>0.89352</td></tr><tr><td>train label accuracy</td><td>0.26</td></tr><tr><td>train label loss</td><td>0.74456</td></tr><tr><td>train triplet loss</td><td>0.0</td></tr><tr><td>validation label accuracy</td><td>0.2</td></tr><tr><td>validation label loss</td><td>0.76803</td></tr><tr><td>validation triplet loss</td><td>0.12549</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jolly-terrain-230</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/n417sgdr' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/n417sgdr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231005_031933-n417sgdr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:n417sgdr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/kmukherjee/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20231005_031943-1atsg81l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/1atsg81l' target=\"_blank\">hearty-leaf-231</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/1atsg81l' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/1atsg81l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:05<00:00, 57.14it/s]\n",
      " 67%|██████▋   | 2/3 [00:24<00:12, 12.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.7960683703422546, 0.7744645476341248, 0.7710610628128052, 0.7754284739494324, 0.7944244742393494, 0.8154653906822205, 0.8320245146751404, 0.8466028571128845, 0.8515462875366211, 0.8392183184623718, 0.8445754051208496, 0.8067044615745544, 0.7654900550842285, 0.7485978007316589, 0.6725867390632629, 0.6060080528259277, 0.612507164478302, 0.6231316924095154, 0.6411065459251404, 0.6418043971061707, 0.6141659617424011, 0.5742014646530151, 0.5387457013130188, 0.5102128982543945, 0.49110427498817444, 0.47888800501823425, 0.4699229896068573, 0.46321240067481995, 0.4582049548625946, 0.4542270600795746, 0.450664758682251, 0.4477056562900543, 0.4449353814125061, 0.4423947334289551, 0.44031858444213867, 0.43868574500083923, 0.43748483061790466, 0.43679556250572205, 0.43629246950149536, 0.4357615113258362, 0.4352361261844635, 0.4345637857913971, 0.43380728363990784, 0.43307915329933167, 0.4323634207248688, 0.43170738220214844, 0.431183785200119, 0.4308241009712219, 0.430526465177536, 0.43041345477104187, 0.43031617999076843, 0.43027088046073914, 0.4301539957523346, 0.42999324202537537, 0.42975884675979614, 0.4294893443584442, 0.42923837900161743, 0.4289587140083313, 0.42874717712402344, 0.4286080300807953, 0.428523987531662, 0.4285278022289276, 0.42847710847854614, 0.4284982681274414, 0.42850151658058167, 0.42848044633865356, 0.4284752309322357, 0.42840537428855896, 0.42832276225090027, 0.42819735407829285, 0.42809587717056274, 0.4280104339122772, 0.4279479682445526, 0.42789769172668457, 0.427858829498291, 0.4278460443019867, 0.42783021926879883, 0.4278247356414795, 0.4277925491333008, 0.4277437627315521, 0.42770346999168396, 0.42765045166015625, 0.42758646607398987, 0.42754194140434265, 0.42751213908195496, 0.42749521136283875, 0.42746636271476746, 0.4274516701698303, 0.4274359345436096, 0.42740607261657715, 0.42736539244651794, 0.4273321330547333, 0.42729640007019043, 0.42724284529685974, 0.4271796643733978, 0.4271312654018402, 0.4271073341369629, 0.4270571172237396, 0.42701855301856995, 0.42698392271995544, 0.4269493520259857, 0.42693644762039185, 0.42691174149513245, 0.4268767535686493, 0.42684122920036316, 0.4268088936805725, 0.4267826974391937, 0.4267411231994629, 0.42669549584388733, 0.42667245864868164, 0.4266221225261688, 0.426584392786026, 0.42654791474342346, 0.4265112578868866, 0.4264683723449707, 0.4264400601387024, 0.4264279901981354, 0.4264194667339325, 0.4263961911201477, 0.4263669550418854, 0.4263399541378021, 0.4263180196285248, 0.4262995719909668, 0.4262700080871582, 0.4262324273586273, 0.42617732286453247, 0.4261259138584137, 0.4260791838169098, 0.4260404706001282, 0.4260181486606598, 0.4260121285915375, 0.42600932717323303, 0.42598792910575867, 0.4259676933288574, 0.4259423315525055, 0.42590752243995667, 0.4258926510810852, 0.42586833238601685, 0.4258381426334381, 0.42581185698509216, 0.4257965087890625, 0.42577001452445984, 0.42573076486587524, 0.42568498849868774, 0.42565488815307617, 0.42562779784202576, 0.425600528717041, 0.4255773723125458, 0.4255619943141937, 0.425534725189209, 0.4255087971687317, 0.42548641562461853, 0.4254457950592041, 0.4254046380519867, 0.42536208033561707, 0.4253304898738861, 0.4253101348876953, 0.4252779483795166, 0.42523202300071716, 0.4252055287361145, 0.4251825511455536, 0.425147145986557, 0.42510682344436646, 0.42508015036582947, 0.42504000663757324, 0.4249970614910126, 0.42495569586753845, 0.4249264895915985, 0.42490848898887634, 0.42488905787467957, 0.4248681664466858, 0.42484250664711, 0.42480745911598206, 0.42477425932884216, 0.4247351288795471, 0.4246959388256073, 0.42465659976005554, 0.42463117837905884, 0.4246072471141815, 0.4245785176753998, 0.42456433176994324, 0.4245529770851135, 0.4245154559612274, 0.42449256777763367, 0.4244616627693176, 0.4244294762611389, 0.42439213395118713, 0.424375057220459, 0.4243331849575043, 0.4242805540561676, 0.42422300577163696, 0.42417168617248535, 0.42414066195487976, 0.42410048842430115, 0.42406564950942993, 0.4240403175354004, 0.4240194857120514, 0.4239822328090668, 0.4239508807659149, 0.42390909790992737, 0.42386436462402344, 0.4238165020942688, 0.4237782657146454, 0.42376288771629333, 0.4237407147884369, 0.4237241744995117, 0.42371034622192383, 0.42366838455200195, 0.42361363768577576, 0.4235784113407135, 0.4235416054725647, 0.42350101470947266, 0.42345696687698364, 0.4234209656715393, 0.4233936369419098, 0.42336368560791016, 0.42334362864494324, 0.4233352839946747, 0.4233139455318451, 0.423295795917511, 0.4232617914676666, 0.42321261763572693, 0.42316341400146484, 0.42311689257621765, 0.4230608642101288, 0.4230116903781891, 0.4229719340801239, 0.4229238033294678, 0.42288443446159363, 0.4228586256504059, 0.4228542447090149, 0.4228444993495941, 0.42284688353538513, 0.42282477021217346, 0.4227933883666992, 0.4227624833583832, 0.4227071702480316, 0.42265817523002625, 0.422595739364624, 0.42255234718322754, 0.42253437638282776, 0.42251864075660706, 0.42249566316604614, 0.4224763810634613, 0.4224475026130676, 0.4224233627319336, 0.42240116000175476, 0.4223836362361908, 0.42235708236694336, 0.4223242402076721, 0.42227670550346375, 0.42223259806632996, 0.4221775233745575, 0.42212948203086853, 0.4220713675022125, 0.42202338576316833, 0.42197370529174805, 0.4219330847263336, 0.4218955934047699, 0.42185765504837036, 0.42184439301490784, 0.4218272864818573, 0.42178988456726074, 0.4217340648174286, 0.42167234420776367, 0.4216112196445465, 0.4215514361858368, 0.4215031564235687, 0.42146071791648865, 0.4214276373386383, 0.42138633131980896, 0.42132264375686646, 0.4212692677974701, 0.42123958468437195, 0.4211975038051605, 0.4211510717868805, 0.4210963845252991, 0.4210434854030609, 0.42096948623657227, 0.42088839411735535, 0.42081913352012634, 0.4207514822483063, 0.42068615555763245, 0.4206169545650482, 0.42056962847709656, 0.4205237329006195, 0.420494943857193, 0.4204592704772949, 0.42038679122924805, 0.42031916975975037, 0.4202629029750824, 0.4201861321926117, 0.42012152075767517, 0.42005857825279236, 0.4200003743171692, 0.4199441075325012, 0.41989246010780334, 0.4198731482028961, 0.41984936594963074, 0.41980457305908203] validation total loss: [1.424686312675476, 1.3610070943832397, 1.3302192687988281, 1.359329342842102, 1.4765591621398926, 1.6520206928253174, 1.9283850193023682, 2.25567364692688, 2.6013569831848145, 2.789639472961426, 3.134434938430786, 2.8467416763305664, 2.3455491065979004, 1.9372913837432861, 1.3346161842346191, 0.8987701535224915, 0.9192544221878052, 1.0309656858444214, 1.2352596521377563, 1.3256886005401611, 1.203614592552185, 1.011277675628662, 0.8578864336013794, 0.7378244996070862, 0.6620429754257202, 0.6245855093002319, 0.608110785484314, 0.5963412523269653, 0.5871284604072571, 0.5790063738822937, 0.5720716714859009, 0.5654061436653137, 0.5592447519302368, 0.5532367825508118, 0.5491966009140015, 0.546268105506897, 0.5446327328681946, 0.5443801879882812, 0.5452402830123901, 0.5460036396980286, 0.5469286441802979, 0.5475764274597168, 0.5479099750518799, 0.5475317239761353, 0.5474193096160889, 0.5473451018333435, 0.5471125841140747, 0.547191858291626, 0.5474407076835632, 0.5480126738548279, 0.5486200451850891, 0.5498149394989014, 0.5508502721786499, 0.5517764091491699, 0.5521679520606995, 0.5524135828018188, 0.5526939034461975, 0.5522428750991821, 0.5521783232688904, 0.552643895149231, 0.5531684160232544, 0.5543036460876465, 0.554983913898468, 0.556175947189331, 0.5573179125785828, 0.5580915808677673, 0.5589784383773804, 0.559239387512207, 0.5594033002853394, 0.5586947202682495, 0.5585402846336365, 0.5585305094718933, 0.5588021278381348, 0.5593201518058777, 0.55974942445755, 0.5604110956192017, 0.5610688924789429, 0.561691403388977, 0.5621570348739624, 0.562379002571106, 0.5626810193061829, 0.5626715421676636, 0.5625137686729431, 0.5626336336135864, 0.563000500202179, 0.5633550882339478, 0.5636054277420044, 0.5639976263046265, 0.564159631729126, 0.564204216003418, 0.5640612840652466, 0.56390780210495, 0.5636446475982666, 0.5633485317230225, 0.5625657439231873, 0.5621786117553711, 0.5620543956756592, 0.5619744062423706, 0.5618852376937866, 0.561883807182312, 0.561884880065918, 0.5620389580726624, 0.5620317459106445, 0.5618655681610107, 0.5616692304611206, 0.5615178346633911, 0.561351478099823, 0.5609775185585022, 0.5607194900512695, 0.5605868101119995, 0.5602970719337463, 0.5599414110183716, 0.5596579909324646, 0.559421956539154, 0.5590822696685791, 0.5588408708572388, 0.5585888028144836, 0.5584813356399536, 0.5584151148796082, 0.5585168600082397, 0.5585787296295166, 0.5587329268455505, 0.5587144494056702, 0.5584630370140076, 0.5580966472625732, 0.5576570630073547, 0.5572899580001831, 0.5568704009056091, 0.5566452741622925, 0.5567346811294556, 0.5574591159820557, 0.5580184459686279, 0.5582572221755981, 0.5582797527313232, 0.5581523776054382, 0.5580168962478638, 0.5580452680587769, 0.5579128861427307, 0.5577524900436401, 0.5576711297035217, 0.5576324462890625, 0.5575478076934814, 0.557218074798584, 0.5568645596504211, 0.5566083192825317, 0.5564455986022949, 0.5561876893043518, 0.5558754801750183, 0.5558387637138367, 0.5557155609130859, 0.5555121302604675, 0.555316150188446, 0.555057168006897, 0.5549402832984924, 0.5547994375228882, 0.5547443628311157, 0.5546219944953918, 0.5544548034667969, 0.5540688633918762, 0.5538774728775024, 0.5535693764686584, 0.5532715916633606, 0.5529972910881042, 0.5527538061141968, 0.5523651242256165, 0.5519774556159973, 0.5515782833099365, 0.5513646602630615, 0.551160454750061, 0.5513095855712891, 0.5514463782310486, 0.5515820980072021, 0.5514587759971619, 0.5513181686401367, 0.5509845018386841, 0.5507383346557617, 0.5504557490348816, 0.5503242611885071, 0.5502610206604004, 0.5501804947853088, 0.5499114990234375, 0.5496525168418884, 0.5493924617767334, 0.5494890809059143, 0.5495002269744873, 0.5493908524513245, 0.549079418182373, 0.5490579009056091, 0.5488601922988892, 0.548499584197998, 0.5481893420219421, 0.5480415225028992, 0.5476871132850647, 0.5473854541778564, 0.5471272468566895, 0.5470485687255859, 0.5469600558280945, 0.5469062924385071, 0.5467679500579834, 0.5464367866516113, 0.5460798144340515, 0.5456703901290894, 0.5453566312789917, 0.5451249480247498, 0.5449339151382446, 0.5448706150054932, 0.5449092984199524, 0.5450259447097778, 0.5450982451438904, 0.54507976770401, 0.5450234413146973, 0.5447584390640259, 0.5444048643112183, 0.5440268516540527, 0.5435958504676819, 0.5433414578437805, 0.543434739112854, 0.5436514019966125, 0.5437009334564209, 0.5437763333320618, 0.5436800718307495, 0.543513834476471, 0.5433424711227417, 0.5432136654853821, 0.5429489612579346, 0.5426371097564697, 0.5422441363334656, 0.5418435335159302, 0.5415148138999939, 0.5415024161338806, 0.5416416525840759, 0.5415703058242798, 0.5414535999298096, 0.5413047075271606, 0.5412015914916992, 0.5410169959068298, 0.5405990481376648, 0.5403573513031006, 0.539996325969696, 0.5396780967712402, 0.5394208431243896, 0.539257824420929, 0.5391339659690857, 0.5391520261764526, 0.5389457941055298, 0.538867712020874, 0.5387384295463562, 0.5386824011802673, 0.5385191440582275, 0.5381363034248352, 0.5377686619758606, 0.5373437404632568, 0.5369312167167664, 0.5366454720497131, 0.5362216234207153, 0.5358341336250305, 0.5355656147003174, 0.5353010892868042, 0.5351330041885376, 0.5348920822143555, 0.5352586507797241, 0.5355241298675537, 0.5354419946670532, 0.5353894233703613, 0.5352163910865784, 0.5350171327590942, 0.5347238779067993, 0.5348151922225952, 0.5346981287002563, 0.5347565412521362, 0.5346086025238037, 0.5342174768447876, 0.5337483286857605, 0.5335788130760193, 0.5335270762443542, 0.5335632562637329, 0.5334857106208801, 0.5333127379417419, 0.5328668355941772, 0.5324282050132751, 0.5321230888366699, 0.5317665934562683, 0.5314139127731323, 0.5310664176940918, 0.5308387875556946, 0.530533492565155, 0.5304094552993774, 0.5304683446884155, 0.53018718957901, 0.5299525856971741, 0.5296915769577026, 0.5293974280357361, 0.5290845036506653, 0.5287736058235168, 0.5284169316291809, 0.528195321559906, 0.5279381275177002, 0.5280779600143433, 0.5281428694725037, 0.5280795097351074] validation accuracy: [0.25, 0.3, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.35, 0.35, 0.45, 0.8, 0.75, 0.7, 0.6, 0.6, 0.65, 0.7, 0.7, 0.75, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1atsg81l) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total train loss</td><td>█▇▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total validation loss</td><td>▄█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train label accuracy</td><td>▁▄▇█████████████████████████████████████</td></tr><tr><td>train label loss</td><td>█▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train triplet loss</td><td>▇█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▁▁▇▆████████████████████████████████████</td></tr><tr><td>validation label loss</td><td>▄█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation triplet loss</td><td>▇█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.40613</td></tr><tr><td>total validation loss</td><td>0.52808</td></tr><tr><td>train label accuracy</td><td>1.0</td></tr><tr><td>train label loss</td><td>0.0</td></tr><tr><td>train triplet loss</td><td>0.40613</td></tr><tr><td>validation label accuracy</td><td>0.95</td></tr><tr><td>validation label loss</td><td>0.10827</td></tr><tr><td>validation triplet loss</td><td>0.4198</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hearty-leaf-231</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/1atsg81l' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/1atsg81l</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231005_031943-1atsg81l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1atsg81l). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/kmukherjee/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20231005_031957-nz2v9xnh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nz2v9xnh' target=\"_blank\">polar-water-232</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nz2v9xnh' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nz2v9xnh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:06<00:00, 47.41it/s]\n",
      "100%|██████████| 3/3 [00:38<00:00, 12.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.6529640555381775, 0.6671941876411438, 0.7182641625404358, 0.731415867805481, 0.7464177012443542, 0.760253369808197, 0.7508258819580078, 0.7437281012535095, 0.7482518553733826, 0.7316343188285828, 0.6754360795021057, 0.5625578761100769, 0.45095378160476685, 0.3391636908054352, 0.24992278218269348, 0.18862663209438324, 0.13574470579624176, 0.09639106690883636, 0.09756048023700714, 0.10287284851074219, 0.08984506875276566, 0.07306090742349625, 0.0625690445303917, 0.07002898305654526, 0.08110565692186356, 0.07455454021692276, 0.0647224560379982, 0.05397266149520874, 0.044199422001838684, 0.03803696110844612, 0.032941784709692, 0.02873416244983673, 0.026792610064148903, 0.025040218606591225, 0.02375943772494793, 0.02269221656024456, 0.02164366841316223, 0.020128443837165833, 0.018858028575778008, 0.017721889540553093, 0.016619166359305382, 0.015707416459918022, 0.015247980132699013, 0.014835000038146973, 0.014401319436728954, 0.014109528623521328, 0.011639639735221863, 0.009852567687630653, 0.00815672893077135, 0.007311496417969465, 0.006652820389717817, 0.005990356206893921, 0.0053938003256917, 0.0054158358834683895, 0.005459207575768232, 0.005488434340804815, 0.0055691273882985115, 0.00394590524956584, 0.004368036985397339, 0.005596628878265619, 0.006480461452156305, 0.007114327047020197, 0.0075092497281730175, 0.0076395063661038876, 0.007669452112168074, 0.007673522923141718, 0.00796634890139103, 0.008311173878610134, 0.00997481681406498, 0.011148658581078053, 0.011816388927400112, 0.012193000875413418, 0.012363997288048267, 0.011946243233978748, 0.011772111058235168, 0.012005520053207874, 0.012424501590430737, 0.012826407328248024, 0.013180208392441273, 0.013260218314826488, 0.01326269842684269, 0.01316033024340868, 0.01303455512970686, 0.012958300299942493, 0.01299374084919691, 0.01295474823564291, 0.013022887520492077, 0.013006073422729969, 0.01291628833860159, 0.01312478817999363, 0.014828795567154884, 0.013846424408257008, 0.012816992588341236, 0.011532098054885864, 0.01041883509606123, 0.009506392292678356, 0.008634790778160095, 0.00799126923084259, 0.007291147019714117, 0.006683644838631153, 0.015162295661866665, 0.018917014822363853, 0.021068353205919266, 0.02250787615776062, 0.026741255074739456, 0.02922935225069523, 0.030710721388459206, 0.0314335823059082, 0.030002087354660034, 0.029133224859833717, 0.028910642489790916, 0.029232261702418327, 0.029802341014146805, 0.03037104941904545, 0.030891019850969315, 0.03182942792773247, 0.03264476731419563, 0.033254846930503845, 0.03303872048854828, 0.03487403318285942, 0.03651921823620796, 0.03766347095370293, 0.03839719295501709, 0.037485118955373764, 0.036785662174224854, 0.036099594086408615, 0.03564765304327011, 0.035032473504543304, 0.03457605466246605, 0.034251272678375244, 0.034047726541757584, 0.033862076699733734, 0.03366129472851753, 0.03350825980305672, 0.033376868814229965, 0.033250030130147934, 0.03312816843390465, 0.03308037295937538, 0.03310881182551384, 0.03316529095172882, 0.03326008841395378, 0.03337309509515762, 0.033499669283628464, 0.03362784907221794, 0.033745355904102325, 0.03385341539978981, 0.03394307196140289, 0.03402448818087578, 0.03409555181860924, 0.0341716930270195, 0.034256186336278915, 0.03433043137192726, 0.03440891206264496, 0.034472327679395676, 0.03453463315963745, 0.03459132835268974, 0.03464571759104729, 0.03468390554189682, 0.03471847623586655, 0.03472619876265526, 0.0347309373319149, 0.03473686799407005, 0.03473971411585808, 0.03474529832601547, 0.03476596996188164, 0.03478251025080681, 0.03478274121880531, 0.034781552851200104, 0.03478734567761421, 0.03479323908686638, 0.034791771322488785, 0.03479158505797386, 0.03479033708572388, 0.03478926420211792, 0.03478546440601349, 0.03478771448135376, 0.03478208929300308, 0.03476333245635033, 0.03474637493491173, 0.03473377600312233, 0.034731265157461166, 0.034730780869722366, 0.0347282811999321, 0.03472236916422844, 0.03471989929676056, 0.03472161665558815, 0.03472242131829262, 0.03472558408975601, 0.03472622111439705, 0.03472757712006569, 0.03472970053553581, 0.03471796214580536, 0.03470868989825249, 0.0346967950463295, 0.03468890115618706, 0.03468162193894386, 0.03466615080833435, 0.034660037606954575, 0.03465231508016586, 0.03464285656809807, 0.03463372215628624, 0.03462620824575424, 0.034622810781002045, 0.034603968262672424, 0.03459390252828598, 0.034598544239997864, 0.03460098057985306, 0.03460158035159111, 0.03459985926747322, 0.03460453823208809, 0.03461230918765068, 0.03461850807070732, 0.03461115062236786, 0.034597668796777725, 0.03458655998110771, 0.03457194194197655, 0.03456556424498558, 0.034562867134809494, 0.034559208899736404, 0.034556444734334946, 0.03455597907304764, 0.03454342484474182, 0.034534845501184464, 0.034521084278821945, 0.034508123993873596, 0.03449837118387222, 0.03448885306715965, 0.03448626026511192, 0.03448137268424034, 0.0344720259308815, 0.034471508115530014, 0.03446066379547119, 0.03445153310894966, 0.03444576635956764, 0.03443899005651474, 0.034423455595970154, 0.034406982362270355, 0.034396376460790634, 0.0343889445066452, 0.034378983080387115, 0.03437156602740288, 0.03436235710978508, 0.034354519098997116, 0.034342315047979355, 0.034331511706113815, 0.03431501239538193, 0.0343032144010067, 0.034293681383132935, 0.03428537771105766, 0.034279290586709976, 0.03427309915423393, 0.034264009445905685, 0.034246139228343964, 0.034224528819322586, 0.03421209007501602, 0.03420284762978554, 0.03419758751988411, 0.03419492021203041, 0.0341903418302536, 0.03418326750397682, 0.03417685255408287, 0.03416550159454346, 0.034144870936870575, 0.03412732481956482, 0.03411000594496727, 0.034091826528310776, 0.03407881408929825, 0.03406978026032448, 0.034061405807733536, 0.0340488962829113, 0.03403729945421219, 0.03402774780988693, 0.03402562066912651, 0.034026358276605606, 0.03402266651391983, 0.03401850536465645, 0.03401168808341026, 0.03400208428502083, 0.03399074450135231, 0.03397321701049805, 0.033960532397031784, 0.033941347151994705, 0.03392249718308449, 0.03390628099441528, 0.033894285559654236, 0.03387138620018959, 0.03384801372885704, 0.03382851183414459, 0.033814992755651474, 0.03380056098103523, 0.03378836065530777, 0.03377821668982506, 0.03376413881778717, 0.033751729875802994, 0.03373795002698898, 0.0337231382727623, 0.033713601529598236, 0.03371007367968559, 0.03370795398950577, 0.03370661288499832] validation total loss: [1.344573974609375, 1.314619779586792, 1.3105268478393555, 1.302032470703125, 1.3300063610076904, 1.4245593547821045, 1.5303730964660645, 1.6723146438598633, 1.8648350238800049, 1.939234972000122, 1.7968025207519531, 1.4727107286453247, 1.1794272661209106, 0.9126423597335815, 0.7287695407867432, 0.5913156270980835, 0.45021700859069824, 0.33254754543304443, 0.31422412395477295, 0.3207942545413971, 0.2579044997692108, 0.24678921699523926, 0.2270074188709259, 0.2581649422645569, 0.3279905617237091, 0.32689961791038513, 0.3294103741645813, 0.3132186532020569, 0.2861143946647644, 0.2647942900657654, 0.24510857462882996, 0.22599686682224274, 0.20965108275413513, 0.18940584361553192, 0.16154633462429047, 0.14127016067504883, 0.12406429648399353, 0.10976430028676987, 0.10096985846757889, 0.09344984591007233, 0.086846262216568, 0.08225148171186447, 0.07922037690877914, 0.07686362415552139, 0.07483435422182083, 0.07386300712823868, 0.06422857940196991, 0.0571284294128418, 0.05308210104703903, 0.05127009004354477, 0.049827203154563904, 0.04843946918845177, 0.04713981971144676, 0.04656023532152176, 0.04626980051398277, 0.046544454991817474, 0.04699349403381348, 0.046113815158605576, 0.047804463654756546, 0.04967470094561577, 0.05107739940285683, 0.05266307666897774, 0.05384853482246399, 0.05399392917752266, 0.05351865664124489, 0.052650049328804016, 0.05182187259197235, 0.050975631922483444, 0.041591107845306396, 0.036707520484924316, 0.03537500277161598, 0.03595660254359245, 0.037118177860975266, 0.031328488141298294, 0.03470400720834732, 0.04361206665635109, 0.04201855883002281, 0.04665220528841019, 0.05183826759457588, 0.05592728778719902, 0.060445211827754974, 0.06391903758049011, 0.06661073118448257, 0.0679953545331955, 0.0688304752111435, 0.06744419783353806, 0.06628456711769104, 0.06519129127264023, 0.06816726922988892, 0.07102618366479874, 0.09745986759662628, 0.07747965306043625, 0.06701387465000153, 0.060580283403396606, 0.053877923637628555, 0.043437838554382324, 0.03303474932909012, 0.02495473437011242, 0.01905696839094162, 0.015034342184662819, 0.020373303443193436, 0.03140302747488022, 0.045695506036281586, 0.056277498602867126, 0.13804535567760468, 0.22033345699310303, 0.2807883024215698, 0.294756144285202, 0.207047238945961, 0.1409084051847458, 0.09977193176746368, 0.07923030853271484, 0.07228197902441025, 0.0749044120311737, 0.0795801654458046, 0.08613806962966919, 0.09426737576723099, 0.10515523701906204, 0.12219889461994171, 0.13686013221740723, 0.15030471980571747, 0.1598753035068512, 0.16437600553035736, 0.1634013056755066, 0.16676026582717896, 0.16863170266151428, 0.17102915048599243, 0.17263174057006836, 0.17397809028625488, 0.17361460626125336, 0.17253904044628143, 0.1700880229473114, 0.16777725517749786, 0.1651199907064438, 0.16342221200466156, 0.16108979284763336, 0.1593322902917862, 0.15733012557029724, 0.15639913082122803, 0.15465524792671204, 0.15272502601146698, 0.1507851481437683, 0.1489524394273758, 0.1470346301794052, 0.14499710500240326, 0.142665296792984, 0.14034679532051086, 0.13853983581066132, 0.1366519182920456, 0.1348188817501068, 0.133045494556427, 0.13120684027671814, 0.1300090253353119, 0.12840670347213745, 0.12655459344387054, 0.12491658329963684, 0.1234428882598877, 0.12207718193531036, 0.12079259008169174, 0.11982639133930206, 0.11873164772987366, 0.11766193807125092, 0.11648651957511902, 0.11523929983377457, 0.11414463818073273, 0.11306202411651611, 0.1119014322757721, 0.11062753200531006, 0.1095922440290451, 0.10854554176330566, 0.10757912695407867, 0.10676097869873047, 0.10598979890346527, 0.10502119362354279, 0.10406860709190369, 0.10327284783124924, 0.1025717556476593, 0.10221989452838898, 0.10164719820022583, 0.1011473536491394, 0.10063757002353668, 0.10031725466251373, 0.09991927444934845, 0.09930695593357086, 0.09881459921598434, 0.09836547076702118, 0.09793484210968018, 0.09753286838531494, 0.09714861214160919, 0.09667389839887619, 0.09621140360832214, 0.09570791572332382, 0.09516492486000061, 0.094697967171669, 0.09429393708705902, 0.09393404424190521, 0.09361762553453445, 0.09332742542028427, 0.09286658465862274, 0.09244325757026672, 0.09202678501605988, 0.09172900766134262, 0.09142713993787766, 0.09116305410861969, 0.09100263565778732, 0.09093671292066574, 0.0908617153763771, 0.09066726267337799, 0.09051302820444107, 0.09029240906238556, 0.09003182500600815, 0.08983089029788971, 0.08960385620594025, 0.0893438383936882, 0.0890331119298935, 0.08870141208171844, 0.08849623799324036, 0.08835558593273163, 0.08829507976770401, 0.08819377422332764, 0.08796650171279907, 0.08777743577957153, 0.08764614164829254, 0.08749239891767502, 0.08724327385425568, 0.08703179657459259, 0.08671838045120239, 0.08648255467414856, 0.08625481277704239, 0.0860307365655899, 0.08590478450059891, 0.08588925004005432, 0.0858144536614418, 0.08571140468120575, 0.08560188859701157, 0.08535921573638916, 0.08511461317539215, 0.08479667454957962, 0.08446845412254333, 0.08427052199840546, 0.08405474573373795, 0.08392418920993805, 0.08381354808807373, 0.0836355984210968, 0.08343926072120667, 0.083224818110466, 0.08307737112045288, 0.08292233943939209, 0.08277972042560577, 0.08263885974884033, 0.0824209451675415, 0.08215245604515076, 0.08198018372058868, 0.08179360628128052, 0.08158925175666809, 0.08144263923168182, 0.08126132190227509, 0.0811675563454628, 0.08098717033863068, 0.08078093826770782, 0.08061297237873077, 0.08046507835388184, 0.0802827775478363, 0.08003877103328705, 0.07983154058456421, 0.07977846264839172, 0.07968871295452118, 0.07951277494430542, 0.07939150929450989, 0.07926389575004578, 0.07914259284734726, 0.07909851521253586, 0.07905523478984833, 0.07893866300582886, 0.07860797643661499, 0.07834310084581375, 0.07805091142654419, 0.07777321338653564, 0.07760687172412872, 0.07749064266681671, 0.0773652195930481, 0.07728594541549683, 0.07719534635543823, 0.07711659371852875, 0.07697168737649918, 0.07681159675121307, 0.07649217545986176, 0.0762033462524414, 0.07602375000715256, 0.07583972811698914, 0.07572914659976959, 0.07565125823020935, 0.07564251869916916, 0.07562565803527832, 0.07560920715332031, 0.0755486786365509, 0.07538412511348724, 0.07526348531246185, 0.07515564560890198, 0.07495394349098206] validation accuracy: [0.25, 0.25, 0.3, 0.4, 0.35, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.45, 0.45, 0.55, 0.6, 0.7, 0.75, 0.9, 0.85, 0.8, 0.8, 0.85, 0.85, 0.9, 0.9, 0.85, 0.85, 0.85, 0.85, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 0.9, 0.9, 0.9, 0.95, 0.95, 1.0, 1.0, 1.0, 1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:nz2v9xnh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total train loss</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total validation loss</td><td>▆█▃▂▂▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train label accuracy</td><td>▁▄▇█████████████████████████████████████</td></tr><tr><td>train label loss</td><td>█▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train triplet loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▂▁▅▇▇▇▇▇▇█▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>validation label loss</td><td>▅█▄▂▂▂▁▁▁▁▁▁▁▁▃▁▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation triplet loss</td><td>██▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.0</td></tr><tr><td>total validation loss</td><td>0.07495</td></tr><tr><td>train label accuracy</td><td>1.0</td></tr><tr><td>train label loss</td><td>0.0</td></tr><tr><td>train triplet loss</td><td>0.0</td></tr><tr><td>validation label accuracy</td><td>0.95</td></tr><tr><td>validation label loss</td><td>0.04125</td></tr><tr><td>validation triplet loss</td><td>0.03371</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-water-232</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nz2v9xnh' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nz2v9xnh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231005_031957-nz2v9xnh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:nz2v9xnh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a386a0f6b92e42fa8cce0afdd2c39556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668504942208527, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/kmukherjee/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20231005_032011-mwyiuown</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/mwyiuown' target=\"_blank\">flowing-mountain-233</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/mwyiuown' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/mwyiuown</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:06<00:00, 47.11it/s]\n",
      " 33%|███▎      | 1/3 [00:14<00:28, 14.29s/it]/mnt/ws/home/kmukherjee/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (nz2v9xnh) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.684748649597168, 0.6086589694023132, 0.6142129302024841, 0.6361970901489258, 0.6346265077590942, 0.6169469952583313, 0.6127795577049255, 0.5802468657493591, 0.5542241930961609, 0.5163036584854126, 0.472921758890152, 0.4259536862373352, 0.3859459459781647, 0.357585608959198, 0.34205397963523865, 0.32246044278144836, 0.3013746440410614, 0.288934588432312, 0.2794848382472992, 0.2813207805156708, 0.2852117717266083, 0.28095993399620056, 0.28175702691078186, 0.2826051414012909, 0.2821059823036194, 0.28391847014427185, 0.27631449699401855, 0.26772069931030273, 0.26132073998451233, 0.2558583915233612, 0.25218912959098816, 0.2514455318450928, 0.250297874212265, 0.24932371079921722, 0.24879947304725647, 0.24836821854114532, 0.24800460040569305, 0.24692454934120178, 0.24767647683620453, 0.2455265074968338, 0.24466359615325928, 0.24909289181232452, 0.2529143989086151, 0.25551578402519226, 0.25534358620643616, 0.25432756543159485, 0.2476930171251297, 0.24171052873134613, 0.23565025627613068, 0.2269740104675293, 0.21994833648204803, 0.214181050658226, 0.20920772850513458, 0.20520810782909393, 0.2060641497373581, 0.20690453052520752, 0.20755496621131897, 0.20494996011257172, 0.20900645852088928, 0.21220088005065918, 0.21543140709400177, 0.21923652291297913, 0.22209703922271729, 0.22476421296596527, 0.22763028740882874, 0.23347258567810059, 0.23872028291225433, 0.2244199514389038, 0.21919365227222443, 0.21528398990631104, 0.20872655510902405, 0.20423975586891174, 0.19981375336647034, 0.19517821073532104, 0.1850154846906662, 0.1784883588552475, 0.17392678558826447, 0.17216062545776367, 0.17045003175735474, 0.16932490468025208, 0.1724684238433838, 0.17476466298103333, 0.17371049523353577, 0.17281948029994965, 0.17166019976139069, 0.17233820259571075, 0.17655447125434875, 0.17855338752269745, 0.17965686321258545, 0.17550180852413177, 0.18276555836200714, 0.18607814610004425, 0.18678219616413116, 0.18569301068782806, 0.18286319077014923, 0.1816895753145218, 0.17692549526691437, 0.1872798055410385, 0.19751085340976715, 0.19802889227867126, 0.20014047622680664, 0.2004561424255371, 0.18492381274700165, 0.17722557485103607, 0.17208902537822723, 0.16940812766551971, 0.17804548144340515, 0.18668092787265778, 0.21200886368751526, 0.23416076600551605, 0.24379809200763702, 0.2437942773103714, 0.24254150688648224, 0.24087904393672943, 0.23736624419689178, 0.24254608154296875, 0.24651698768138885, 0.24723029136657715, 0.2485378533601761, 0.24981467425823212, 0.24856983125209808, 0.2475615292787552, 0.2462574690580368, 0.24509072303771973, 0.24409055709838867, 0.24298615753650665, 0.2418702095746994, 0.23777198791503906, 0.23093640804290771, 0.22334948182106018, 0.2188689261674881, 0.21421967446804047, 0.21031303703784943, 0.20359575748443604, 0.19763055443763733, 0.19485576450824738, 0.18994402885437012, 0.18747159838676453, 0.1801670640707016, 0.16993795335292816, 0.16798947751522064, 0.16503675282001495, 0.1621551811695099, 0.15383215248584747, 0.1488509625196457, 0.14201544225215912, 0.1388668268918991, 0.13494019210338593, 0.13528011739253998, 0.13265246152877808, 0.13255707919597626, 0.14248166978359222, 0.15858851373195648, 0.1646086424589157, 0.16390404105186462, 0.1659984290599823, 0.18080289661884308, 0.1967533826828003, 0.22392411530017853, 0.23856334388256073, 0.23874282836914062, 0.2188710719347, 0.21255584061145782, 0.20848968625068665, 0.2063373178243637, 0.20503740012645721, 0.19592007994651794, 0.18979552388191223, 0.18760131299495697, 0.19632434844970703, 0.2021469920873642, 0.20603148639202118, 0.20932570099830627, 0.1946212649345398, 0.19444335997104645, 0.20644202828407288, 0.21478310227394104, 0.23181307315826416, 0.2432098686695099, 0.2494651824235916, 0.2529817223548889, 0.2375340312719345, 0.22558806836605072, 0.21401362121105194, 0.20165680348873138, 0.19866013526916504, 0.20027771592140198, 0.20170031487941742, 0.20334234833717346, 0.20474152266979218, 0.2058301419019699, 0.20650170743465424, 0.20731759071350098, 0.20827651023864746, 0.20968018472194672, 0.21140049397945404, 0.21292078495025635, 0.214325949549675, 0.21549873054027557, 0.2166018784046173, 0.21760857105255127, 0.2185523509979248, 0.2193479835987091, 0.22002916038036346, 0.22064518928527832, 0.22192421555519104, 0.22330355644226074, 0.22464780509471893, 0.2258429080247879, 0.22680631279945374, 0.22767551243305206, 0.22859378159046173, 0.22939912974834442, 0.23010869324207306, 0.2330601066350937, 0.23218892514705658, 0.2298184186220169, 0.22693805396556854, 0.22377514839172363, 0.22118087112903595, 0.21898141503334045, 0.2170892059803009, 0.2159145325422287, 0.22120420634746552, 0.23289147019386292, 0.23882727324962616, 0.24193572998046875, 0.24360767006874084, 0.24382570385932922, 0.2460349053144455, 0.23532132804393768, 0.22807030379772186, 0.20740090310573578, 0.20885781943798065, 0.21467529237270355, 0.2223588079214096, 0.2272159606218338, 0.2303571254014969, 0.23188996315002441, 0.23231029510498047, 0.23157835006713867, 0.23545122146606445, 0.2357260286808014, 0.2323123961687088, 0.21881404519081116, 0.21426264941692352, 0.21023061871528625, 0.22108331322669983, 0.23246507346630096, 0.23640398681163788, 0.2403431236743927, 0.24145916104316711, 0.2411581128835678, 0.23997317254543304, 0.23797042667865753, 0.2386804074048996, 0.240160271525383, 0.24347352981567383, 0.24519796669483185, 0.24614734947681427, 0.24675647914409637, 0.23052750527858734, 0.2244461625814438, 0.22487221658229828, 0.22444501519203186, 0.22175519168376923, 0.2180441915988922, 0.2119787186384201, 0.19826586544513702, 0.2011212557554245, 0.20264343917369843, 0.1996699571609497, 0.2010325938463211, 0.1982593685388565, 0.20868586003780365, 0.2211189568042755, 0.22678795456886292, 0.23417659103870392, 0.2371026575565338, 0.2378547489643097, 0.23646676540374756, 0.2216472178697586, 0.2065214365720749, 0.1984935998916626, 0.19144611060619354, 0.1862058937549591, 0.18118999898433685, 0.17702695727348328, 0.1714770346879959, 0.16861306130886078, 0.16782227158546448, 0.17122457921504974, 0.17327480018138885, 0.17354445159435272, 0.1760311871767044, 0.17338018119335175, 0.17120501399040222, 0.1683833748102188, 0.17049537599086761, 0.17322364449501038] validation total loss: [1.3737843036651611, 1.3084864616394043, 1.3208085298538208, 1.3475308418273926, 1.3461027145385742, 1.3375318050384521, 1.34407377243042, 1.3151917457580566, 1.2916829586029053, 1.2555930614471436, 1.214025855064392, 1.1695106029510498, 1.1283587217330933, 1.0984705686569214, 1.0834951400756836, 1.0640312433242798, 1.043477177619934, 1.03223717212677, 1.0244548320770264, 1.0287039279937744, 1.0355174541473389, 1.0314522981643677, 1.0318039655685425, 1.0319459438323975, 1.0305662155151367, 1.0319544076919556, 1.0238326787948608, 1.0159878730773926, 1.0095727443695068, 1.003930687904358, 0.999880313873291, 0.9988422393798828, 0.9975274801254272, 0.9964907765388489, 0.9961220026016235, 0.9960322380065918, 0.9962496757507324, 0.9969372749328613, 0.9994655847549438, 0.9983522891998291, 0.9978504180908203, 1.0027649402618408, 1.0080395936965942, 1.0113698244094849, 1.0109227895736694, 1.0098388195037842, 1.0021406412124634, 0.9946985244750977, 0.9876691699028015, 0.977569580078125, 0.9693921804428101, 0.9627514481544495, 0.9570352435112, 0.9580745100975037, 0.9619759321212769, 0.9650866985321045, 0.9665321111679077, 0.963421106338501, 0.9670101404190063, 0.9704435467720032, 0.9744014739990234, 0.9787091016769409, 0.9844240546226501, 0.9895234704017639, 0.9940148591995239, 1.00046968460083, 1.0056982040405273, 0.9881758689880371, 0.9782879948616028, 0.9713364839553833, 0.9628629684448242, 0.9568207263946533, 0.9508013725280762, 0.9446926712989807, 0.9318909645080566, 0.9228857755661011, 0.9168168902397156, 0.9140800833702087, 0.9119893312454224, 0.9106472730636597, 0.911450207233429, 0.911615252494812, 0.9093211889266968, 0.9077432155609131, 0.9062272906303406, 0.9047104120254517, 0.9065593481063843, 0.9079530239105225, 0.908924400806427, 0.9042510986328125, 0.9118611812591553, 0.9161796569824219, 0.9174984693527222, 0.9162570834159851, 0.9100504517555237, 0.9065139293670654, 0.9020956158638, 0.9133830666542053, 0.924005925655365, 0.9221547842025757, 0.92104572057724, 0.9190815091133118, 0.9005252122879028, 0.8905152678489685, 0.8834415674209595, 0.8798292279243469, 0.887334942817688, 0.8973824381828308, 0.926202654838562, 0.9515872597694397, 0.9657647013664246, 0.9703158736228943, 0.9717556238174438, 0.9729863405227661, 0.9709887504577637, 0.9779431223869324, 0.9823317527770996, 0.9812573790550232, 0.9805095195770264, 0.9802112579345703, 0.9788868427276611, 0.9778266549110413, 0.9763808250427246, 0.9749017357826233, 0.9735435843467712, 0.9721623063087463, 0.9707793593406677, 0.9632527828216553, 0.9546970129013062, 0.9454213380813599, 0.9389062523841858, 0.9330639243125916, 0.928674578666687, 0.9189029932022095, 0.9106929302215576, 0.9069563746452332, 0.9004595279693604, 0.8953899145126343, 0.8864690065383911, 0.8731722235679626, 0.8685786128044128, 0.864691436290741, 0.8635280132293701, 0.8566180467605591, 0.8524026870727539, 0.8460085391998291, 0.8441671133041382, 0.841327428817749, 0.8430044054985046, 0.8414238691329956, 0.8439967036247253, 0.8669041991233826, 0.8888127207756042, 0.8963627219200134, 0.8947763442993164, 0.8962897062301636, 0.9117935299873352, 0.9280949831008911, 0.9564002156257629, 0.9717684984207153, 0.9695049524307251, 0.9475060701370239, 0.9397517442703247, 0.9293380975723267, 0.9228755235671997, 0.9183839559555054, 0.9053988456726074, 0.8983913660049438, 0.8962312340736389, 0.9077100157737732, 0.915242075920105, 0.920120358467102, 0.9233212471008301, 0.9054824709892273, 0.9069548845291138, 0.9232466220855713, 0.935427188873291, 0.9563395977020264, 0.970396876335144, 0.9779850244522095, 0.982305109500885, 0.9672732949256897, 0.9565544128417969, 0.9457864761352539, 0.9315943121910095, 0.9279088377952576, 0.9295419454574585, 0.9313913583755493, 0.9337528944015503, 0.9357874989509583, 0.9377628564834595, 0.9393066763877869, 0.9409227967262268, 0.9425854086875916, 0.9446505904197693, 0.946961522102356, 0.9489893317222595, 0.9508185982704163, 0.9523745775222778, 0.953850507736206, 0.9552229046821594, 0.9564927816390991, 0.957565426826477, 0.9584853649139404, 0.9593342542648315, 0.9609357118606567, 0.9627124667167664, 0.9644234776496887, 0.9660135507583618, 0.9673475027084351, 0.9686131477355957, 0.9699685573577881, 0.9712094068527222, 0.9723039865493774, 0.9767412543296814, 0.9780408143997192, 0.9768319725990295, 0.9747151732444763, 0.9717891812324524, 0.9695116281509399, 0.9677722454071045, 0.9660658836364746, 0.9650638103485107, 0.9697357416152954, 0.9858078956604004, 0.994877815246582, 1.0001006126403809, 1.0035204887390137, 1.0046477317810059, 1.0088361501693726, 0.9899990558624268, 0.979048490524292, 0.9488739371299744, 0.9484196901321411, 0.9542189240455627, 0.9624611139297485, 0.968519926071167, 0.9727153778076172, 0.974968671798706, 0.9755191802978516, 0.9745693802833557, 0.9811062812805176, 0.981977105140686, 0.9777956008911133, 0.9578151702880859, 0.950534462928772, 0.9444293975830078, 0.960262656211853, 0.9746619462966919, 0.978653609752655, 0.9832495450973511, 0.9853878021240234, 0.9856510758399963, 0.9842721819877625, 0.9819948077201843, 0.9830426573753357, 0.9846019148826599, 0.9882472157478333, 0.9899729490280151, 0.9907370805740356, 0.9912010431289673, 0.9753655195236206, 0.9693551659584045, 0.9685195684432983, 0.9690345525741577, 0.9660135507583618, 0.9649319648742676, 0.9587814807891846, 0.9428364038467407, 0.9434041380882263, 0.9433348178863525, 0.9385707378387451, 0.9408646821975708, 0.938969612121582, 0.9544640779495239, 0.9711011648178101, 0.9796619415283203, 0.9875290393829346, 0.989753246307373, 0.9892446994781494, 0.9862406849861145, 0.9692262411117554, 0.9495106339454651, 0.9375662803649902, 0.9273497462272644, 0.9202070236206055, 0.9135345816612244, 0.907931923866272, 0.9016839265823364, 0.8994558453559875, 0.8995929956436157, 0.9027184844017029, 0.9017333984375, 0.8988766074180603, 0.8996193408966064, 0.8956241607666016, 0.8927654027938843, 0.8808096051216125, 0.8877704739570618, 0.8954330682754517] validation accuracy: [0.25, 0.2, 0.15, 0.25, 0.25, 0.25, 0.3, 0.3, 0.25, 0.25, 0.25, 0.3, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.25, 0.3, 0.3, 0.3, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.25, 0.3, 0.3, 0.2, 0.25, 0.25, 0.25, 0.2, 0.2, 0.3, 0.25, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.25, 0.25, 0.25, 0.25, 0.25, 0.15, 0.15, 0.2, 0.2, 0.15, 0.2, 0.1, 0.1, 0.05, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.15, 0.2, 0.2, 0.3, 0.3, 0.35, 0.35, 0.35, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.15, 0.15, 0.15, 0.2, 0.25, 0.2, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.25, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.3, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:mwyiuown) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total train loss</td><td>▇▅▄▆▇█▇▇█▆▆▅▅▃▁▂▄▃▃▂▂▃▄▄▆▇████▆▆▆▆▄▃▅▇▇▇</td></tr><tr><td>total validation loss</td><td>██▄▄▃▃▃▃▃▃▂▂▂▂▂▃▃▂▁▁▁▃▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▂▂▂</td></tr><tr><td>train label accuracy</td><td>▆▇▇▆▆▆▆▆▃▆▆▅▆▅▁▄▄▄▄▄▇▄▂▁▆▅▆▆▆▆▅▃▂▄█▄▅▅▅▆</td></tr><tr><td>train label loss</td><td>▁▃▅▆▇████▇▆▆▆▄▃▄▅▅▅▄▄▅▅▅▇█████▇▇▇▇▆▅▆▇▇▇</td></tr><tr><td>train triplet loss</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▇█▇▇▇█▇▇▇▇▇▅█▇█▇▇▇█▇▇▇▄▁▄▇▇▅▇▄█▇▅▄▅▅▇▅▅▅</td></tr><tr><td>validation label loss</td><td>▂▅▅▆▆▇▇▇█▇▆▅▄▃▂▅▄▃▁▁▃▄▂▃▄▄▅▅▆▆▇▅▆▆▆▆▆▅▄▃</td></tr><tr><td>validation triplet loss</td><td>█▇▄▃▃▃▂▂▂▂▂▂▂▂▂▃▃▂▁▁▁▂▂▂▂▂▂▂▂▂▃▂▂▂▃▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.73071</td></tr><tr><td>total validation loss</td><td>0.89543</td></tr><tr><td>train label accuracy</td><td>0.25</td></tr><tr><td>train label loss</td><td>0.73071</td></tr><tr><td>train triplet loss</td><td>0.0</td></tr><tr><td>validation label accuracy</td><td>0.2</td></tr><tr><td>validation label loss</td><td>0.72221</td></tr><tr><td>validation triplet loss</td><td>0.17322</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">flowing-mountain-233</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/mwyiuown' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/mwyiuown</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231005_032011-mwyiuown/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:mwyiuown). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5a5f4c5ea84942b8900ac01a00fb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668725572526456, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/kmukherjee/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20231005_032025-nvy1sjsg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nvy1sjsg' target=\"_blank\">divine-mountain-234</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nvy1sjsg' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nvy1sjsg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:05<00:00, 55.69it/s]\n",
      " 67%|██████▋   | 2/3 [00:27<00:13, 13.58s/it]/mnt/ws/home/kmukherjee/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (nz2v9xnh) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.8203703761100769, 0.7507730722427368, 0.7528653144836426, 0.7605627179145813, 0.748887836933136, 0.745153546333313, 0.7679628729820251, 0.7537299990653992, 0.7212414145469666, 0.7650923132896423, 0.7430674433708191, 0.7356300354003906, 0.7206581234931946, 0.6830807328224182, 0.6447067260742188, 0.6178328394889832, 0.5945550799369812, 0.5783262252807617, 0.5642932653427124, 0.5524737238883972, 0.548245370388031, 0.5425359606742859, 0.5344192385673523, 0.5275249481201172, 0.5229005813598633, 0.51861971616745, 0.5137683749198914, 0.5085318684577942, 0.5034374594688416, 0.498715877532959, 0.49484559893608093, 0.4915456771850586, 0.48870736360549927, 0.4865911602973938, 0.485175222158432, 0.48451921343803406, 0.48403388261795044, 0.48384523391723633, 0.48392221331596375, 0.48430272936820984, 0.4845491945743561, 0.4844597280025482, 0.4840044677257538, 0.4832061231136322, 0.48209354281425476, 0.4807193875312805, 0.4793531894683838, 0.4779515266418457, 0.47672587633132935, 0.47583460807800293, 0.4752558767795563, 0.47502875328063965, 0.4750375747680664, 0.475161075592041, 0.47530898451805115, 0.47530150413513184, 0.4751470983028412, 0.4748004376888275, 0.47432953119277954, 0.4737953245639801, 0.4732219874858856, 0.4727766215801239, 0.47246265411376953, 0.4722933769226074, 0.47213074564933777, 0.4719739556312561, 0.47185489535331726, 0.4717560410499573, 0.47165727615356445, 0.4715152680873871, 0.47126537561416626, 0.4709521234035492, 0.4705873429775238, 0.47020331025123596, 0.4698280990123749, 0.4694760739803314, 0.4691580832004547, 0.4688754677772522, 0.4686225950717926, 0.46844950318336487, 0.46829739212989807, 0.46816903352737427, 0.46805840730667114, 0.4679643213748932, 0.4678468704223633, 0.4676661491394043, 0.4674246907234192, 0.46714600920677185, 0.46685791015625, 0.46660956740379333, 0.4663785994052887, 0.4661816656589508, 0.46598777174949646, 0.4658263325691223, 0.46569547057151794, 0.46558627486228943, 0.4654655158519745, 0.46534261107444763, 0.4652426838874817, 0.4651280343532562, 0.46500474214553833, 0.4648699462413788, 0.46476274728775024, 0.464669793844223, 0.46461376547813416, 0.4645436406135559, 0.46449390053749084, 0.4644336402416229, 0.4643940031528473, 0.4643417298793793, 0.46424636244773865, 0.4641380310058594, 0.4640003740787506, 0.4638651907444, 0.46373674273490906, 0.4635975956916809, 0.46343371272087097, 0.4633084833621979, 0.46320801973342896, 0.46314412355422974, 0.46310916543006897, 0.463062584400177, 0.46300774812698364, 0.46292373538017273, 0.4628256857395172, 0.4627090394496918, 0.4626033902168274, 0.46248531341552734, 0.4623514711856842, 0.46226951479911804, 0.46221238374710083, 0.4621466100215912, 0.46206846833229065, 0.46197137236595154, 0.4618972837924957, 0.46183258295059204, 0.46174222230911255, 0.4616653025150299, 0.4615868031978607, 0.4614934027194977, 0.4614119231700897, 0.4613112509250641, 0.4612191319465637, 0.4611191749572754, 0.46099257469177246, 0.4608837068080902, 0.4607837200164795, 0.46066799759864807, 0.4605632424354553, 0.4604528844356537, 0.4603652060031891, 0.46026611328125, 0.4601472318172455, 0.46004125475883484, 0.4599333703517914, 0.4598279893398285, 0.4597507417201996, 0.45966893434524536, 0.4595843255519867, 0.45948272943496704, 0.45939549803733826, 0.4592849910259247, 0.45917654037475586, 0.459085613489151, 0.45900198817253113, 0.4588969349861145, 0.45881175994873047, 0.4587087333202362, 0.45860353112220764, 0.45848312973976135, 0.4583936631679535, 0.45827481150627136, 0.4581471383571625, 0.4580240249633789, 0.4579017758369446, 0.45780882239341736, 0.4577062726020813, 0.45761123299598694, 0.4575013220310211, 0.4573887288570404, 0.45729270577430725, 0.45719337463378906, 0.45709019899368286, 0.45698997378349304, 0.4568781852722168, 0.45673543214797974, 0.4565962851047516, 0.45646753907203674, 0.45636770129203796, 0.456289678812027, 0.45623037219047546, 0.4561634957790375, 0.4560665786266327, 0.4559743106365204, 0.4558740556240082, 0.4557935297489166, 0.45570775866508484, 0.45562082529067993, 0.45550116896629333, 0.4553816318511963, 0.45525819063186646, 0.4551205337047577, 0.4550071656703949, 0.454942524433136, 0.45487698912620544, 0.45477601885795593, 0.454735666513443, 0.4546426236629486, 0.4545416533946991, 0.454439640045166, 0.45436030626296997, 0.4542756974697113, 0.4542202651500702, 0.4541458785533905, 0.4540560245513916, 0.45392242074012756, 0.453796923160553, 0.4536925256252289, 0.45359721779823303, 0.45350170135498047, 0.45341330766677856, 0.45333805680274963, 0.45325860381126404, 0.4531519114971161, 0.45303773880004883, 0.4529225826263428, 0.4528101980686188, 0.45268502831459045, 0.45258960127830505, 0.4524649679660797, 0.45237231254577637, 0.4522983133792877, 0.4522678554058075, 0.4522176682949066, 0.4521593153476715, 0.4521162211894989, 0.45203161239624023, 0.4519060254096985, 0.45181378722190857, 0.45169392228126526, 0.45158815383911133, 0.45146337151527405, 0.45131802558898926, 0.4511464536190033, 0.45097777247428894, 0.4508512020111084, 0.4508000314235687, 0.45072755217552185, 0.45061197876930237, 0.45048490166664124, 0.4503724277019501, 0.45024481415748596, 0.4501282870769501, 0.4500718116760254, 0.4499959945678711, 0.44992247223854065, 0.44984540343284607, 0.44979333877563477, 0.44974285364151, 0.44971489906311035, 0.44971537590026855, 0.4496586322784424, 0.4495728611946106, 0.44951197504997253, 0.44941446185112, 0.4492512345314026, 0.44901761412620544, 0.4487628936767578, 0.448536217212677, 0.44833770394325256, 0.44819656014442444, 0.44805747270584106, 0.44793662428855896, 0.44783836603164673, 0.44780394434928894, 0.44772621989250183, 0.44760456681251526, 0.4474627673625946, 0.44728565216064453, 0.4471552073955536, 0.4470166862010956, 0.44688400626182556, 0.4467941224575043, 0.44672146439552307, 0.44665583968162537, 0.4465723931789398, 0.44644615054130554, 0.44634491205215454, 0.44626447558403015, 0.446183443069458, 0.44604501128196716, 0.44587692618370056, 0.4457693099975586, 0.4457229673862457, 0.4456239640712738, 0.4455042779445648, 0.4453648030757904, 0.44529294967651367, 0.4452143609523773, 0.44510871171951294] validation total loss: [1.4619579315185547, 1.364525318145752, 1.3574061393737793, 1.3485795259475708, 1.2730605602264404, 1.2007888555526733, 1.2245666980743408, 1.1905438899993896, 1.0772993564605713, 1.1066557168960571, 1.096514344215393, 1.115317463874817, 1.1811110973358154, 1.1435370445251465, 0.9863497018814087, 0.885505199432373, 0.8158175349235535, 0.770477294921875, 0.7312793135643005, 0.6962447166442871, 0.6766393184661865, 0.6542993783950806, 0.6289322376251221, 0.6092891097068787, 0.5971792936325073, 0.5905029773712158, 0.5845361351966858, 0.5781385898590088, 0.5720139145851135, 0.5660791993141174, 0.5603997111320496, 0.5558289289474487, 0.5519683957099915, 0.5480480790138245, 0.544312059879303, 0.5412154197692871, 0.5378019213676453, 0.5340733528137207, 0.5309034585952759, 0.5284231901168823, 0.5266397595405579, 0.5250493288040161, 0.5236302018165588, 0.5226477980613708, 0.5222387313842773, 0.5226807594299316, 0.5233189463615417, 0.523928701877594, 0.5241958498954773, 0.5239807367324829, 0.5232824087142944, 0.5224157571792603, 0.5211892127990723, 0.519760012626648, 0.5186757445335388, 0.5180587768554688, 0.5176923274993896, 0.5176320672035217, 0.5178691744804382, 0.5183620452880859, 0.5185917615890503, 0.5175572633743286, 0.5162026286125183, 0.5145595669746399, 0.51292884349823, 0.5111493468284607, 0.5095233917236328, 0.5082199573516846, 0.5071762204170227, 0.5064173936843872, 0.5059771537780762, 0.5056875944137573, 0.5056293606758118, 0.5055938959121704, 0.5054994225502014, 0.5053528547286987, 0.5052481889724731, 0.5051291584968567, 0.5049598813056946, 0.504748523235321, 0.5045350790023804, 0.5042541027069092, 0.5040451884269714, 0.5036643743515015, 0.5032119750976562, 0.5027331113815308, 0.502208948135376, 0.501792848110199, 0.5013819336891174, 0.5009064674377441, 0.5004385113716125, 0.4999718964099884, 0.49960964918136597, 0.4993080794811249, 0.4990684986114502, 0.49890321493148804, 0.4988309144973755, 0.4988733232021332, 0.4990116357803345, 0.499286025762558, 0.49965742230415344, 0.5001125335693359, 0.5005549192428589, 0.5009877681732178, 0.501305341720581, 0.5015566349029541, 0.5018151998519897, 0.5019236207008362, 0.501837968826294, 0.5016825795173645, 0.5012977719306946, 0.5010309815406799, 0.5009480118751526, 0.501057505607605, 0.5012921690940857, 0.5016075968742371, 0.5020453929901123, 0.5023749470710754, 0.5026784539222717, 0.5028294324874878, 0.5028113722801208, 0.5026799440383911, 0.5023810267448425, 0.5020385980606079, 0.5017090439796448, 0.5014411211013794, 0.5013499855995178, 0.5012883543968201, 0.5013156533241272, 0.5013242959976196, 0.5013742446899414, 0.5014859437942505, 0.5016278624534607, 0.5017748475074768, 0.5018362998962402, 0.5017315149307251, 0.5016444325447083, 0.5015520453453064, 0.5015097260475159, 0.5015280842781067, 0.501512348651886, 0.501570999622345, 0.5016613006591797, 0.5017462372779846, 0.5019029974937439, 0.5020272731781006, 0.5020753145217896, 0.5020931959152222, 0.502002477645874, 0.5019225478172302, 0.5018008351325989, 0.5016317963600159, 0.50151127576828, 0.5013795495033264, 0.5013128519058228, 0.5012997984886169, 0.5012515187263489, 0.5011914372444153, 0.5011734962463379, 0.5011546611785889, 0.5011640787124634, 0.5011827349662781, 0.5012572407722473, 0.5013260841369629, 0.5014028549194336, 0.5014651417732239, 0.5015168190002441, 0.5015487670898438, 0.5015493035316467, 0.5015750527381897, 0.5015501976013184, 0.5014567375183105, 0.5014036893844604, 0.5013195872306824, 0.5013008117675781, 0.501352846622467, 0.5014541745185852, 0.501571774482727, 0.5017226934432983, 0.5017338991165161, 0.5017308592796326, 0.5017114281654358, 0.5016846656799316, 0.5016264319419861, 0.501552402973175, 0.5015869736671448, 0.5015451312065125, 0.5015485882759094, 0.5014635920524597, 0.501383364200592, 0.5012826919555664, 0.501232385635376, 0.5012666583061218, 0.5013276934623718, 0.5014299750328064, 0.5014562606811523, 0.5014569163322449, 0.5013565421104431, 0.5013339519500732, 0.5013103485107422, 0.5013260841369629, 0.5013947486877441, 0.501474142074585, 0.5015454888343811, 0.5016071796417236, 0.501646101474762, 0.5014908313751221, 0.5013832449913025, 0.5012905597686768, 0.50120609998703, 0.5011496543884277, 0.5012590289115906, 0.5012661218643188, 0.5012381672859192, 0.5012338757514954, 0.5013276934623718, 0.5014688968658447, 0.5016087889671326, 0.5016428828239441, 0.5016710758209229, 0.5015712380409241, 0.5013842582702637, 0.5012156367301941, 0.5012246966362, 0.5013195872306824, 0.5013865828514099, 0.501422643661499, 0.501461923122406, 0.5014544725418091, 0.5014895796775818, 0.5014756321907043, 0.5013901591300964, 0.5012806057929993, 0.50111323595047, 0.5008907318115234, 0.5006858110427856, 0.500592052936554, 0.5006282329559326, 0.5005627870559692, 0.500483512878418, 0.5003767013549805, 0.500291109085083, 0.500390887260437, 0.500582754611969, 0.50084388256073, 0.5009219646453857, 0.5007765889167786, 0.5006510019302368, 0.5006095170974731, 0.5006464123725891, 0.5006422400474548, 0.5006910562515259, 0.5006896257400513, 0.500589907169342, 0.5004717111587524, 0.5003937482833862, 0.5003331303596497, 0.5002520084381104, 0.5001282095909119, 0.4998754858970642, 0.4995752274990082, 0.4993963837623596, 0.49931907653808594, 0.4991646409034729, 0.4991092383861542, 0.4992308020591736, 0.4995412826538086, 0.4998968839645386, 0.5001944303512573, 0.5004692077636719, 0.5006535053253174, 0.5007840394973755, 0.5007490515708923, 0.5006784200668335, 0.5004580020904541, 0.5002481341362, 0.5000525712966919, 0.5000017285346985, 0.5002002716064453, 0.5003210306167603, 0.5003790259361267, 0.5004063844680786, 0.5002477169036865, 0.5000780820846558, 0.49988695979118347, 0.5000038743019104, 0.5002089738845825, 0.5002292990684509, 0.5000717639923096, 0.49982354044914246, 0.4998677968978882, 0.5001581907272339, 0.5002081394195557, 0.49999237060546875, 0.4999476969242096, 0.5001126527786255, 0.5002661943435669, 0.5000463724136353, 0.4998549222946167, 0.49985310435295105] validation accuracy: [0.4, 0.25, 0.4, 0.35, 0.4, 0.55, 0.45, 0.5, 0.7, 0.75, 0.7, 0.75, 0.7, 0.7, 0.75, 0.85, 0.85, 0.9, 0.9, 0.9, 0.9, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 0.95, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:nvy1sjsg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total train loss</td><td>█▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total validation loss</td><td>█▇▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train label accuracy</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train label loss</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train triplet loss</td><td>▁▅▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>validation label accuracy</td><td>▁▃▆████████████████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>validation label loss</td><td>█▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation triplet loss</td><td>██▅▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.48309</td></tr><tr><td>total validation loss</td><td>0.49985</td></tr><tr><td>train label accuracy</td><td>1.0</td></tr><tr><td>train label loss</td><td>0.0</td></tr><tr><td>train triplet loss</td><td>0.48309</td></tr><tr><td>validation label accuracy</td><td>0.95</td></tr><tr><td>validation label loss</td><td>0.05474</td></tr><tr><td>validation triplet loss</td><td>0.44511</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-mountain-234</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nvy1sjsg' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/nvy1sjsg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231005_032025-nvy1sjsg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:nvy1sjsg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa52693733c48439d3f20e45df7da08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668480075895786, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/kmukherjee/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20231005_032038-jhknmzjg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/jhknmzjg' target=\"_blank\">confused-disco-235</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/jhknmzjg' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/jhknmzjg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:06<00:00, 46.36it/s]\n",
      "100%|██████████| 3/3 [00:41<00:00, 13.89s/it]/mnt/ws/home/kmukherjee/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (nz2v9xnh) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "100%|██████████| 3/3 [00:41<00:00, 13.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.7163375616073608, 0.7155418395996094, 0.717268168926239, 0.6877397894859314, 0.6773072481155396, 0.665557324886322, 0.6049054265022278, 0.5830373167991638, 0.5900408625602722, 0.5938760638237, 0.5966400504112244, 0.5866459608078003, 0.5334599018096924, 0.47422319650650024, 0.404109925031662, 0.3126247823238373, 0.2577734887599945, 0.22873620688915253, 0.2374824732542038, 0.24769015610218048, 0.25351324677467346, 0.24947679042816162, 0.2531905770301819, 0.2607647180557251, 0.24932144582271576, 0.22204045951366425, 0.19891981780529022, 0.1776742935180664, 0.1420324146747589, 0.12342579662799835, 0.11009359359741211, 0.09697508066892624, 0.08807722479104996, 0.08507118374109268, 0.07909972220659256, 0.07401665300130844, 0.06898420304059982, 0.06346767395734787, 0.05905212089419365, 0.05649843439459801, 0.05681758001446724, 0.058299969881772995, 0.06083018332719803, 0.05715053901076317, 0.053660690784454346, 0.05146346241235733, 0.04752609506249428, 0.046111851930618286, 0.04577995464205742, 0.04371606186032295, 0.044455405324697495, 0.04485166817903519, 0.04622276499867439, 0.0474206916987896, 0.048542123287916183, 0.04993418604135513, 0.051412928849458694, 0.052770137786865234, 0.05398089811205864, 0.05549174174666405, 0.056784044951200485, 0.05782902240753174, 0.050437845289707184, 0.04444071650505066, 0.04208662733435631, 0.04116791486740112, 0.040918707847595215, 0.041088804602622986, 0.041209202259778976, 0.041310865432024, 0.041401490569114685, 0.04139271378517151, 0.04139327630400658, 0.04133789986371994, 0.041238125413656235, 0.04109317064285278, 0.04088651388883591, 0.04070550203323364, 0.04047999531030655, 0.040283203125, 0.04008471965789795, 0.03996509686112404, 0.04003654792904854, 0.040018100291490555, 0.04000536724925041, 0.039935123175382614, 0.03999483957886696, 0.04006451368331909, 0.0401252917945385, 0.04018930718302727, 0.04025229439139366, 0.04032604396343231, 0.04040726274251938, 0.040475763380527496, 0.04059585556387901, 0.040687765926122665, 0.04077732190489769, 0.04089156165719032, 0.040978919714689255, 0.041077155619859695, 0.04118881747126579, 0.04129263758659363, 0.041359711438417435, 0.041444871574640274, 0.04150424525141716, 0.04159323498606682, 0.04170487821102142, 0.0418199822306633, 0.04191325977444649, 0.04198712110519409, 0.04205581545829773, 0.042155805975198746, 0.04221448674798012, 0.042270150035619736, 0.042319405823946, 0.04239283502101898, 0.04243800789117813, 0.04249180853366852, 0.042537085711956024, 0.04259081929922104, 0.04264818876981735, 0.04270639270544052, 0.042783405631780624, 0.042827308177948, 0.042854491621255875, 0.04290090501308441, 0.04294760152697563, 0.042987845838069916, 0.043040305376052856, 0.04306397959589958, 0.04307713732123375, 0.043102163821458817, 0.04313959553837776, 0.04318196699023247, 0.043208926916122437, 0.04323370382189751, 0.043246496468782425, 0.04326395317912102, 0.0432518869638443, 0.04326307401061058, 0.043281663209199905, 0.04331488534808159, 0.043372441083192825, 0.04343600198626518, 0.04348533973097801, 0.04353811964392662, 0.04358634352684021, 0.04362421855330467, 0.043645989149808884, 0.0436612069606781, 0.043678101152181625, 0.043700169771909714, 0.043738581240177155, 0.043772142380476, 0.04381091147661209, 0.04383550584316254, 0.04385675489902496, 0.04388051480054855, 0.043914999812841415, 0.04395875707268715, 0.04400342330336571, 0.044037993997335434, 0.04404888674616814, 0.044071462005376816, 0.04408442601561546, 0.04409994184970856, 0.04411911964416504, 0.04415212944149971, 0.044180091470479965, 0.044196728616952896, 0.04423246905207634, 0.04426427558064461, 0.04428218677639961, 0.044291745871305466, 0.044297534972429276, 0.04432278871536255, 0.04432714730501175, 0.04435514658689499, 0.044365789741277695, 0.044384777545928955, 0.04440495744347572, 0.0444069430232048, 0.044414181262254715, 0.04441969841718674, 0.04442992061376572, 0.04444947838783264, 0.044469382613897324, 0.044481996446847916, 0.04450250416994095, 0.04451245442032814, 0.04452335834503174, 0.044539544731378555, 0.04455232992768288, 0.044564615935087204, 0.04457497224211693, 0.044600311666727066, 0.044614195823669434, 0.04463180527091026, 0.04464436322450638, 0.04467475414276123, 0.04468598589301109, 0.044691141694784164, 0.04469475522637367, 0.04470420628786087, 0.04471592605113983, 0.044722914695739746, 0.04472939670085907, 0.04474979266524315, 0.04475519433617592, 0.04474420100450516, 0.04474031552672386, 0.044744983315467834, 0.04474804922938347, 0.04478124901652336, 0.044778868556022644, 0.04476121813058853, 0.04476247355341911, 0.0447707399725914, 0.04477791488170624, 0.04476555436849594, 0.0447678379714489, 0.04478002339601517, 0.04479452595114708, 0.044799432158470154, 0.04481140524148941, 0.04481400549411774, 0.04480774700641632, 0.04480644688010216, 0.044811856001615524, 0.044809646904468536, 0.04478347674012184, 0.04476767033338547, 0.044750701636075974, 0.0447305403649807, 0.044699642807245255, 0.04468124732375145, 0.044658880680799484, 0.0446452759206295, 0.04463687911629677, 0.04463654384016991, 0.04465092346072197, 0.04466884210705757, 0.04468683525919914, 0.044688016176223755, 0.0447041355073452, 0.044715918600559235, 0.04470854625105858, 0.04469332471489906, 0.04468929022550583, 0.044690415263175964, 0.04468272998929024, 0.04467320442199707, 0.044666536152362823, 0.04466637969017029, 0.044669944792985916, 0.0446615032851696, 0.044659268110990524, 0.04465074464678764, 0.04465673491358757, 0.044676732271909714, 0.044688351452350616, 0.04468433931469917, 0.04468917474150658, 0.04470539465546608, 0.04472285136580467, 0.04475203901529312, 0.04476788267493248, 0.04477628320455551, 0.04480038955807686, 0.044797029346227646, 0.044773444533348083, 0.04475334286689758, 0.044747643172740936, 0.044715750962495804, 0.044708751142024994, 0.044697538018226624, 0.04468243941664696, 0.04467617720365524, 0.04467903822660446, 0.04468189924955368, 0.0446787066757679, 0.04469895362854004, 0.04471754655241966, 0.0447344146668911, 0.04474128410220146, 0.04473414272069931, 0.044728245586156845, 0.044700607657432556, 0.04469806328415871, 0.044688332825899124, 0.04468642547726631, 0.044686656445264816, 0.04468860104680061, 0.04467953369021416, 0.04468930885195732, 0.04470174014568329, 0.04473539814352989, 0.0447433739900589, 0.04475467652082443, 0.044758185744285583] validation total loss: [1.4086437225341797, 1.3794336318969727, 1.348918080329895, 1.277297854423523, 1.2429883480072021, 1.2323671579360962, 1.1688406467437744, 1.1439685821533203, 1.1623046398162842, 1.1777336597442627, 1.1965712308883667, 1.1775665283203125, 1.0701501369476318, 0.9747435450553894, 0.865300178527832, 0.6832427978515625, 0.5678027272224426, 0.49887943267822266, 0.506197452545166, 0.5327406525611877, 0.5591989159584045, 0.5475751161575317, 0.5409932732582092, 0.5631214380264282, 0.5798364281654358, 0.5532132983207703, 0.5269314050674438, 0.5157849788665771, 0.5070268511772156, 0.5144336819648743, 0.5291028022766113, 0.5351917743682861, 0.547300398349762, 0.5594627857208252, 0.5557286739349365, 0.5510047674179077, 0.549281895160675, 0.5527513027191162, 0.5555406212806702, 0.5572508573532104, 0.5441153645515442, 0.545519232749939, 0.5521308779716492, 0.5656759142875671, 0.5750176310539246, 0.5831049084663391, 0.5990696549415588, 0.6198689937591553, 0.6402443647384644, 0.6493882536888123, 0.6159706711769104, 0.6368064284324646, 0.6497893333435059, 0.6525971293449402, 0.6510353684425354, 0.646774411201477, 0.6396055817604065, 0.6335927844047546, 0.6295300722122192, 0.6266881227493286, 0.6250565648078918, 0.6245247721672058, 0.6035657525062561, 0.5905905961990356, 0.5888810157775879, 0.6013840436935425, 0.6169613599777222, 0.6352985501289368, 0.6537599563598633, 0.6707302927970886, 0.6874819397926331, 0.703620195388794, 0.7189614176750183, 0.731155276298523, 0.741873562335968, 0.7513523101806641, 0.7586066126823425, 0.7645143270492554, 0.7679668068885803, 0.7702919244766235, 0.7716361880302429, 0.7728979587554932, 0.7738018035888672, 0.7742948532104492, 0.7746384739875793, 0.7748702168464661, 0.7752676606178284, 0.7754344940185547, 0.7754174470901489, 0.7756122350692749, 0.775559663772583, 0.7754566669464111, 0.7751868963241577, 0.774698793888092, 0.7743517160415649, 0.773870587348938, 0.7731477618217468, 0.7723256945610046, 0.7717550992965698, 0.771058201789856, 0.7705739736557007, 0.7701340913772583, 0.7697564363479614, 0.7692578434944153, 0.7687366604804993, 0.7682790160179138, 0.7677649855613708, 0.7671455144882202, 0.7666279673576355, 0.7659921646118164, 0.7654061317443848, 0.7647734880447388, 0.7641410231590271, 0.7638139128684998, 0.7633544206619263, 0.7628837823867798, 0.7624579668045044, 0.76200932264328, 0.7615878582000732, 0.7612131834030151, 0.7608898878097534, 0.7605330348014832, 0.7599976658821106, 0.7596464157104492, 0.7592576742172241, 0.7588374614715576, 0.7585472464561462, 0.7583575248718262, 0.758053183555603, 0.7577140927314758, 0.7574495673179626, 0.7570633888244629, 0.7567644715309143, 0.7563470602035522, 0.7558188438415527, 0.7554010152816772, 0.7551147937774658, 0.7548410892486572, 0.754544734954834, 0.7542362809181213, 0.7539309859275818, 0.753639280796051, 0.753483235836029, 0.7531492710113525, 0.7528495192527771, 0.7525305151939392, 0.7521255016326904, 0.7517987489700317, 0.7515890002250671, 0.7513521909713745, 0.7510718703269958, 0.7508154511451721, 0.7505606412887573, 0.750295102596283, 0.7500550150871277, 0.7497109174728394, 0.7493461966514587, 0.7489326000213623, 0.748528003692627, 0.7481964826583862, 0.7477922439575195, 0.7473334074020386, 0.7470048666000366, 0.7467283606529236, 0.7464891076087952, 0.746279776096344, 0.7460572719573975, 0.7457318902015686, 0.7453984618186951, 0.7451303005218506, 0.7447724342346191, 0.7443956732749939, 0.744044840335846, 0.7437095046043396, 0.7433724403381348, 0.7430756092071533, 0.7428122162818909, 0.7425138354301453, 0.7422898411750793, 0.7419484257698059, 0.7416263818740845, 0.7414118647575378, 0.7411563396453857, 0.7409240007400513, 0.7406164407730103, 0.7403638362884521, 0.7399560809135437, 0.7394987940788269, 0.7390023469924927, 0.7387509346008301, 0.7385479211807251, 0.7383708953857422, 0.7380749583244324, 0.7377553582191467, 0.7373999953269958, 0.7369368076324463, 0.7365179657936096, 0.7360357642173767, 0.7354764938354492, 0.7348888516426086, 0.7344982624053955, 0.7339992523193359, 0.7336331009864807, 0.7333492636680603, 0.7329971790313721, 0.7326231598854065, 0.7322590351104736, 0.7319388389587402, 0.731555700302124, 0.7314597368240356, 0.7311708927154541, 0.730862557888031, 0.7305771708488464, 0.7301502227783203, 0.7297756671905518, 0.729350745677948, 0.7289183735847473, 0.7284631729125977, 0.7280415892601013, 0.7278008460998535, 0.7274640798568726, 0.7271365523338318, 0.7267459630966187, 0.7263345718383789, 0.7259891629219055, 0.7255299687385559, 0.7250463962554932, 0.724571943283081, 0.7240527272224426, 0.7235028743743896, 0.7230611443519592, 0.7228472232818604, 0.7225692272186279, 0.7222830057144165, 0.7219472527503967, 0.7216049432754517, 0.7213596105575562, 0.721038281917572, 0.7207176089286804, 0.7203532457351685, 0.7199581265449524, 0.7195730805397034, 0.7191529273986816, 0.718780517578125, 0.718389093875885, 0.7180413603782654, 0.7177173495292664, 0.7174795269966125, 0.7171761393547058, 0.7169867753982544, 0.7167128324508667, 0.7164443731307983, 0.7161466479301453, 0.7158620357513428, 0.7154498100280762, 0.7150033712387085, 0.7145264744758606, 0.7140494585037231, 0.7136144042015076, 0.7131933569908142, 0.7127460837364197, 0.7123420238494873, 0.7119471430778503, 0.7115011811256409, 0.7111765742301941, 0.7107518911361694, 0.7104034423828125, 0.7100884318351746, 0.7095904350280762, 0.7091583609580994, 0.7089331746101379, 0.7087275981903076, 0.7084798812866211, 0.7082200050354004, 0.7078003883361816, 0.7073199152946472, 0.7069348692893982, 0.7066003680229187, 0.7062061429023743, 0.7057793140411377, 0.7053574919700623, 0.704794704914093, 0.7043222784996033, 0.7038689255714417, 0.7033907175064087, 0.7030431628227234, 0.7027803659439087, 0.7024992108345032, 0.7021996378898621, 0.7019938826560974, 0.7017285227775574, 0.7013714909553528, 0.7009832262992859, 0.7008567452430725, 0.7006497979164124, 0.7002981305122375, 0.6998319029808044, 0.6993436813354492, 0.6989176273345947, 0.6984825730323792] validation accuracy: [0.25, 0.2, 0.3, 0.3, 0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.4, 0.4, 0.5, 0.65, 0.65, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.85, 0.85, 0.85, 0.85, 0.8, 0.8, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.75, 0.75, 0.75, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jhknmzjg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total train loss</td><td>█▆▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total validation loss</td><td>█▇▂▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>train label accuracy</td><td>▁▃▇█████████████████████████████████████</td></tr><tr><td>train label loss</td><td>█▆▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train triplet loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▂▁▆███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>validation label loss</td><td>▆▅▂▁▃▄▅▆▅▇████████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>validation triplet loss</td><td>█▇▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.0</td></tr><tr><td>total validation loss</td><td>0.69848</td></tr><tr><td>train label accuracy</td><td>1.0</td></tr><tr><td>train label loss</td><td>0.0</td></tr><tr><td>train triplet loss</td><td>0.0</td></tr><tr><td>validation label accuracy</td><td>0.85</td></tr><tr><td>validation label loss</td><td>0.65372</td></tr><tr><td>validation triplet loss</td><td>0.04476</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confused-disco-235</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/jhknmzjg' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/jhknmzjg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231005_032038-jhknmzjg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jhknmzjg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d42980cb7942c9b00e905778dd9a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668398507560293, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/kmukherjee/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20231005_032053-8134vku6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/8134vku6' target=\"_blank\">silver-water-236</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/8134vku6' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/8134vku6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:06<00:00, 46.90it/s]\n",
      " 33%|███▎      | 1/3 [00:14<00:28, 14.11s/it]/mnt/ws/home/kmukherjee/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (jhknmzjg) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.5521707534790039, 0.39689046144485474, 0.3344293534755707, 0.29128164052963257, 0.33400851488113403, 0.36135396361351013, 0.3918306827545166, 0.41050729155540466, 0.43030744791030884, 0.4535525441169739, 0.4388657510280609, 0.4184727370738983, 0.37934380769729614, 0.3518800735473633, 0.31744351983070374, 0.2780458629131317, 0.24228592216968536, 0.22208920121192932, 0.20546482503414154, 0.1918599158525467, 0.1802792102098465, 0.17060087621212006, 0.16149449348449707, 0.15760014951229095, 0.1534852534532547, 0.14911124110221863, 0.14876142144203186, 0.14749346673488617, 0.13666729629039764, 0.13072586059570312, 0.1262650489807129, 0.1229868158698082, 0.12063316255807877, 0.11903542280197144, 0.11825302988290787, 0.11760518699884415, 0.11701243370771408, 0.11648907512426376, 0.1127203106880188, 0.11382788419723511, 0.11742427200078964, 0.11362522840499878, 0.12459228187799454, 0.1328897774219513, 0.13820065557956696, 0.14113914966583252, 0.14280973374843597, 0.14358572661876678, 0.14333637058734894, 0.13853056728839874, 0.13497494161128998, 0.12913265824317932, 0.12313001602888107, 0.11722978204488754, 0.11148997396230698, 0.10823769867420197, 0.10684793442487717, 0.10604657232761383, 0.10534389317035675, 0.10504307597875595, 0.10492240637540817, 0.1045941710472107, 0.10460617393255234, 0.10492070764303207, 0.10507837682962418, 0.10478787869215012, 0.1030159518122673, 0.1021108403801918, 0.1015007272362709, 0.10089800506830215, 0.10033097118139267, 0.10006430000066757, 0.10236115753650665, 0.10191061347723007, 0.10192586481571198, 0.09895630180835724, 0.09526985138654709, 0.10500431060791016, 0.10600435733795166, 0.10999348014593124, 0.114105224609375, 0.11649461090564728, 0.1175733357667923, 0.11681132763624191, 0.11536320298910141, 0.11325711011886597, 0.11085180193185806, 0.10905367136001587, 0.10941300541162491, 0.10601767152547836, 0.10140296071767807, 0.09808296710252762, 0.09497625380754471, 0.09166648238897324, 0.08846118301153183, 0.08539430797100067, 0.08258171379566193, 0.08025572448968887, 0.07843752205371857, 0.0753612294793129, 0.07375103235244751, 0.0731164738535881, 0.07280712574720383, 0.07274282723665237, 0.07297865301370621, 0.0733989030122757, 0.0708853006362915, 0.0755734071135521, 0.0783785954117775, 0.0794927105307579, 0.07968075573444366, 0.0876423716545105, 0.09382881224155426, 0.09187354147434235, 0.08966317772865295, 0.08853283524513245, 0.08936820179224014, 0.08991985768079758, 0.08616962283849716, 0.0740998312830925, 0.0677824541926384, 0.06535954773426056, 0.06433888524770737, 0.06424245238304138, 0.06413387507200241, 0.06400690227746964, 0.06381191313266754, 0.06364492326974869, 0.06322373449802399, 0.0627623125910759, 0.06227456405758858, 0.06196903809905052, 0.06175083667039871, 0.06153087690472603, 0.061337053775787354, 0.0633433386683464, 0.06474025547504425, 0.0660114511847496, 0.06648765504360199, 0.06808791309595108, 0.06935863941907883, 0.0692354068160057, 0.06860077381134033, 0.06822015345096588, 0.06922627985477448, 0.07016976922750473, 0.07118263095617294, 0.07219662517309189, 0.07321827858686447, 0.07424622774124146, 0.07519569247961044, 0.07667992264032364, 0.07829762995243073, 0.0797029659152031, 0.0810016319155693, 0.08225490897893906, 0.08341597765684128, 0.08479448407888412, 0.085628941655159, 0.08627522736787796, 0.08669884502887726, 0.08684708178043365, 0.08692499250173569, 0.08694606274366379, 0.08368192613124847, 0.08316033333539963, 0.08349711447954178, 0.08469249308109283, 0.08538847416639328, 0.08714594691991806, 0.088919997215271, 0.09026288241147995, 0.09125206619501114, 0.09220518171787262, 0.0935148373246193, 0.09034629911184311, 0.08884266018867493, 0.08801921457052231, 0.08767139911651611, 0.08835190534591675, 0.08927488327026367, 0.09029237180948257, 0.09558909386396408, 0.09883984178304672, 0.10023560374975204, 0.10117534548044205, 0.10117458552122116, 0.09957414120435715, 0.09788244962692261, 0.09605926275253296, 0.09401538223028183, 0.0923120379447937, 0.0909486934542656, 0.09051870554685593, 0.09014692902565002, 0.0899369940161705, 0.08976959437131882, 0.08955001085996628, 0.08941338211297989, 0.08924901485443115, 0.08923401683568954, 0.08929483592510223, 0.08963751047849655, 0.08991055935621262, 0.09023459255695343, 0.09053119271993637, 0.09078383445739746, 0.08681623637676239, 0.08469313383102417, 0.08223596960306168, 0.07989529520273209, 0.07827910035848618, 0.07911982387304306, 0.08083101361989975, 0.08211614191532135, 0.08337956666946411, 0.08459443598985672, 0.08548133820295334, 0.0901075005531311, 0.0929078757762909, 0.10025613754987717, 0.10313941538333893, 0.10723165422677994, 0.10885877907276154, 0.10950853675603867, 0.08979639410972595, 0.0828179270029068, 0.07757711410522461, 0.0597015917301178, 0.05805877968668938, 0.06722461432218552, 0.07773005217313766, 0.08602055162191391, 0.09117519110441208, 0.09442362189292908, 0.09752215445041656, 0.09845784306526184, 0.09044148772954941, 0.08502064645290375, 0.08133625239133835, 0.07641718536615372, 0.07064191997051239, 0.06459655612707138, 0.053321074694395065, 0.04682624340057373, 0.042667169123888016, 0.04006817564368248, 0.03669167682528496, 0.03148018568754196, 0.029867127537727356, 0.029417378827929497, 0.028499403968453407, 0.02922101691365242, 0.030174298211932182, 0.030724933370947838, 0.033046286553144455, 0.035078465938568115, 0.038881637156009674, 0.04174673184752464, 0.04445266351103783, 0.043091993778944016, 0.04132714122533798, 0.040833730250597, 0.04167639836668968, 0.04177749902009964, 0.04148080572485924, 0.04118787869811058, 0.04196842759847641, 0.04262064769864082, 0.04214099422097206, 0.042614053934812546, 0.042968347668647766, 0.04714313521981239, 0.050016749650239944, 0.052497442811727524, 0.054507769644260406, 0.056177761405706406, 0.05740290880203247, 0.0583096519112587, 0.059027303010225296, 0.0603337399661541, 0.05967283248901367, 0.05935906246304512, 0.06035522371530533, 0.06060969457030296, 0.06590336561203003, 0.06980591267347336, 0.07280635088682175, 0.07625432312488556, 0.07995102554559708, 0.08343712240457535, 0.08628425002098083, 0.08788586407899857, 0.0892871618270874, 0.08988243341445923, 0.09004609286785126, 0.09022878855466843, 0.09024420380592346, 0.09255582839250565, 0.09900426119565964] validation total loss: [1.2692546844482422, 1.093501091003418, 1.0290744304656982, 0.9808555245399475, 1.024535894393921, 1.0567926168441772, 1.0927166938781738, 1.1112715005874634, 1.128952980041504, 1.1489005088806152, 1.1323785781860352, 1.1113828420639038, 1.0738933086395264, 1.0488204956054688, 1.0183569192886353, 0.9823311567306519, 0.9510340094566345, 0.9333318471908569, 0.9185245633125305, 0.905774712562561, 0.8944006562232971, 0.8855867981910706, 0.8767048716545105, 0.8752334117889404, 0.872838020324707, 0.8699226379394531, 0.8700443506240845, 0.8692485094070435, 0.8598393201828003, 0.8545822501182556, 0.8507391214370728, 0.8482360243797302, 0.8468514680862427, 0.8461183309555054, 0.846031665802002, 0.8458312749862671, 0.8454680442810059, 0.8451712727546692, 0.8413065671920776, 0.8423970341682434, 0.8457202315330505, 0.8402571082115173, 0.8534610867500305, 0.8632473945617676, 0.8697822093963623, 0.87343430519104, 0.875495195388794, 0.8765268325805664, 0.8765405416488647, 0.8742212057113647, 0.8724345564842224, 0.8688156604766846, 0.864596962928772, 0.859817624092102, 0.8518143892288208, 0.8467282056808472, 0.8438321948051453, 0.8419658541679382, 0.8404750823974609, 0.8395645618438721, 0.8390818238258362, 0.8383862972259521, 0.8369130492210388, 0.8357713222503662, 0.8353312611579895, 0.8347621560096741, 0.8320966362953186, 0.8308269381523132, 0.8301502466201782, 0.8296079635620117, 0.8291682004928589, 0.8290160298347473, 0.830289900302887, 0.8266235589981079, 0.8240363001823425, 0.819805920124054, 0.81538325548172, 0.8221219778060913, 0.8193054795265198, 0.8189346790313721, 0.8192529082298279, 0.818236231803894, 0.8172459006309509, 0.8148966431617737, 0.8123136162757874, 0.80954909324646, 0.8067331910133362, 0.8043792247772217, 0.8075546622276306, 0.8075841665267944, 0.805241048336029, 0.8041467070579529, 0.8029657602310181, 0.8010671138763428, 0.7987029552459717, 0.7958742380142212, 0.7932906150817871, 0.7910352349281311, 0.7891609072685242, 0.787801206111908, 0.7877175807952881, 0.7881162762641907, 0.7885594367980957, 0.7891690135002136, 0.7899113893508911, 0.7907414436340332, 0.7872797250747681, 0.7905985116958618, 0.7922366857528687, 0.7923631072044373, 0.7918659448623657, 0.7997530698776245, 0.8067169785499573, 0.8051215410232544, 0.8033033609390259, 0.8033349514007568, 0.805473804473877, 0.8072577714920044, 0.8044468760490417, 0.7924704551696777, 0.7863543033599854, 0.7844614386558533, 0.7834681272506714, 0.7814909219741821, 0.7798939347267151, 0.7785441279411316, 0.7772378921508789, 0.7762130498886108, 0.7749366760253906, 0.7736554145812988, 0.7724445462226868, 0.7715111970901489, 0.7707003355026245, 0.7700062990188599, 0.7693802714347839, 0.7728331089019775, 0.7750698328018188, 0.7769677639007568, 0.7778299450874329, 0.7835529446601868, 0.787153959274292, 0.7878407835960388, 0.7870150804519653, 0.7860972285270691, 0.7866894006729126, 0.7871964573860168, 0.7876036763191223, 0.7880969643592834, 0.7885313034057617, 0.788873553276062, 0.7891491055488586, 0.7900367975234985, 0.7911602854728699, 0.792159378528595, 0.7931118607521057, 0.7939903140068054, 0.7948258519172668, 0.7950420379638672, 0.7950147390365601, 0.7950924038887024, 0.7950911521911621, 0.7948228120803833, 0.7945370078086853, 0.7942848801612854, 0.7941222786903381, 0.7948449850082397, 0.7961876392364502, 0.7975946664810181, 0.7985284924507141, 0.800338625907898, 0.8020076751708984, 0.8034095168113708, 0.8044431805610657, 0.8056071400642395, 0.8070575594902039, 0.8047189116477966, 0.8040661811828613, 0.8041015863418579, 0.804440975189209, 0.8054282665252686, 0.8066398501396179, 0.8079655170440674, 0.8122421503067017, 0.8138046860694885, 0.8127283453941345, 0.8113079071044922, 0.8092300295829773, 0.8055167198181152, 0.8028565645217896, 0.8000442385673523, 0.7971821427345276, 0.7947952747344971, 0.792818009853363, 0.7919625639915466, 0.7912068367004395, 0.7907289862632751, 0.7903444766998291, 0.7899792790412903, 0.7897385954856873, 0.7895036935806274, 0.7894280552864075, 0.7894889116287231, 0.7898557782173157, 0.7901511192321777, 0.7904762029647827, 0.7907803654670715, 0.7910454273223877, 0.7876509428024292, 0.7861398458480835, 0.7840622663497925, 0.7821946144104004, 0.7811839580535889, 0.7814205884933472, 0.782943069934845, 0.7842792868614197, 0.7856044769287109, 0.7868633270263672, 0.7877054214477539, 0.7910130023956299, 0.7924672365188599, 0.7992317080497742, 0.8022361397743225, 0.8099390864372253, 0.8139441013336182, 0.8164671659469604, 0.8010666370391846, 0.7976673245429993, 0.7958559393882751, 0.7752962112426758, 0.7731385231018066, 0.7829064726829529, 0.7958226799964905, 0.8061949610710144, 0.8124223351478577, 0.8160451650619507, 0.8198399543762207, 0.8188776969909668, 0.8074012398719788, 0.7993494272232056, 0.7937638163566589, 0.7878981232643127, 0.7822134494781494, 0.7764661908149719, 0.7670437097549438, 0.7614078521728516, 0.7560598850250244, 0.7516234517097473, 0.745954155921936, 0.736500084400177, 0.7320495247840881, 0.7294104099273682, 0.7269494533538818, 0.72608482837677, 0.7256793975830078, 0.7251061201095581, 0.7263511419296265, 0.7276191115379333, 0.7303751111030579, 0.7323224544525146, 0.7341998219490051, 0.7335771918296814, 0.7322646379470825, 0.7324870824813843, 0.7338116765022278, 0.7338176369667053, 0.7333710789680481, 0.7330169677734375, 0.7337875366210938, 0.7344948649406433, 0.7366572022438049, 0.7402105331420898, 0.7426577210426331, 0.7486597299575806, 0.7530698180198669, 0.756432056427002, 0.7594638466835022, 0.7618728280067444, 0.7636880874633789, 0.7652421593666077, 0.7665324807167053, 0.7703084349632263, 0.7708762288093567, 0.7718235850334167, 0.7733017206192017, 0.7738096117973328, 0.7785294651985168, 0.7817883491516113, 0.784429669380188, 0.7882292866706848, 0.7925339341163635, 0.7962737083435059, 0.799453616142273, 0.8010542988777161, 0.8025849461555481, 0.8031219244003296, 0.8031986951828003, 0.803337574005127, 0.8033158779144287, 0.805079460144043, 0.8114653825759888] validation accuracy: [0.3, 0.3, 0.3, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.15, 0.1, 0.15, 0.1, 0.1, 0.1, 0.1, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.15, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.2, 0.2, 0.2, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.3, 0.3, 0.3, 0.3, 0.25, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8134vku6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total train loss</td><td>▇▂▃▃▃▃▃▄▄▃▃▄▅▇█▇▆▄▃▃▃▃▃▄▄▃▃▃▄▄▄▇▆▂▁▂▅▅▅▅</td></tr><tr><td>total validation loss</td><td>▆█▆▄▃▃▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▃▂▃▂▁▁▁▂▂▂▃</td></tr><tr><td>train label accuracy</td><td>▅▃▄▅▃▃▅▅▃▄▅▃▄▆▆▃▃▃▃▃▃▃▄▄▄▃▄▅▄▂▃▁▅▇▆▅▇█▆█</td></tr><tr><td>train label loss</td><td>▂▂▃▃▃▃▃▄▄▃▃▄▅▇█▇▆▄▃▃▃▃▃▄▄▃▃▃▄▄▄▇▆▂▁▂▅▅▅▅</td></tr><tr><td>train triplet loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▄▄██▆▆▄▄▄▆██▆▆▄▄▁▄▄▆▆▆▆▄▄▄▃▄▄▁▄▄▄▃▃▄▆▄█▄</td></tr><tr><td>validation label loss</td><td>▁▃▃▅▆▇▇█▇▇▅▂▃▄▅▅▅▄▅▅▄▄▄▅▅▃▃▃▃▃▅▆▄▂▁▁▃▄▄▄</td></tr><tr><td>validation triplet loss</td><td>▆█▆▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.71054</td></tr><tr><td>total validation loss</td><td>0.81147</td></tr><tr><td>train label accuracy</td><td>0.33</td></tr><tr><td>train label loss</td><td>0.71054</td></tr><tr><td>train triplet loss</td><td>0.0</td></tr><tr><td>validation label accuracy</td><td>0.2</td></tr><tr><td>validation label loss</td><td>0.71246</td></tr><tr><td>validation triplet loss</td><td>0.099</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-water-236</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/8134vku6' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/8134vku6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231005_032053-8134vku6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8134vku6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02be6f87fc574e028a6cad5d94bb47e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666849033596615, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/kmukherjee/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20231005_032107-9udrjwy5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/9udrjwy5' target=\"_blank\">lunar-sea-237</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/9udrjwy5' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/9udrjwy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:05<00:00, 55.77it/s]\n",
      " 67%|██████▋   | 2/3 [00:29<00:14, 14.62s/it]/mnt/ws/home/kmukherjee/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (jhknmzjg) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.6911994218826294, 0.7093791365623474, 0.7188628911972046, 0.7621520161628723, 0.8047378659248352, 0.8175811767578125, 0.8054048418998718, 0.7805483937263489, 0.7308078408241272, 0.678548276424408, 0.6232680678367615, 0.5949092507362366, 0.5672026872634888, 0.5422172546386719, 0.5129274129867554, 0.4805111587047577, 0.46587324142456055, 0.4545253813266754, 0.444928377866745, 0.4364512860774994, 0.4303024709224701, 0.42738398909568787, 0.42585378885269165, 0.4247901141643524, 0.4229370057582855, 0.4212643802165985, 0.41916584968566895, 0.4169701635837555, 0.4144483506679535, 0.4122041165828705, 0.41043004393577576, 0.4089180529117584, 0.40731629729270935, 0.4059363901615143, 0.4047562777996063, 0.4040282666683197, 0.40367060899734497, 0.40344902873039246, 0.403303861618042, 0.4032896161079407, 0.4033872187137604, 0.4036906361579895, 0.4040639102458954, 0.40447521209716797, 0.4048921763896942, 0.40532001852989197, 0.4057399332523346, 0.40611791610717773, 0.40651342272758484, 0.4068771004676819, 0.40722352266311646, 0.40756693482398987, 0.40789103507995605, 0.40819644927978516, 0.4084892272949219, 0.4087699055671692, 0.4090214669704437, 0.40925008058547974, 0.4094465672969818, 0.4096214771270752, 0.4097902774810791, 0.4099559783935547, 0.41010600328445435, 0.41025182604789734, 0.41038641333580017, 0.4105062484741211, 0.4106254279613495, 0.41071996092796326, 0.4108036160469055, 0.4108714163303375, 0.41092681884765625, 0.4109747111797333, 0.4110213816165924, 0.41106730699539185, 0.41110849380493164, 0.4111555218696594, 0.411204069852829, 0.41123420000076294, 0.4112611711025238, 0.41129541397094727, 0.4113183915615082, 0.4113306701183319, 0.4113319516181946, 0.41133353114128113, 0.4113324284553528, 0.4113471508026123, 0.41135430335998535, 0.41136589646339417, 0.41137418150901794, 0.41139546036720276, 0.4114091992378235, 0.4114207327365875, 0.41142258048057556, 0.41141757369041443, 0.41140833497047424, 0.4114072024822235, 0.4114168584346771, 0.4114168584346771, 0.41142091155052185, 0.4114144444465637, 0.4114173948764801, 0.4114172160625458, 0.4114189147949219, 0.41141462326049805, 0.4114108681678772, 0.4114082455635071, 0.4114033877849579, 0.4114011228084564, 0.41139769554138184, 0.4113945960998535, 0.41139695048332214, 0.411411851644516, 0.41141578555107117, 0.41140928864479065, 0.41140708327293396, 0.41140708327293396, 0.4114070534706116, 0.4114115834236145, 0.4114238917827606, 0.4114299714565277, 0.4114384651184082, 0.41144123673439026, 0.4114375710487366, 0.41143733263015747, 0.4114513397216797, 0.4114657938480377, 0.41146373748779297, 0.4114641845226288, 0.41146236658096313, 0.4114665687084198, 0.4114665687084198, 0.41147205233573914, 0.4114719033241272, 0.4114771783351898, 0.4114716649055481, 0.41146355867385864, 0.4114723205566406, 0.41147729754447937, 0.4114537239074707, 0.41143694519996643, 0.4114254415035248, 0.41141477227211, 0.41140252351760864, 0.41139793395996094, 0.4113925099372864, 0.41138800978660583, 0.41138434410095215, 0.4113728702068329, 0.4113645553588867, 0.4113520085811615, 0.4113466441631317, 0.4113422930240631, 0.4113479554653168, 0.41134530305862427, 0.41134244203567505, 0.41134315729141235, 0.41134101152420044, 0.41133442521095276, 0.4113314151763916, 0.4113151729106903, 0.41130954027175903, 0.41130533814430237, 0.41129523515701294, 0.41128039360046387, 0.41126927733421326, 0.4112606942653656, 0.4112519323825836, 0.41126060485839844, 0.4112587571144104, 0.4112586975097656, 0.4112568795681, 0.4112529754638672, 0.41125956177711487, 0.4112663269042969, 0.4112691581249237, 0.4112648069858551, 0.41126298904418945, 0.4112570881843567, 0.4112355411052704, 0.4112221896648407, 0.41121822595596313, 0.4112173616886139, 0.4112044870853424, 0.4112059772014618, 0.4112006723880768, 0.4111902415752411, 0.4111778736114502, 0.4111611843109131, 0.41114550828933716, 0.4111326336860657, 0.41112494468688965, 0.4111202657222748, 0.4111221432685852, 0.4111234247684479, 0.4111250042915344, 0.4111187160015106, 0.411113977432251, 0.41111212968826294, 0.41110917925834656, 0.4110853374004364, 0.4110491871833801, 0.41100844740867615, 0.4109761714935303, 0.4109472930431366, 0.4109184741973877, 0.410888671875, 0.41086721420288086, 0.41085085272789, 0.41083088517189026, 0.4108128547668457, 0.4107811450958252, 0.41076546907424927, 0.41075459122657776, 0.41074472665786743, 0.41074514389038086, 0.41074952483177185, 0.41073623299598694, 0.41071730852127075, 0.4106854498386383, 0.41065463423728943, 0.41063380241394043, 0.41061362624168396, 0.4106006622314453, 0.4105819761753082, 0.4105619490146637, 0.4105401039123535, 0.41051384806632996, 0.41048821806907654, 0.4104599058628082, 0.4104389250278473, 0.4104290008544922, 0.4104175567626953, 0.4103994369506836, 0.4103846251964569, 0.4103820323944092, 0.4103633463382721, 0.41034555435180664, 0.4103279709815979, 0.4103035032749176, 0.4102795720100403, 0.41025152802467346, 0.41023093461990356, 0.41020822525024414, 0.4101952016353607, 0.4101819694042206, 0.41017112135887146, 0.41015544533729553, 0.4101366698741913, 0.4101172983646393, 0.41009455919265747, 0.4100673794746399, 0.4100242555141449, 0.40997394919395447, 0.4099373519420624, 0.4099073112010956, 0.40988895297050476, 0.4098682403564453, 0.4098599851131439, 0.4098454415798187, 0.40982183814048767, 0.4097938537597656, 0.40976834297180176, 0.40974217653274536, 0.40970727801322937, 0.40967217087745667, 0.40966030955314636, 0.4096531867980957, 0.4096429944038391, 0.40961796045303345, 0.4095814824104309, 0.40954676270484924, 0.40950971841812134, 0.4094820022583008, 0.40944868326187134, 0.4094214141368866, 0.40938177704811096, 0.4093606173992157, 0.40933725237846375, 0.409313827753067, 0.4092903137207031, 0.4092631936073303, 0.40924331545829773, 0.4092099368572235, 0.4091765582561493, 0.40914469957351685, 0.4090990126132965, 0.4090472161769867, 0.40900370478630066, 0.40897199511528015, 0.40894633531570435, 0.4089168608188629, 0.4088791310787201, 0.4088331162929535, 0.4088006019592285, 0.40877267718315125, 0.4087502658367157, 0.4087253212928772, 0.40869951248168945, 0.40866461396217346, 0.4086281359195709] validation total loss: [1.3259856700897217, 1.2751330137252808, 1.2331165075302124, 1.3597991466522217, 1.571082592010498, 1.6561920642852783, 1.6378296613693237, 1.5564072132110596, 1.3671061992645264, 1.1440953016281128, 0.9369031190872192, 0.8601516485214233, 0.8097777366638184, 0.7579566836357117, 0.7017966508865356, 0.630836546421051, 0.6047922968864441, 0.5819952487945557, 0.5612688064575195, 0.5416325330734253, 0.5215862989425659, 0.5070112943649292, 0.496018648147583, 0.4849855303764343, 0.4724558889865875, 0.46566715836524963, 0.46503791213035583, 0.4624534845352173, 0.45842245221138, 0.4576302766799927, 0.458647221326828, 0.4612087309360504, 0.4640951454639435, 0.4684433341026306, 0.4737032651901245, 0.4796917140483856, 0.4871199429035187, 0.4927281439304352, 0.49678972363471985, 0.5000925660133362, 0.5022708177566528, 0.5041408538818359, 0.5056719779968262, 0.5081425905227661, 0.5109463930130005, 0.5141111612319946, 0.5173743963241577, 0.5192528367042542, 0.5209078788757324, 0.5217800736427307, 0.5220752358436584, 0.5219741463661194, 0.521655261516571, 0.5211166739463806, 0.5208772420883179, 0.5209454894065857, 0.5207722187042236, 0.520979642868042, 0.521287202835083, 0.5215553045272827, 0.521742045879364, 0.5218525528907776, 0.5216984748840332, 0.5216224193572998, 0.5213539600372314, 0.5208609700202942, 0.520463764667511, 0.5201553106307983, 0.51983642578125, 0.5196606516838074, 0.519447922706604, 0.5192985534667969, 0.5190806984901428, 0.5190011262893677, 0.5189397931098938, 0.5185413956642151, 0.5180699229240417, 0.517575740814209, 0.517143189907074, 0.5169278383255005, 0.516653835773468, 0.5163787007331848, 0.5161213874816895, 0.5158627033233643, 0.5155177712440491, 0.5150400996208191, 0.514492392539978, 0.5139517188072205, 0.5133971571922302, 0.5134097337722778, 0.5134201645851135, 0.5134547352790833, 0.5135567784309387, 0.5136730670928955, 0.5137962102890015, 0.5139576196670532, 0.5141162276268005, 0.5140447020530701, 0.5138412117958069, 0.5136094689369202, 0.513361930847168, 0.5131067037582397, 0.5128454566001892, 0.5126541256904602, 0.5124341249465942, 0.5122970938682556, 0.5121312737464905, 0.5118487477302551, 0.5115501880645752, 0.511172354221344, 0.5107237100601196, 0.510271430015564, 0.509871780872345, 0.5094811916351318, 0.509281575679779, 0.509109616279602, 0.5089824795722961, 0.5089815855026245, 0.5090380311012268, 0.509080171585083, 0.5091401934623718, 0.5092415809631348, 0.5093377232551575, 0.5095662474632263, 0.5096683502197266, 0.509807288646698, 0.509948194026947, 0.5099753141403198, 0.5099511742591858, 0.5100646018981934, 0.5100820064544678, 0.5100473165512085, 0.5100015997886658, 0.5098695158958435, 0.5095999240875244, 0.5093408823013306, 0.5091229677200317, 0.508966326713562, 0.5085704922676086, 0.5081683397293091, 0.5078432559967041, 0.5076525211334229, 0.5073127150535583, 0.5071046948432922, 0.5068490505218506, 0.5065996646881104, 0.5064091086387634, 0.5062135457992554, 0.5061859488487244, 0.5061684250831604, 0.5060535073280334, 0.5059018135070801, 0.5056549906730652, 0.5054192543029785, 0.5051418542861938, 0.5046557784080505, 0.5042297840118408, 0.5038009881973267, 0.5032945871353149, 0.5027939081192017, 0.5025454163551331, 0.5022322535514832, 0.5018199682235718, 0.501462459564209, 0.5010699033737183, 0.5007627010345459, 0.5005202293395996, 0.5004235506057739, 0.5001652836799622, 0.4997960329055786, 0.4994187653064728, 0.49909403920173645, 0.4986855387687683, 0.49834051728248596, 0.4979565739631653, 0.4975225329399109, 0.4970141053199768, 0.4965035617351532, 0.49592411518096924, 0.49541163444519043, 0.49481093883514404, 0.49421370029449463, 0.4935709834098816, 0.49307528138160706, 0.4926287829875946, 0.4921327531337738, 0.491598904132843, 0.4911757707595825, 0.4907556176185608, 0.4902721047401428, 0.48981794714927673, 0.4893633723258972, 0.4889800250530243, 0.48853063583374023, 0.4881495535373688, 0.487712025642395, 0.4872034788131714, 0.48686009645462036, 0.4865443706512451, 0.48608851432800293, 0.48554620146751404, 0.48506563901901245, 0.4845983386039734, 0.4842579960823059, 0.48387131094932556, 0.48355308175086975, 0.4832487106323242, 0.48290032148361206, 0.48246872425079346, 0.4819778501987457, 0.48141592741012573, 0.4808593988418579, 0.48039665818214417, 0.47995778918266296, 0.4794802665710449, 0.4789179563522339, 0.47838377952575684, 0.47790074348449707, 0.477343887090683, 0.4767976999282837, 0.47626206278800964, 0.47573724389076233, 0.4752904176712036, 0.47489410638809204, 0.47442755103111267, 0.47395119071006775, 0.4734947383403778, 0.47302311658859253, 0.47245341539382935, 0.4719291627407074, 0.4714818000793457, 0.47101250290870667, 0.4706011414527893, 0.4701683521270752, 0.4699059724807739, 0.4695785641670227, 0.46917617321014404, 0.46869030594825745, 0.4681699872016907, 0.4676024615764618, 0.46695953607559204, 0.46646109223365784, 0.46593165397644043, 0.46547359228134155, 0.46485868096351624, 0.4644491970539093, 0.4640257656574249, 0.4635385572910309, 0.4631917178630829, 0.46286872029304504, 0.4625283181667328, 0.46212512254714966, 0.46158748865127563, 0.4611698389053345, 0.46068641543388367, 0.46034184098243713, 0.45981845259666443, 0.4593018591403961, 0.45880597829818726, 0.4583739638328552, 0.45793503522872925, 0.45762795209884644, 0.4572991728782654, 0.45684781670570374, 0.45638275146484375, 0.45605725049972534, 0.4556446671485901, 0.45517870783805847, 0.4545818865299225, 0.4540337920188904, 0.4534783661365509, 0.4529081881046295, 0.45241302251815796, 0.4520089626312256, 0.4516574740409851, 0.45118248462677, 0.4508114159107208, 0.45043984055519104, 0.45009341835975647, 0.44975507259368896, 0.4493747651576996, 0.44907429814338684, 0.4487324059009552, 0.44837287068367004, 0.44798943400382996, 0.4475896656513214, 0.4471864700317383, 0.44681504368782043, 0.4464346766471863, 0.44599616527557373, 0.4455919861793518, 0.4451513886451721, 0.44475436210632324, 0.4443722367286682, 0.44402581453323364, 0.4437129497528076, 0.4434106647968292, 0.44314146041870117, 0.4428379237651825, 0.4425404369831085] validation accuracy: [0.3, 0.35, 0.4, 0.35, 0.25, 0.25, 0.35, 0.45, 0.55, 0.55, 0.75, 0.8, 0.85, 0.8, 0.8, 0.85, 0.85, 0.85, 0.9, 0.9, 0.9, 0.9, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9udrjwy5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total train loss</td><td>█▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total validation loss</td><td>▇█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train label accuracy</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>train label loss</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train triplet loss</td><td>▁▅▇███████████████████████████▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>validation label accuracy</td><td>▁▂▇█████████████████████████████████████</td></tr><tr><td>validation label loss</td><td>▆█▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation triplet loss</td><td>██▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.38306</td></tr><tr><td>total validation loss</td><td>0.44254</td></tr><tr><td>train label accuracy</td><td>1.0</td></tr><tr><td>train label loss</td><td>0.0</td></tr><tr><td>train triplet loss</td><td>0.38306</td></tr><tr><td>validation label accuracy</td><td>0.95</td></tr><tr><td>validation label loss</td><td>0.03391</td></tr><tr><td>validation triplet loss</td><td>0.40863</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lunar-sea-237</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/9udrjwy5' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/9udrjwy5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231005_032107-9udrjwy5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9udrjwy5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7488a4cb5e63416181e51faa52b6c397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668500006198884, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/kmukherjee/repos/ConceptualAlignmentLanguage/code/python/wandb/run-20231005_032122-2u7zeo4q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/2u7zeo4q' target=\"_blank\">fluent-dragon-238</a></strong> to <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/2u7zeo4q' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/2u7zeo4q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:05<00:00, 53.29it/s]\n",
      "100%|██████████| 3/3 [00:42<00:00, 14.03s/it]/mnt/ws/home/kmukherjee/miniconda3/envs/sketch_tools/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (jhknmzjg) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "100%|██████████| 3/3 [00:42<00:00, 14.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation triplet loss: [0.5582502484321594, 0.47157904505729675, 0.4538884162902832, 0.4453936517238617, 0.446988970041275, 0.4235537648200989, 0.4044879972934723, 0.40253233909606934, 0.4473557472229004, 0.47154757380485535, 0.4861145615577698, 0.49492621421813965, 0.5103545188903809, 0.533054530620575, 0.5019141435623169, 0.4676712453365326, 0.4148188531398773, 0.3440546691417694, 0.3125035762786865, 0.2982857823371887, 0.26579615473747253, 0.2236388772726059, 0.173209547996521, 0.1281779408454895, 0.10198425501585007, 0.08558379858732224, 0.06799783557653427, 0.056807439774274826, 0.047674477100372314, 0.04102534055709839, 0.0364123210310936, 0.033108364790678024, 0.03365987166762352, 0.034381818026304245, 0.03562166914343834, 0.03647340461611748, 0.037025269120931625, 0.03702318295836449, 0.04084473475813866, 0.043173421174287796, 0.04443296417593956, 0.044872500002384186, 0.04020344465970993, 0.035594258457422256, 0.030746012926101685, 0.02761121653020382, 0.02535751461982727, 0.023811569437384605, 0.022436687722802162, 0.021199483424425125, 0.020114758983254433, 0.01914479210972786, 0.018293870612978935, 0.017529690638184547, 0.01687084510922432, 0.01633160375058651, 0.0185104887932539, 0.017757097259163857, 0.01617577113211155, 0.01293080486357212, 0.010920906439423561, 0.009379187598824501, 0.008292138576507568, 0.007380998227745295, 0.00657383818179369, 0.0059674144722521305, 0.005709195043891668, 0.00546076288446784, 0.005122089292854071, 0.004843241069465876, 0.004617550875991583, 0.004461133386939764, 0.004349046852439642, 0.0042517841793596745, 0.0041736154817044735, 0.004106295295059681, 0.005357608199119568, 0.006384372711181641, 0.007839510217308998, 0.00846227165311575, 0.0087571507319808, 0.008606932125985622, 0.008058718405663967, 0.007259112782776356, 0.0061762067489326, 0.004986980464309454, 0.0038397430907934904, 0.0028034597635269165, 0.0019614279735833406, 0.0013283550506457686, 0.0008706599473953247, 0.0005255311843939126, 0.00028548241243697703, 0.00010360181477153674, 0.0, 7.838010787963867e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] validation total loss: [1.1988661289215088, 1.0735400915145874, 1.0191960334777832, 0.9730734825134277, 0.9430129528045654, 0.8917651176452637, 0.8477919101715088, 0.8259046077728271, 0.8718722462654114, 0.8897327780723572, 0.9038879871368408, 0.927553653717041, 0.9841092824935913, 1.0485692024230957, 1.0252963304519653, 1.0122126340866089, 0.9669443368911743, 0.8712754249572754, 0.8471775054931641, 0.8498178720474243, 0.7941473722457886, 0.7029456496238708, 0.5755523443222046, 0.42650964856147766, 0.29523178935050964, 0.20890294015407562, 0.14198163151741028, 0.10143512487411499, 0.07639332860708237, 0.06238262355327606, 0.053188011050224304, 0.047191035002470016, 0.04730382189154625, 0.047171201556921005, 0.04763394966721535, 0.04923419654369354, 0.050027500838041306, 0.04959901422262192, 0.04803823307156563, 0.04777500778436661, 0.047553036361932755, 0.04693195968866348, 0.04059230908751488, 0.03571228310465813, 0.03080456145107746, 0.02764826826751232, 0.025384332984685898, 0.02383248880505562, 0.022453967481851578, 0.02121439203619957, 0.020128000527620316, 0.019156722351908684, 0.018304618075489998, 0.017539434134960175, 0.016879726201295853, 0.016339685767889023, 0.018514715135097504, 0.01776069961488247, 0.016180099919438362, 0.012936157174408436, 0.010926984250545502, 0.00938617717474699, 0.008300160989165306, 0.0073875486850738525, 0.006579869892448187, 0.005973099265247583, 0.0057146307080984116, 0.0054661002941429615, 0.005127340089529753, 0.0048484355211257935, 0.004622708074748516, 0.004466250538825989, 0.004354117438197136, 0.004256816115230322, 0.004178632516413927, 0.004111298825591803, 0.005363328382372856, 0.006390961818397045, 0.007848053239285946, 0.008474783971905708, 0.00877494178712368, 0.008630570955574512, 0.00808729324489832, 0.007290113251656294, 0.006206202786415815, 0.005014127120375633, 0.003863561199977994, 0.002823976567015052, 0.0019792423117905855, 0.0013444935902953148, 0.0008861289243213832, 0.0005409364239312708, 0.00030124670593068004, 0.00011994691885774955, 1.6998747014440596e-05, 2.5537958208587952e-05, 1.1191234989382792e-05, 8.76589820109075e-06, 7.648491191503126e-06, 7.082323918439215e-06, 6.745598511770368e-06, 6.548926648974884e-06, 6.429730547097279e-06, 8.135729331115726e-06, 1.1773848200391512e-05, 1.743221037031617e-05, 2.6183004592894576e-05, 3.864958489430137e-05, 5.311256609275006e-05, 6.302154361037537e-05, 6.690669397357851e-05, 6.429521454265341e-05, 5.9008027164964005e-05, 5.36937550350558e-05, 4.8058267566375434e-05, 4.275212268112227e-05, 3.818173718173057e-05, 3.418860069359653e-05, 3.10945397359319e-05, 2.8505706723080948e-05, 2.620665509311948e-05, 2.4416280211880803e-05, 2.274924008816015e-05, 2.1440793716465123e-05, 2.0371911887195893e-05, 1.9268723917775787e-05, 1.83828469744185e-05, 1.7629443391342647e-05, 1.6972782759694383e-05, 1.636821252759546e-05, 1.585148311278317e-05, 1.5438974514836445e-05, 1.5063686078065075e-05, 1.4807540537731256e-05, 1.4585634744435083e-05, 1.4406921764020808e-05, 1.4262463082559407e-05, 1.4109063158684876e-05, 1.3949701497040223e-05, 1.3830554053129163e-05, 1.3773955288343132e-05, 1.3721830327995121e-05, 1.3714389751839917e-05, 1.3684605619346257e-05, 1.367715867672814e-05, 1.3657800991495606e-05, 1.3647377272718586e-05, 1.3632488844450563e-05, 1.3611638678412419e-05, 1.3617601325677242e-05, 1.3638452401210088e-05, 1.3644411410496105e-05, 1.3666753147845156e-05, 1.367718414257979e-05, 1.3695059351448435e-05, 1.3747189768764656e-05, 1.382016944262432e-05, 1.388123655488016e-05, 1.3902087630413007e-05, 1.3906559615861624e-05, 1.3927412510383874e-05, 1.3942309124104213e-05, 1.3975080037198495e-05, 1.4031678801984526e-05, 1.4070404176891316e-05, 1.4121047570370138e-05, 1.4165729226078838e-05, 1.4231268323783297e-05, 1.4265526260714978e-05, 1.4292335436039139e-05, 1.4304250726127066e-05, 1.4342974282044452e-05, 1.438021172361914e-05, 1.4429363545787055e-05, 1.4474045201495755e-05, 1.4511283552565146e-05, 1.4550008927471936e-05, 1.4587244550057221e-05, 1.462299405829981e-05, 1.4625976291426923e-05, 1.4633426872023847e-05, 1.4651302080892492e-05, 1.4703429769724607e-05, 1.4761515558348037e-05, 1.4803218618908431e-05, 1.4846412341285031e-05, 1.4894076230120845e-05, 1.495365268056048e-05, 1.5001312021922786e-05, 1.5028123925731052e-05, 1.5045998225104995e-05, 1.5089192856976297e-05, 1.510855508968234e-05, 1.5163662283157464e-05, 1.5230686585709918e-05, 1.5288775102817453e-05, 1.533792783448007e-05, 1.537069329060614e-05, 1.5416864698636346e-05, 1.5470481230295263e-05, 1.552260982862208e-05, 1.5550909665762447e-05, 1.5583673302899115e-05, 1.56700534716947e-05, 1.5723668184364215e-05, 1.5784728020662442e-05, 1.579813579155598e-05, 1.582792538101785e-05, 1.5869629351072945e-05, 1.5921763406367972e-05, 1.598580456629861e-05, 1.604985118319746e-05, 1.610197614354547e-05, 1.611984771443531e-05, 1.616304143681191e-05, 1.618091482669115e-05, 1.61987918545492e-05, 1.6230071196332574e-05, 1.6285181118291803e-05, 1.6311994841089472e-05, 1.630454971746076e-05, 1.6323914678650908e-05, 1.6350726582459174e-05, 1.6370093362638727e-05, 1.64043467520969e-05, 1.6414775018347427e-05, 1.645350130274892e-05, 1.6517547919647768e-05, 1.656967833696399e-05, 1.6623298506601714e-05, 1.6663518181303516e-05, 1.66977770277299e-05, 1.6708210750948638e-05, 1.6703750588931143e-05, 1.671567042649258e-05, 1.6711206626496278e-05, 1.6727595721022226e-05, 1.6749936548876576e-05, 1.6791642337921076e-05, 1.681845787970815e-05, 1.682591209828388e-05, 1.687952499196399e-05, 1.691526995273307e-05, 1.694654747552704e-05, 1.698229243629612e-05, 1.7026977729983628e-05, 1.7053789633791894e-05, 1.7116342860390432e-05, 1.717591476335656e-05, 1.7177411791635677e-05, 1.719529063848313e-05, 1.7187849152833223e-05, 1.7214664694620296e-05, 1.7268281226279214e-05, 1.7353169823763892e-05, 1.7411255612387322e-05, 1.743658140185289e-05, 1.74827546288725e-05, 1.75289242179133e-05, 1.7628708519623615e-05, 1.7691261746222153e-05, 1.7731479601934552e-05, 1.7805945390136912e-05, 1.7861051674117334e-05, 1.7901265891850926e-05, 1.792062903405167e-05, 1.7947444575838745e-05, 1.7923626728588715e-05, 1.7920659956871532e-05, 1.790428905223962e-05, 1.7910257156472653e-05, 1.7963877326110378e-05, 1.8057702618534677e-05, 1.8133656340069138e-05, 1.8203651052317582e-05, 1.8199198166257702e-05, 1.8203676518169232e-05, 1.8267715859110467e-05, 1.8334738342673518e-05, 1.8389842807664536e-05, 1.844048711063806e-05, 1.8465807443135418e-05, 1.8456876205164008e-05, 1.8456890757079236e-05, 1.8477749108569697e-05, 1.8520941011956893e-05, 1.8544777049100958e-05, 1.8613285647006705e-05, 1.8620738046593033e-05, 1.866690581664443e-05, 1.8681805158848874e-05, 1.8695214748731814e-05, 1.8698201529332437e-05, 1.869821426225826e-05, 1.8690776414587162e-05, 1.872503708000295e-05, 1.8765260392683558e-05, 1.8789092791848816e-05, 1.883675213321112e-05, 1.8872500731959008e-05] validation accuracy: [0.15, 0.25, 0.4, 0.5, 0.5, 0.55, 0.55, 0.6, 0.6, 0.65, 0.65, 0.65, 0.7, 0.7, 0.7, 0.7, 0.7, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.85, 0.85, 0.9, 0.9, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total train loss</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total validation loss</td><td>█▇█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train label accuracy</td><td>▁▄██████████████████████████████████████</td></tr><tr><td>train label loss</td><td>█▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train triplet loss</td><td>█▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation label accuracy</td><td>▁▂▄▆████████████████████████████████████</td></tr><tr><td>validation label loss</td><td>█▆█▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation triplet loss</td><td>█▇█▃▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latent separation accuracy</td><td>1.0</td></tr><tr><td>total train loss</td><td>0.0</td></tr><tr><td>total validation loss</td><td>2e-05</td></tr><tr><td>train label accuracy</td><td>1.0</td></tr><tr><td>train label loss</td><td>0.0</td></tr><tr><td>train triplet loss</td><td>0.0</td></tr><tr><td>validation label accuracy</td><td>1.0</td></tr><tr><td>validation label loss</td><td>2e-05</td></tr><tr><td>validation triplet loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fluent-dragon-238</strong> at: <a href='https://wandb.ai/psych-711/ConceptualAlignment2023/runs/2u7zeo4q' target=\"_blank\">https://wandb.ai/psych-711/ConceptualAlignment2023/runs/2u7zeo4q</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231005_032122-2u7zeo4q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "\n",
    "num_classes = 4 # Number of unique class labels in the dataset\n",
    "latent_dims = 64\n",
    "epochs = 300\n",
    "lr = 0.005\n",
    "num_models = 1\n",
    "batch_size = 256\n",
    "save_dir = save_dir\n",
    "main_code(save_dir, num_models, epochs, num_classes, batch_size,\n",
    "             lr, latent_dims)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5ec221b8ce1ddc6eafacfbc77a75e3f09c9fea6e76ac8503f9810425480e77e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
